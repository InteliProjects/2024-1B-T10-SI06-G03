{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo do Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook é organizado em três seções principais:\n",
    "\n",
    "1. **Vetores de Palavras Pré-Treinados**:\n",
    "   - Carregue os vetores de palavras FastText de 300 dimensões e reduza-os para 100 dimensões. Esta redução otimiza o uso de espaço e melhora o desempenho do modelo sem perder muita informação contextual.\n",
    "\n",
    "2. **Treinamento do Modelo**:\n",
    "   - Treine e salve modelos XGBoost utilizando os vetores de 100 dimensões. Esta seção é subdividida em dois modelos distintos:\n",
    "     - **XGBoost para Classificação Negativo vs Restante**:\n",
    "       - Treine um modelo XGBoost para distinguir entre sentimentos negativos e todos os outros sentimentos (não-negativos).\n",
    "     - **XGBoost para Classificação Positivo vs Neutro**:\n",
    "       - Treine um modelo XGBoost para diferenciar entre sentimentos positivos e neutros, excluindo frases com sentimentos negativos.\n",
    "\n",
    "3. **API: Processamento de Texto**:\n",
    "   - Esta seção integra todos os processos anteriores para analisar e classificar uma frase individual. Inclui as etapas de pré-processamento, vetorização (utilizando os vetores de 100 dimensões) e classificação. A API final permite determinar se a frase é negativa, positiva ou neutra, aplicando o pipeline de classificação desenvolvido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções para o uso do Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar o notebook a seguir corretamente, é necessário seguir alguns passos prévios. Primeiro, baixe os datasets disponíveis no link a seguir: [Datasets](https://drive.google.com/drive/folders/1bm6iQenyZ63gbw_s7j0md8UaHP84O0Vn?usp=sharing) (não é necessário todos, a princípio, somente os citados abaixo). Esses datasets não estão incluídos no notebook devido ao seu tamanho, por isso são ignorados pelo arquivo `.gitignore`. Todos eles devem ser colocados na pasta [`data`](data), localizada em `src/Notebook/Api_Sprint_4/data`.\n",
    "\n",
    "Os arquivos necessários são:\n",
    "\n",
    "- **cc.en.300.bin**: [Download](https://drive.google.com/file/d/1eM6TfyZIUt6YlCOus5C8nCU1UkmqRu2V/view?usp=sharing) - arquivo bin com palavras pré-treinadas para o FastText.\n",
    "- **tweets_uber.csv**: [Baixar Arquivo](https://drive.google.com/file/d/1cot0O9YoNDQa6bPpVgboRhMIOOsoh2rI/view?usp=sharing) - Este arquivo contém uma coleção de tweets relacionados à Uber. Vale destacar que este dataset não é o original fornecido pela *Uber*, pois foi modificado para fins de análise.\n",
    "\n",
    "Todos são necessários para rodar as células seguintes.\n",
    "\n",
    "Outras recomendações:\n",
    "\n",
    "- Mantenha o C++ do seu computador atualizado, pois determinadas bibliotecas são compiladas com essa linguagem.\n",
    "- Caso queira conferir as versões mais atualizaas das bibliotecas, elas estão detalhadas no aquivo [``requirements.txt``](../../requirements.txt). Para usar este arquivo, basta rodar no terminal o comando: ``pip install -r requirements.txt``. Porém, a princípio, é possível ter acesso a todas as bilioteca ao rodar a célula de \"Instalação e Importação das Bibliotecas\" deste Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções abaixo adaptam e integram as etapas de pré-processamento de texto, vetorização utilizando FastText, e qualificação do modelo através do XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de \"Bibliotecas Mágicas\"\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emoji\n",
    "%pip install wordcloud\n",
    "%pip install emot\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "%pip install scipy==1.11\n",
    "%pip install gensim\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install xgboost\n",
    "%pip install --upgrade scikit-learn\n",
    "%pip install --upgrade imbalanced-learn\n",
    "%pip install fasttext\n",
    "%pip install fasttext-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import os\n",
    "import pickle\n",
    "import fasttext\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Certifique-se de ter baixado os pacotes necessários do NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetores de Palavras Pré-Treinadas\n",
    "\n",
    "Para otimizar o desempenho e a eficiência do modelo, o arquivo de vetores de palavras pré-treinadas fornecido pelo Facebook foi modificado. Originalmente, os vetores possuem 300 dimensões. Utilizando a vetorização FastText, reduzimos esses vetores para apenas 100 dimensões. Essa transformação reduz significativamente a complexidade computacional e o espaço de armazenamento, mantendo, ao mesmo tempo, uma representação adequada das palavras para o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_fasttext_dimensions(input_model_path, output_file_path, num_dimensions=100):\n",
    "    \"\"\"\n",
    "    Reduz as dimensões dos vetores de um modelo fastText, mantendo apenas as primeiras num_dimensions dimensões,\n",
    "    e salva o resultado em um arquivo de texto no formato .vec.\n",
    "\n",
    "    Args:\n",
    "    input_model_path (str): Caminho para o modelo fastText original com 300 dimensões.\n",
    "    output_file_path (str): Caminho para salvar o novo arquivo com dimensões reduzidas.\n",
    "    num_dimensions (int): Número de dimensões a manter. O padrão é 100.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Carregar o modelo fastText completo\n",
    "    model = fasttext.load_model(input_model_path)\n",
    "    \n",
    "    words = model.get_words()\n",
    "    \n",
    "    # Abrir o arquivo para escrever os vetores reduzidos com codificação UTF-8\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        # Escrever a linha de cabeçalho com o número de palavras e dimensões\n",
    "        f.write(f\"{len(words)} {num_dimensions}\\n\")\n",
    "        \n",
    "        for word in words:\n",
    "            # Obter o vetor do modelo original\n",
    "            original_vector = model.get_word_vector(word)\n",
    "            \n",
    "            # Manter apenas as primeiras num_dimensions dimensões\n",
    "            reduced_vector = original_vector[:num_dimensions]\n",
    "            \n",
    "            # Converter o vetor reduzido para uma string de valores\n",
    "            vector_str = ' '.join(map(str, reduced_vector))\n",
    "            \n",
    "            # Escrever a palavra e o vetor reduzido no arquivo\n",
    "            f.write(f\"{word} {vector_str}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "input_model_path = './data/cc.en.300.bin'  # Caminho para o modelo fastText original com 300 dimensões\n",
    "output_file_path = './data/cc.en.100.vec'  # Caminho para salvar o novo arquivo com dimensões reduzidas\n",
    "\n",
    "reduce_fasttext_dimensions(input_model_path, output_file_path, num_dimensions=100)\n",
    "\n",
    "print(f\"Novo arquivo com vetores reduzidos salvo em {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_vectorize_csv(csv_path, vec_path, output_path):\n",
    "    \"\"\"\n",
    "    Carrega os embeddings do fastText a partir de um arquivo .vec, vetoriza a coluna 'processed_text' de um CSV,\n",
    "    e salva o resultado em um novo CSV.\n",
    "\n",
    "    Args:\n",
    "    csv_path (str): Caminho para o arquivo CSV com os dados de entrada.\n",
    "    vec_path (str): Caminho para o arquivo .vec com os embeddings.\n",
    "    output_path (str): Caminho para salvar o arquivo CSV com os resultados.\n",
    "    \"\"\"\n",
    "    # Verificar se os arquivos existem\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Arquivo CSV não encontrado no caminho: {csv_path}\")\n",
    "    if not os.path.exists(vec_path):\n",
    "        raise FileNotFoundError(f\"Arquivo de embeddings não encontrado no caminho: {vec_path}\")\n",
    "    \n",
    "    # Carregar os embeddings do arquivo .vec\n",
    "    print(\"Carregando os embeddings...\")\n",
    "    embeddings = {}\n",
    "    with open(vec_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "\n",
    "    # Função para converter texto em vetor\n",
    "    def text_to_vector(text, embeddings):\n",
    "        words = text.split()\n",
    "        word_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(len(next(iter(embeddings.values()))))  # Tamanho do vetor de embeddings\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    # Carregar o arquivo CSV\n",
    "    print(\"Carregando o arquivo CSV...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Vetorizar a coluna 'processed_text'\n",
    "    print(\"Vetorizar a coluna 'processed_text'...\")\n",
    "    vectors = df['processed_text'].apply(lambda text: text_to_vector(text, embeddings))\n",
    "    \n",
    "    # Converter os vetores para um DataFrame\n",
    "    vectors_df = pd.DataFrame(vectors.tolist(), index=df.index)\n",
    "    \n",
    "    # Combinar com a coluna 'sentiment'\n",
    "    result_df = pd.concat([df['sentiment'], vectors_df], axis=1)\n",
    "    \n",
    "    # Salvar o DataFrame resultante em um novo arquivo CSV\n",
    "    print(\"Salvando o arquivo resultante...\")\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Vetorizações salvas em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './data/tweets_uber.csv'\n",
    "vec_path = './data/cc.en.100.vec'\n",
    "output_path = './data/tweets_uber_vectorized.csv'\n",
    "\n",
    "process_and_vectorize_csv(csv_path, vec_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo\n",
    "\n",
    "O código do modelo foi adaptado a partir do arquivo [Model_improvements](../Api_Sprint_4/Model_improvements.ipynb) para realizar as seguintes melhorias:\n",
    "\n",
    "- **Redução do Conjunto de Treinamento**: O treinamento agora utiliza apenas 100 vetores, permitindo uma execução mais rápida e eficiente para fins de testes.\n",
    "- **Suporte a Arquivos .vec**: O modelo foi ajustado para aceitar arquivos de vetor no formato `.vec` ao invés de `.bin`, oferecendo maior flexibilidade no uso de diferentes fontes de dados.\n",
    "- **Persistência do Modelo**: Implementação de uma funcionalidade para salvar o modelo treinado, facilitando o armazenamento e a reutilização do modelo em futuras previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_negative_vs_rest(data_for_training_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária (negativa ou não negativa) e salva o modelo treinado em um arquivo .pkl.\n",
    "\n",
    "    Inputs:\n",
    "        data_for_training_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        model_save_path (str): Caminho para salvar o modelo treinado como um arquivo .pkl.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados de treinamento a partir de um arquivo CSV\n",
    "        data_for_training = pd.read_csv(data_for_training_path)\n",
    "\n",
    "        if 'id' in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=['id'])\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data_for_training.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data_for_training.drop('sentiment', axis=1)\n",
    "        y = data_for_training['sentiment']\n",
    "\n",
    "        # Reclassificar as classes para binário\n",
    "        y = y.apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Aplicar SMOTE para balancear as classes no conjunto de treinamento\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Configurar o modelo XGBoost\n",
    "        model = xgb.XGBClassifier(max_depth=6, eta=0.3, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        # Realizar validação cruzada\n",
    "        cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=5, scoring='f1_weighted')\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Verificar os rótulos únicos nas previsões\n",
    "        print(\"Rótulos únicos nas previsões:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "            'mean_cross_val_f1': cv_scores.mean(),\n",
    "            'std_cross_val_f1': cv_scores.std()\n",
    "        }\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        # Salvar o modelo treinado em um arquivo .pkl\n",
    "        with open(model_save_path, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da Função:\n",
    "metrics = train_xgboost_model_negative_vs_rest('./data/tweets_uber_vectorized.csv', './models/xgboost_negative_vs_rest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_positive_vs_neutral(file_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária (neutra ou positiva), excluindo frases negativas,\n",
    "    e salva o modelo treinado em um arquivo .pkl.\n",
    "\n",
    "    Inputs:\n",
    "        file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        model_save_path (str): Caminho para salvar o modelo treinado como um arquivo .pkl.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Filtrar para remover frases negativas\n",
    "        data = data[data['sentiment'] != -1]\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data.drop('sentiment', axis=1)\n",
    "        y = data['sentiment']\n",
    "\n",
    "        # Reclassificar as classes para binário\n",
    "        y = y.apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Aplicar SMOTE para balancear as classes no conjunto de treinamento\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Configurar o modelo XGBoost\n",
    "        model = xgb.XGBClassifier(max_depth=6, eta=0.3, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        # Realizar validação cruzada\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Verificar os rótulos únicos nas previsões\n",
    "        print(\"Rótulos únicos nas previsões:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "            'mean_cross_val_f1': cv_scores.mean(),\n",
    "            'std_cross_val_f1': cv_scores.std()\n",
    "        }\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        # Salvar o modelo treinado em um arquivo .pkl\n",
    "        with open(model_save_path, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da função:\n",
    "metrics = train_xgboost_model_positive_vs_neutral('./data/tweets_uber_vectorized.csv', './models/xgboost_positive_vs_neutral.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API: Processamento de Texto\n",
    "\n",
    "Os códigos de pré-processamento, vetorização e aplicação do modelo foram reorganizados e otimizados para processar uma única frase por vez. Esta abordagem simplificada permite que a API:\n",
    "\n",
    "- **Pré-Processamento**: Limpar e preparar a frase para vetorização, removendo ruídos e aplicando técnicas de normalização.\n",
    "- **Vetorização**: Converter a frase em vetores utilizando a vetorização FastText adaptada, agora com 100 dimensões, garantindo uma representação compacta e eficaz.\n",
    "- **Modelo**: Aplicar o modelo treinado à frase vetorizada, fornecendo resultados rápidos e precisos para análises de sentimento ou outras classificações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter emojis em palavras\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Função para limpar o texto e substituir apóstrofos por espaços\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove tags HTML\n",
    "    text = re.sub(r'\\'', ' ', text)  # Substitui apóstrofos por espaços\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove caracteres especiais e números, exceto espaços\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Corrige espaços extras\n",
    "    return text\n",
    "\n",
    "# Função para tokenizar o texto e converter para minúsculas\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Função para remover stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Função para aplicar lematização\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Lista de palavras a serem mantidas\n",
    "words_to_keep = ['uber']\n",
    "\n",
    "# Função para corrigir a ortografia, mantendo palavras específicas\n",
    "def correct_spelling(text):\n",
    "    for i, word in enumerate(words_to_keep):\n",
    "        text = text.replace(word, f'PLACEHOLDER_{i}')\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    for i, word in enumerate(words_to_keep):\n",
    "        corrected_text = corrected_text.replace(f'PLACEHOLDER_{i}', word)\n",
    "    return corrected_text\n",
    "\n",
    "# Função principal de pré-processamento para uma única frase\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Converte para minúsculas\n",
    "    text = convert_emojis(text)\n",
    "    text = clean_text(text)\n",
    "    text = correct_spelling(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    lemmatized_tokens = apply_lemmatization(tokens)\n",
    "    return ' '.join(lemmatized_tokens)  # Retorna a frase lematizada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "example_text = \"I love Uber's service! 🚗 #Awesome\"\n",
    "processed_text = preprocess_text(example_text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(sentence, vec_path):\n",
    "    \"\"\"\n",
    "    Carrega os embeddings do fastText a partir de um arquivo .vec e converte uma frase em um vetor de embeddings.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): Frase de entrada.\n",
    "    vec_path (str): Caminho para o arquivo .vec com os embeddings.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Vetor que representa a frase.\n",
    "    \"\"\"\n",
    "    # Verifica se o arquivo .vec existe\n",
    "    if not os.path.exists(vec_path):\n",
    "        raise FileNotFoundError(f\"Arquivo de embeddings não encontrado no caminho: {vec_path}\")\n",
    "    \n",
    "    # Carrega os embeddings do arquivo .vec\n",
    "    embeddings = {}\n",
    "    with open(vec_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    # Converte a frase em um vetor usando a média dos embeddings das palavras\n",
    "    words = sentence.split()\n",
    "    word_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        # Retorna um vetor de zeros se nenhuma palavra da frase estiver nos embeddings\n",
    "        return np.zeros(len(next(iter(embeddings.values()))))\n",
    "    \n",
    "    # Calcula a média dos vetores das palavras\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "vec_path = './data/cc.en.100.vec'\n",
    "sentence = \"love using uber automobile visit\"\n",
    "vector = text_to_vector(sentence, vec_path)\n",
    "\n",
    "print(\"Vetor da frase:\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Negativo vs Positivos e Neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificar o texto como negativo vs resto\n",
    "def classify_text_neg_rest(vectorized_text, model_file):\n",
    "    \"\"\"\n",
    "    Classifica uma frase como negativa (0) ou não negativa (1) usando um modelo XGBoost treinado.\n",
    "\n",
    "    Inputs:\n",
    "        vectorized_text (np.ndarray): Vetor representando a frase vetorizada.\n",
    "        model_file (str): Caminho para o arquivo pickle contendo o modelo XGBoost treinado.\n",
    "\n",
    "    Output: int: Classificação da frase (0: negativa, 1: não negativa).\n",
    "    \"\"\"\n",
    "    # Carregar o modelo XGBoost treinado\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    # Verificar se o vetor é unidimensional e transformar em um array 2D se necessário\n",
    "    if len(vectorized_text.shape) == 1:\n",
    "        vectorized_text = vectorized_text.reshape(1, -1)\n",
    "\n",
    "    # Fazer a predição usando o modelo carregado\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    \n",
    "    return int(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Positivos vs Neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificar o texto como positivo vs neutro\n",
    "def classify_text_pos_neutral(vectorized_text, model_file):\n",
    "    \"\"\"\n",
    "    Classifica uma frase como positiva (2) ou neutra (1) usando um modelo XGBoost treinado.\n",
    "\n",
    "    Inputs:\n",
    "        vectorized_text (np.ndarray): Vetor representando a frase vetorizada.\n",
    "        model_file (str): Caminho para o arquivo pickle contendo o modelo XGBoost treinado.\n",
    "\n",
    "    Output: int: Classificação da frase (2: positiva, 1: neutra).\n",
    "    \"\"\"\n",
    "    # Carregar o modelo XGBoost treinado\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    # Verificar se o vetor é unidimensional e transformar em um array 2D se necessário\n",
    "    if len(vectorized_text.shape) == 1:\n",
    "        vectorized_text = vectorized_text.reshape(1, -1)\n",
    "\n",
    "    # Fazer a predição usando o modelo carregado\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    \n",
    "    return int(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junção das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_classify_text(text, fasttext_model_path, model_neg_vs_rest_path, model_pos_vs_neutral_path):\n",
    "    # Pré-processar o texto\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Vetorizar o texto\n",
    "    vectorized_text = text_to_vector(preprocessed_text, fasttext_model_path)\n",
    "    \n",
    "    # Classificar o texto como negativo vs resto\n",
    "    neg_vs_rest = classify_text_neg_rest(vectorized_text, model_neg_vs_rest_path)\n",
    "    \n",
    "    if neg_vs_rest == 0:  # Se não for negativo, classificar entre positivo e neutro\n",
    "        pos_vs_neutral = classify_text_pos_neutral(vectorized_text, model_pos_vs_neutral_path)\n",
    "        return 1 if pos_vs_neutral == 1 else 0  # 1: positivo, 0: neutro\n",
    "    else:\n",
    "        return -1  # -1: negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "fasttext_model_path = './data/cc.en.100.vec'\n",
    "model_neg_vs_rest_path = './models/xgboost_negative_vs_rest.pkl'\n",
    "model_pos_vs_neutral_path = './models/xgboost_positive_vs_neutral.pkl'\n",
    "\n",
    "text = \"I love Uber's service! 🚗 #Awesome\"\n",
    "\n",
    "result = process_and_classify_text(text, fasttext_model_path, model_neg_vs_rest_path, model_pos_vs_neutral_path)\n",
    "print(f\"Classificação: {result} (-1: Negativo, 0: Neutro, 1: Positivo)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
