{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo do Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook √© organizado em tr√™s se√ß√µes principais:\n",
    "\n",
    "1. **Vetores de Palavras Pr√©-Treinados**:\n",
    "   - Carregue os vetores de palavras FastText de 300 dimens√µes e reduza-os para 100 dimens√µes. Esta redu√ß√£o otimiza o uso de espa√ßo e melhora o desempenho do modelo sem perder muita informa√ß√£o contextual.\n",
    "\n",
    "2. **Treinamento do Modelo**:\n",
    "   - Treine e salve modelos XGBoost utilizando os vetores de 100 dimens√µes. Esta se√ß√£o √© subdividida em dois modelos distintos:\n",
    "     - **XGBoost para Classifica√ß√£o Negativo vs Restante**:\n",
    "       - Treine um modelo XGBoost para distinguir entre sentimentos negativos e todos os outros sentimentos (n√£o-negativos).\n",
    "     - **XGBoost para Classifica√ß√£o Positivo vs Neutro**:\n",
    "       - Treine um modelo XGBoost para diferenciar entre sentimentos positivos e neutros, excluindo frases com sentimentos negativos.\n",
    "\n",
    "3. **API: Processamento de Texto**:\n",
    "   - Esta se√ß√£o integra todos os processos anteriores para analisar e classificar uma frase individual. Inclui as etapas de pr√©-processamento, vetoriza√ß√£o (utilizando os vetores de 100 dimens√µes) e classifica√ß√£o. A API final permite determinar se a frase √© negativa, positiva ou neutra, aplicando o pipeline de classifica√ß√£o desenvolvido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instru√ß√µes para o uso do Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar o notebook a seguir corretamente, √© necess√°rio seguir alguns passos pr√©vios. Primeiro, baixe os datasets dispon√≠veis no link a seguir: [Datasets](https://drive.google.com/drive/folders/1bm6iQenyZ63gbw_s7j0md8UaHP84O0Vn?usp=sharing) (n√£o √© necess√°rio todos, a princ√≠pio, somente os citados abaixo). Esses datasets n√£o est√£o inclu√≠dos no notebook devido ao seu tamanho, por isso s√£o ignorados pelo arquivo `.gitignore`. Todos eles devem ser colocados na pasta [`data`](data), localizada em `src/Notebook/Api_Sprint_4/data`.\n",
    "\n",
    "Os arquivos necess√°rios s√£o:\n",
    "\n",
    "- **cc.en.300.bin**: [Download](https://drive.google.com/file/d/1eM6TfyZIUt6YlCOus5C8nCU1UkmqRu2V/view?usp=sharing) - arquivo bin com palavras pr√©-treinadas para o FastText.\n",
    "- **tweets_uber.csv**: [Baixar Arquivo](https://drive.google.com/file/d/1cot0O9YoNDQa6bPpVgboRhMIOOsoh2rI/view?usp=sharing) - Este arquivo cont√©m uma cole√ß√£o de tweets relacionados √† Uber. Vale destacar que este dataset n√£o √© o original fornecido pela *Uber*, pois foi modificado para fins de an√°lise.\n",
    "\n",
    "Todos s√£o necess√°rios para rodar as c√©lulas seguintes.\n",
    "\n",
    "Outras recomenda√ß√µes:\n",
    "\n",
    "- Mantenha o C++ do seu computador atualizado, pois determinadas bibliotecas s√£o compiladas com essa linguagem.\n",
    "- Caso queira conferir as vers√µes mais atualizaas das bibliotecas, elas est√£o detalhadas no aquivo [``requirements.txt``](../../requirements.txt). Para usar este arquivo, basta rodar no terminal o comando: ``pip install -r requirements.txt``. Por√©m, a princ√≠pio, √© poss√≠vel ter acesso a todas as bilioteca ao rodar a c√©lula de \"Instala√ß√£o e Importa√ß√£o das Bibliotecas\" deste Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As fun√ß√µes abaixo adaptam e integram as etapas de pr√©-processamento de texto, vetoriza√ß√£o utilizando FastText, e qualifica√ß√£o do modelo atrav√©s do XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importa√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de \"Bibliotecas M√°gicas\"\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emoji\n",
    "%pip install wordcloud\n",
    "%pip install emot\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "%pip install scipy==1.11\n",
    "%pip install gensim\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install xgboost\n",
    "%pip install --upgrade scikit-learn\n",
    "%pip install --upgrade imbalanced-learn\n",
    "%pip install fasttext\n",
    "%pip install fasttext-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import os\n",
    "import pickle\n",
    "import fasttext\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Certifique-se de ter baixado os pacotes necess√°rios do NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetores de Palavras Pr√©-Treinadas\n",
    "\n",
    "Para otimizar o desempenho e a efici√™ncia do modelo, o arquivo de vetores de palavras pr√©-treinadas fornecido pelo Facebook foi modificado. Originalmente, os vetores possuem 300 dimens√µes. Utilizando a vetoriza√ß√£o FastText, reduzimos esses vetores para apenas 100 dimens√µes. Essa transforma√ß√£o reduz significativamente a complexidade computacional e o espa√ßo de armazenamento, mantendo, ao mesmo tempo, uma representa√ß√£o adequada das palavras para o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_fasttext_dimensions(input_model_path, output_file_path, num_dimensions=100):\n",
    "    \"\"\"\n",
    "    Reduz as dimens√µes dos vetores de um modelo fastText, mantendo apenas as primeiras num_dimensions dimens√µes,\n",
    "    e salva o resultado em um arquivo de texto no formato .vec.\n",
    "\n",
    "    Args:\n",
    "    input_model_path (str): Caminho para o modelo fastText original com 300 dimens√µes.\n",
    "    output_file_path (str): Caminho para salvar o novo arquivo com dimens√µes reduzidas.\n",
    "    num_dimensions (int): N√∫mero de dimens√µes a manter. O padr√£o √© 100.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Carregar o modelo fastText completo\n",
    "    model = fasttext.load_model(input_model_path)\n",
    "    \n",
    "    words = model.get_words()\n",
    "    \n",
    "    # Abrir o arquivo para escrever os vetores reduzidos com codifica√ß√£o UTF-8\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        # Escrever a linha de cabe√ßalho com o n√∫mero de palavras e dimens√µes\n",
    "        f.write(f\"{len(words)} {num_dimensions}\\n\")\n",
    "        \n",
    "        for word in words:\n",
    "            # Obter o vetor do modelo original\n",
    "            original_vector = model.get_word_vector(word)\n",
    "            \n",
    "            # Manter apenas as primeiras num_dimensions dimens√µes\n",
    "            reduced_vector = original_vector[:num_dimensions]\n",
    "            \n",
    "            # Converter o vetor reduzido para uma string de valores\n",
    "            vector_str = ' '.join(map(str, reduced_vector))\n",
    "            \n",
    "            # Escrever a palavra e o vetor reduzido no arquivo\n",
    "            f.write(f\"{word} {vector_str}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "input_model_path = './data/cc.en.300.bin'  # Caminho para o modelo fastText original com 300 dimens√µes\n",
    "output_file_path = './data/cc.en.100.vec'  # Caminho para salvar o novo arquivo com dimens√µes reduzidas\n",
    "\n",
    "reduce_fasttext_dimensions(input_model_path, output_file_path, num_dimensions=100)\n",
    "\n",
    "print(f\"Novo arquivo com vetores reduzidos salvo em {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_vectorize_csv(csv_path, vec_path, output_path):\n",
    "    \"\"\"\n",
    "    Carrega os embeddings do fastText a partir de um arquivo .vec, vetoriza a coluna 'processed_text' de um CSV,\n",
    "    e salva o resultado em um novo CSV.\n",
    "\n",
    "    Args:\n",
    "    csv_path (str): Caminho para o arquivo CSV com os dados de entrada.\n",
    "    vec_path (str): Caminho para o arquivo .vec com os embeddings.\n",
    "    output_path (str): Caminho para salvar o arquivo CSV com os resultados.\n",
    "    \"\"\"\n",
    "    # Verificar se os arquivos existem\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Arquivo CSV n√£o encontrado no caminho: {csv_path}\")\n",
    "    if not os.path.exists(vec_path):\n",
    "        raise FileNotFoundError(f\"Arquivo de embeddings n√£o encontrado no caminho: {vec_path}\")\n",
    "    \n",
    "    # Carregar os embeddings do arquivo .vec\n",
    "    print(\"Carregando os embeddings...\")\n",
    "    embeddings = {}\n",
    "    with open(vec_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "\n",
    "    # Fun√ß√£o para converter texto em vetor\n",
    "    def text_to_vector(text, embeddings):\n",
    "        words = text.split()\n",
    "        word_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(len(next(iter(embeddings.values()))))  # Tamanho do vetor de embeddings\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    # Carregar o arquivo CSV\n",
    "    print(\"Carregando o arquivo CSV...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Vetorizar a coluna 'processed_text'\n",
    "    print(\"Vetorizar a coluna 'processed_text'...\")\n",
    "    vectors = df['processed_text'].apply(lambda text: text_to_vector(text, embeddings))\n",
    "    \n",
    "    # Converter os vetores para um DataFrame\n",
    "    vectors_df = pd.DataFrame(vectors.tolist(), index=df.index)\n",
    "    \n",
    "    # Combinar com a coluna 'sentiment'\n",
    "    result_df = pd.concat([df['sentiment'], vectors_df], axis=1)\n",
    "    \n",
    "    # Salvar o DataFrame resultante em um novo arquivo CSV\n",
    "    print(\"Salvando o arquivo resultante...\")\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Vetoriza√ß√µes salvas em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './data/tweets_uber.csv'\n",
    "vec_path = './data/cc.en.100.vec'\n",
    "output_path = './data/tweets_uber_vectorized.csv'\n",
    "\n",
    "process_and_vectorize_csv(csv_path, vec_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo\n",
    "\n",
    "O c√≥digo do modelo foi adaptado a partir do arquivo [Model_improvements](../Api_Sprint_4/Model_improvements.ipynb) para realizar as seguintes melhorias:\n",
    "\n",
    "- **Redu√ß√£o do Conjunto de Treinamento**: O treinamento agora utiliza apenas 100 vetores, permitindo uma execu√ß√£o mais r√°pida e eficiente para fins de testes.\n",
    "- **Suporte a Arquivos .vec**: O modelo foi ajustado para aceitar arquivos de vetor no formato `.vec` ao inv√©s de `.bin`, oferecendo maior flexibilidade no uso de diferentes fontes de dados.\n",
    "- **Persist√™ncia do Modelo**: Implementa√ß√£o de uma funcionalidade para salvar o modelo treinado, facilitando o armazenamento e a reutiliza√ß√£o do modelo em futuras previs√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_negative_vs_rest(data_for_training_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classifica√ß√£o bin√°ria (negativa ou n√£o negativa) e salva o modelo treinado em um arquivo .pkl.\n",
    "\n",
    "    Inputs:\n",
    "        data_for_training_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        model_save_path (str): Caminho para salvar o modelo treinado como um arquivo .pkl.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicion√°rio contendo as m√©tricas de avalia√ß√£o do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados de treinamento a partir de um arquivo CSV\n",
    "        data_for_training = pd.read_csv(data_for_training_path)\n",
    "\n",
    "        if 'id' in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=['id'])\n",
    "\n",
    "        # Verificar a presen√ßa da coluna 'sentiment'\n",
    "        if 'sentiment' not in data_for_training.columns:\n",
    "            raise KeyError(\"'sentiment' n√£o encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data_for_training.drop('sentiment', axis=1)\n",
    "        y = data_for_training['sentiment']\n",
    "\n",
    "        # Reclassificar as classes para bin√°rio\n",
    "        y = y.apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Aplicar SMOTE para balancear as classes no conjunto de treinamento\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Configurar o modelo XGBoost\n",
    "        model = xgb.XGBClassifier(max_depth=6, eta=0.3, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        # Realizar valida√ß√£o cruzada\n",
    "        cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=5, scoring='f1_weighted')\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Fazer previs√µes\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Verificar os r√≥tulos √∫nicos nas previs√µes\n",
    "        print(\"R√≥tulos √∫nicos nas previs√µes:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relat√≥rio de classifica√ß√£o\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular m√©tricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "            'mean_cross_val_f1': cv_scores.mean(),\n",
    "            'std_cross_val_f1': cv_scores.std()\n",
    "        }\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        # Salvar o modelo treinado em um arquivo .pkl\n",
    "        with open(model_save_path, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da Fun√ß√£o:\n",
    "metrics = train_xgboost_model_negative_vs_rest('./data/tweets_uber_vectorized.csv', './models/xgboost_negative_vs_rest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_positive_vs_neutral(file_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classifica√ß√£o bin√°ria (neutra ou positiva), excluindo frases negativas,\n",
    "    e salva o modelo treinado em um arquivo .pkl.\n",
    "\n",
    "    Inputs:\n",
    "        file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        model_save_path (str): Caminho para salvar o modelo treinado como um arquivo .pkl.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicion√°rio contendo as m√©tricas de avalia√ß√£o do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Verificar a presen√ßa da coluna 'sentiment'\n",
    "        if 'sentiment' not in data.columns:\n",
    "            raise KeyError(\"'sentiment' n√£o encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Filtrar para remover frases negativas\n",
    "        data = data[data['sentiment'] != -1]\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data.drop('sentiment', axis=1)\n",
    "        y = data['sentiment']\n",
    "\n",
    "        # Reclassificar as classes para bin√°rio\n",
    "        y = y.apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Aplicar SMOTE para balancear as classes no conjunto de treinamento\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Configurar o modelo XGBoost\n",
    "        model = xgb.XGBClassifier(max_depth=6, eta=0.3, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        # Realizar valida√ß√£o cruzada\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Fazer previs√µes\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Verificar os r√≥tulos √∫nicos nas previs√µes\n",
    "        print(\"R√≥tulos √∫nicos nas previs√µes:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relat√≥rio de classifica√ß√£o\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular m√©tricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "            'mean_cross_val_f1': cv_scores.mean(),\n",
    "            'std_cross_val_f1': cv_scores.std()\n",
    "        }\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        # Salvar o modelo treinado em um arquivo .pkl\n",
    "        with open(model_save_path, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da fun√ß√£o:\n",
    "metrics = train_xgboost_model_positive_vs_neutral('./data/tweets_uber_vectorized.csv', './models/xgboost_positive_vs_neutral.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API: Processamento de Texto\n",
    "\n",
    "Os c√≥digos de pr√©-processamento, vetoriza√ß√£o e aplica√ß√£o do modelo foram reorganizados e otimizados para processar uma √∫nica frase por vez. Esta abordagem simplificada permite que a API:\n",
    "\n",
    "- **Pr√©-Processamento**: Limpar e preparar a frase para vetoriza√ß√£o, removendo ru√≠dos e aplicando t√©cnicas de normaliza√ß√£o.\n",
    "- **Vetoriza√ß√£o**: Converter a frase em vetores utilizando a vetoriza√ß√£o FastText adaptada, agora com 100 dimens√µes, garantindo uma representa√ß√£o compacta e eficaz.\n",
    "- **Modelo**: Aplicar o modelo treinado √† frase vetorizada, fornecendo resultados r√°pidos e precisos para an√°lises de sentimento ou outras classifica√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©-processamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para converter emojis em palavras\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Fun√ß√£o para limpar o texto e substituir ap√≥strofos por espa√ßos\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove tags HTML\n",
    "    text = re.sub(r'\\'', ' ', text)  # Substitui ap√≥strofos por espa√ßos\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove caracteres especiais e n√∫meros, exceto espa√ßos\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Corrige espa√ßos extras\n",
    "    return text\n",
    "\n",
    "# Fun√ß√£o para tokenizar o texto e converter para min√∫sculas\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Fun√ß√£o para remover stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Fun√ß√£o para aplicar lematiza√ß√£o\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Lista de palavras a serem mantidas\n",
    "words_to_keep = ['uber']\n",
    "\n",
    "# Fun√ß√£o para corrigir a ortografia, mantendo palavras espec√≠ficas\n",
    "def correct_spelling(text):\n",
    "    for i, word in enumerate(words_to_keep):\n",
    "        text = text.replace(word, f'PLACEHOLDER_{i}')\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    for i, word in enumerate(words_to_keep):\n",
    "        corrected_text = corrected_text.replace(f'PLACEHOLDER_{i}', word)\n",
    "    return corrected_text\n",
    "\n",
    "# Fun√ß√£o principal de pr√©-processamento para uma √∫nica frase\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Converte para min√∫sculas\n",
    "    text = convert_emojis(text)\n",
    "    text = clean_text(text)\n",
    "    text = correct_spelling(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    lemmatized_tokens = apply_lemmatization(tokens)\n",
    "    return ' '.join(lemmatized_tokens)  # Retorna a frase lematizada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "example_text = \"I love Uber's service! üöó #Awesome\"\n",
    "processed_text = preprocess_text(example_text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetoriza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(sentence, vec_path):\n",
    "    \"\"\"\n",
    "    Carrega os embeddings do fastText a partir de um arquivo .vec e converte uma frase em um vetor de embeddings.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): Frase de entrada.\n",
    "    vec_path (str): Caminho para o arquivo .vec com os embeddings.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Vetor que representa a frase.\n",
    "    \"\"\"\n",
    "    # Verifica se o arquivo .vec existe\n",
    "    if not os.path.exists(vec_path):\n",
    "        raise FileNotFoundError(f\"Arquivo de embeddings n√£o encontrado no caminho: {vec_path}\")\n",
    "    \n",
    "    # Carrega os embeddings do arquivo .vec\n",
    "    embeddings = {}\n",
    "    with open(vec_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    # Converte a frase em um vetor usando a m√©dia dos embeddings das palavras\n",
    "    words = sentence.split()\n",
    "    word_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        # Retorna um vetor de zeros se nenhuma palavra da frase estiver nos embeddings\n",
    "        return np.zeros(len(next(iter(embeddings.values()))))\n",
    "    \n",
    "    # Calcula a m√©dia dos vetores das palavras\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "vec_path = './data/cc.en.100.vec'\n",
    "sentence = \"love using uber automobile visit\"\n",
    "vector = text_to_vector(sentence, vec_path)\n",
    "\n",
    "print(\"Vetor da frase:\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun√ß√£o Negativo vs Positivos e Neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classificar o texto como negativo vs resto\n",
    "def classify_text_neg_rest(vectorized_text, model_file):\n",
    "    \"\"\"\n",
    "    Classifica uma frase como negativa (0) ou n√£o negativa (1) usando um modelo XGBoost treinado.\n",
    "\n",
    "    Inputs:\n",
    "        vectorized_text (np.ndarray): Vetor representando a frase vetorizada.\n",
    "        model_file (str): Caminho para o arquivo pickle contendo o modelo XGBoost treinado.\n",
    "\n",
    "    Output: int: Classifica√ß√£o da frase (0: negativa, 1: n√£o negativa).\n",
    "    \"\"\"\n",
    "    # Carregar o modelo XGBoost treinado\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    # Verificar se o vetor √© unidimensional e transformar em um array 2D se necess√°rio\n",
    "    if len(vectorized_text.shape) == 1:\n",
    "        vectorized_text = vectorized_text.reshape(1, -1)\n",
    "\n",
    "    # Fazer a predi√ß√£o usando o modelo carregado\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    \n",
    "    return int(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun√ß√£o Positivos vs Neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classificar o texto como positivo vs neutro\n",
    "def classify_text_pos_neutral(vectorized_text, model_file):\n",
    "    \"\"\"\n",
    "    Classifica uma frase como positiva (2) ou neutra (1) usando um modelo XGBoost treinado.\n",
    "\n",
    "    Inputs:\n",
    "        vectorized_text (np.ndarray): Vetor representando a frase vetorizada.\n",
    "        model_file (str): Caminho para o arquivo pickle contendo o modelo XGBoost treinado.\n",
    "\n",
    "    Output: int: Classifica√ß√£o da frase (2: positiva, 1: neutra).\n",
    "    \"\"\"\n",
    "    # Carregar o modelo XGBoost treinado\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    # Verificar se o vetor √© unidimensional e transformar em um array 2D se necess√°rio\n",
    "    if len(vectorized_text.shape) == 1:\n",
    "        vectorized_text = vectorized_text.reshape(1, -1)\n",
    "\n",
    "    # Fazer a predi√ß√£o usando o modelo carregado\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    \n",
    "    return int(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jun√ß√£o das fun√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_classify_text(text, fasttext_model_path, model_neg_vs_rest_path, model_pos_vs_neutral_path):\n",
    "    # Pr√©-processar o texto\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Vetorizar o texto\n",
    "    vectorized_text = text_to_vector(preprocessed_text, fasttext_model_path)\n",
    "    \n",
    "    # Classificar o texto como negativo vs resto\n",
    "    neg_vs_rest = classify_text_neg_rest(vectorized_text, model_neg_vs_rest_path)\n",
    "    \n",
    "    if neg_vs_rest == 0:  # Se n√£o for negativo, classificar entre positivo e neutro\n",
    "        pos_vs_neutral = classify_text_pos_neutral(vectorized_text, model_pos_vs_neutral_path)\n",
    "        return 1 if pos_vs_neutral == 1 else 0  # 1: positivo, 0: neutro\n",
    "    else:\n",
    "        return -1  # -1: negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "fasttext_model_path = './data/cc.en.100.vec'\n",
    "model_neg_vs_rest_path = './models/xgboost_negative_vs_rest.pkl'\n",
    "model_pos_vs_neutral_path = './models/xgboost_positive_vs_neutral.pkl'\n",
    "\n",
    "text = \"I love Uber's service! üöó #Awesome\"\n",
    "\n",
    "result = process_and_classify_text(text, fasttext_model_path, model_neg_vs_rest_path, model_pos_vs_neutral_path)\n",
    "print(f\"Classifica√ß√£o: {result} (-1: Negativo, 0: Neutro, 1: Positivo)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
