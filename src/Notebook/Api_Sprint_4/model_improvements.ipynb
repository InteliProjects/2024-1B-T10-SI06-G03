{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções para o uso do Notebook\n",
    "Este notebook documenta uma série de testes conduzidos para aprimorar nosso modelo de previsão de linguagem natural. Os resultados mais relevantes e suas análises detalhadas estão disponíveis na seção 9 da documentação do projeto, \"Análise dos Modelos\". Para acessar essa seção, clique [aqui](#).\n",
    "\n",
    "Para rodar o notebook a seguir corretamente, é necessário seguir alguns passos prévios. Primeiro, baixe os datasets disponíveis no link a seguir: [Datasets](https://drive.google.com/drive/folders/1bm6iQenyZ63gbw_s7j0md8UaHP84O0Vn?usp=sharing) (não é necessário todos, a princípio, somente os citados abaixo). Esses datasets não estão incluídos no notebook devido ao seu tamanho, por isso são ignorados pelo arquivo `.gitignore`. Todos eles devem ser colocados na pasta [`data`](data), localizada em `src/Notebook/Api_Sprint_4/data`.\n",
    "\n",
    "Os arquivos necessários são:\n",
    "\n",
    "- **processed_text_data.csv**: [Download](https://drive.google.com/file/d/1hCW3pliSKDWKyr0h0tvNBtUgtw9EsE3c/view?usp=drive_link) - dataset pré-pocessado.\n",
    "- **extended_dataset.csv**: [Download](https://docs.google.com/spreadsheets/d/12q2Z3TthaaPSafgpridg4ikgAJiewhA_PuPuS6cMY60/edit?usp=drive_link) - dataset passado pelo pré-processamento e passado por um processo de equilíbrio de classes com dados sintéticos. \n",
    "- **cc.en.300.bin**: [Download](https://drive.google.com/file/d/1eM6TfyZIUt6YlCOus5C8nCU1UkmqRu2V/view?usp=sharing) - arquivo bin com palavras pré-treinadas para o FastText.\n",
    "- **glove.6B.50d.txt**: [Download](https://drive.google.com/open?id=1xOZ1Gilf38-GtQ9u_ko8AB6XOBkt7UA7&usp=drive_copy) - contém palavras pré-treinadas em 50 dimensões.\n",
    "\n",
    "Todos são necessários para rodar as células seguintes.\n",
    "\n",
    "Outras recomendações:\n",
    "\n",
    "- Mantenha o C++ do seu computador atualizado, pois determinadas bibliotecas são compiladas com essa linguagem.\n",
    "- Caso queira conferir as versões mais atualizaas das bibliotecas, elas estão detalhadas no aquivo [``requirements.txt``](../../requirements.txt). Para usar este arquivo, basta rodar no terminal o comando: ``pip install -r requirements.txt``. Porém, a princípio, é possível ter acesso a todas as bilioteca ao rodar a célula de \"Instalação e Importação das Bibliotecas\" deste Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação e Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na seção \"Instalação e Importação das Bibliotecas\", prepara-se o ambiente de codificação configurando e carregando as bibliotecas necessárias para o projeto. Esta seção é importante pelos seguintes motivos:\n",
    "\n",
    "1. **Instalação de Bibliotecas**: Utilizam-se comandos como `%pip install nome_da_biblioteca` para instalar bibliotecas Python que não estão presentes por padrão no ambiente de execução, mas que são essenciais para a execução do código. Este comando mágico oferece uma integração mais direta com o IPython, garantindo que as instalações ocorram no kernel correto do Python utilizado pelo Jupyter.\n",
    "\n",
    "2. **Importação de Bibliotecas**: Após a instalação, as bibliotecas são importadas usando o comando `import`, permitindo o acesso às funções e ferramentas disponibilizadas por elas. Comandos típicos incluem `import numpy as np` e `import pandas as pd`. Esta prática garante que todas as funcionalidades necessárias estejam disponíveis e prontas para uso nas seções subsequentes do notebook.\n",
    "\n",
    "Esta seção é posicionada no início do notebook para assegurar que todas as dependências estejam corretamente configuradas antes de prosseguir com a análise de dados ou modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de Bibliotecas Necessárias\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emot\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install gensim\n",
    "%pip install seaborn\n",
    "%pip install xgboost\n",
    "%pip install scipy==1.11\n",
    "%pip install fasttext\n",
    "%pip install fasttext-wheel\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "%pip install --upgrade scikit-learn\n",
    "%pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report, \n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar recursos necessários do nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook, exploramos diversos testes para determinar a melhor abordagem de vetorização no contexto do modelo de previsão de linguagem natural escolhido na sprint 3, que é o XGBoost. A decisão sobre o modelo foi detalhadamente analisada na seção 7.2 da documentação do projeto, intitulada \"Resultados\".\n",
    "\n",
    "Aqui, focamos na comparação entre diferentes técnicas de vetorização. No decorrer deste notebook, você encontrará testes e avaliações que ajudaram a identificar a vetorização mais eficaz. Na seção \"Extração de Features\", você poderá observar o processo de pré-treinamento das seguintes técnicas de vetorização:\n",
    "\n",
    "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "2. **GloVe com 50 dimensões (Global Vectors for Word Representation)**\n",
    "3. **FastText**\n",
    "4. **Part-of-Speech Tagging**\n",
    "5. **N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O TF-IDF é uma técnica amplamente utilizada para transformar texto em uma representação numérica que pode ser alimentada a um algoritmo de aprendizado de máquina. Esta técnica calcula a frequência de uma palavra em um documento (Term Frequency - TF) e a pondera com a raridade dessa palavra em todos os documentos (Inverse Document Frequency - IDF). O resultado é uma métrica que reflete o quão importante uma palavra é para um documento específico, em relação a todo o corpus. No nosso projeto, o TF-IDF ajudou a capturar a relevância das palavras em contextos específicos, proporcionando uma base sólida para a modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(dataframe_path, text_column):\n",
    "    \"\"\"\n",
    "    Calcula a matriz TF-IDF para um DataFrame e coluna de texto fornecidos.\n",
    "\n",
    "    Input:\n",
    "    dataframe (pd.DataFrame): O DataFrame de entrada contendo os dados de texto.\n",
    "    text_column (str): O nome da coluna contendo o texto processado.\n",
    "\n",
    "    Output:\n",
    "    pd.DataFrame: Um DataFrame contendo a matriz TF-IDF.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = pd.read_csv(dataframe_path)\n",
    "\n",
    "    # Cria um Vetorizador TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Ajusta e transforma o texto processado\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataframe[text_column])\n",
    "\n",
    "    # Converte a matriz TF-IDF para um DataFrame para melhor legibilidade\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return tfidf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uploaded CSV file\n",
    "file_path = './data/processed_text_data.csv'\n",
    "\n",
    "# Calcula a matriz TF-IDF\n",
    "tfidf_df = compute_tfidf(file_path, 'processed_text')\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe é uma técnica de vetorização de palavras que cria vetores densos a partir de grandes corpora de texto. Ele é treinado para capturar a co-ocorrência de palavras em um espaço vetorial, onde palavras semanticamente similares estão próximas. Usamos a versão com 50 dimensões para representar cada palavra em nosso corpus, proporcionando uma representação semântica que vai além das frequências de palavras simples. Essa técnica é especialmente útil para capturar contextos mais sutis e relações semânticas entre palavras que o TF-IDF pode não captar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(file_path):\n",
    "    '''\n",
    "    Carrega o modelo GloVe de um arquivo de texto.\n",
    "\n",
    "    Input:\n",
    "    - file_path (str): Caminho para o arquivo de vetores GloVe.\n",
    "\n",
    "    Output:\n",
    "    - dict: Dicionário onde as chaves são palavras e os valores são vetores.\n",
    "    '''\n",
    "    glove_model = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            glove_model[word] = vector\n",
    "    return glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_to_vectors_glove(csv_input_path, text_column, glove_file_path):\n",
    "    '''\n",
    "    Processa um arquivo CSV, transforma o texto em vetores de palavras usando GloVe, e salva o resultado em um novo arquivo CSV.\n",
    "\n",
    "    Input:\n",
    "    - csv_input_path (str): Caminho para o arquivo CSV de entrada.\n",
    "    - text_column (str): Nome da coluna que contém o texto processado.\n",
    "    - glove_file_path (str): Caminho para o arquivo de vetores GloVe.\n",
    "\n",
    "    Output:\n",
    "    - pd.DataFrame: DataFrame contendo os textos e seus vetores correspondentes divididos em colunas.\n",
    "    '''\n",
    "    # Carregar o modelo GloVe\n",
    "    glove_model = load_glove_model(glove_file_path)\n",
    "\n",
    "    # Função para obter o vetor de uma palavra\n",
    "    def get_word_vector(word):\n",
    "        return glove_model.get(word, np.zeros(50))\n",
    "\n",
    "    # Função para calcular a média dos vetores das palavras em um texto\n",
    "    def average_word_vectors(text):\n",
    "        words = text.split()\n",
    "        if len(words) == 0:\n",
    "            return np.zeros(50)\n",
    "        vectors = np.array([get_word_vector(word) for word in words])\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    # Carregar os dados do arquivo CSV\n",
    "    data = pd.read_csv(csv_input_path)\n",
    "\n",
    "    # Calcular o vetor médio para cada linha de texto\n",
    "    data['vectors'] = data[text_column].apply(average_word_vectors)\n",
    "\n",
    "    # Dividir os vetores em colunas separadas\n",
    "    vector_columns = [f'vector_{i}' for i in range(50)]\n",
    "    vectors_df = pd.DataFrame(data['vectors'].tolist(), columns=vector_columns)\n",
    "\n",
    "    # Concatenar os vetores de volta ao DataFrame original\n",
    "    data = pd.concat([data, vectors_df], axis=1)\n",
    "\n",
    "    # Manter apenas as colunas de sentimento e vetores\n",
    "    result_data = data[['sentiment'] + vector_columns]\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O FastText, desenvolvido pelo Facebook, é uma ferramenta que também gera vetores de palavras, mas com a vantagem adicional de considerar subpalavras e morfologia. Utilizamos vetores pré-treinados do FastText, que oferecem uma representação mais robusta para palavras raras e variações morfológicas, como sufixos e prefixos. Isso melhora a capacidade do modelo de lidar com palavras novas ou raras, capturando nuances que outros métodos podem não conseguir. Esta característica torna o FastText particularmente eficaz em nosso modelo para tratar a diversidade linguística presente nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, model):\n",
    "    \"\"\"\n",
    "    Converte um texto em um vetor usando a média dos embeddings de palavras.\n",
    "\n",
    "    Args:\n",
    "    text (str): Texto de entrada.\n",
    "    model: Modelo fastText carregado.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Vetor que representa o texto.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    word_vectors = [model.get_word_vector(word) for word in words if word in model.words]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.get_dimension())\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def vectorize_text_with_fasttext(dataframe, text_column, fasttext_model_path):\n",
    "    \"\"\"\n",
    "    Converte uma coluna de texto em embeddings usando um modelo fastText.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): DataFrame contendo a coluna de texto.\n",
    "    text_column (str): Nome da coluna de texto a ser convertida.\n",
    "    fasttext_model_path (str): Caminho para o modelo fastText pré-treinado.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Matriz de embeddings onde cada linha corresponde a um texto.\n",
    "    \"\"\"\n",
    "    # Carregar o modelo fastText\n",
    "    if not os.path.exists(fasttext_model_path):\n",
    "        raise FileNotFoundError(f\"Modelo fastText não encontrado no caminho: {fasttext_model_path}\")\n",
    "    fasttext_model = fasttext.load_model(fasttext_model_path)\n",
    "\n",
    "    # Verificar se a coluna de texto existe e está no formato correto\n",
    "    if text_column not in dataframe.columns:\n",
    "        raise KeyError(f\"A coluna '{text_column}' não foi encontrada no DataFrame.\")\n",
    "    if not pd.api.types.is_string_dtype(dataframe[text_column]):\n",
    "        raise ValueError(f\"A coluna '{text_column}' deve ser do tipo string.\")\n",
    "    \n",
    "    # Converter cada texto no dataset para um vetor de embeddings\n",
    "    dataframe['embedding'] = dataframe[text_column].apply(lambda x: text_to_vector(x, fasttext_model))\n",
    "\n",
    "    # Verificar se todos os textos foram convertidos corretamente\n",
    "    if dataframe['embedding'].apply(lambda x: len(x) != fasttext_model.get_dimension()).any():\n",
    "        raise ValueError(\"Alguns textos não foram convertidos corretamente em embeddings.\")\n",
    "\n",
    "    # Extrair as embeddings como matriz de features\n",
    "    X = np.vstack(dataframe['embedding'].values)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Part-of-Speech Tagging (POS Tagging) é uma técnica que atribui rótulos gramaticais a cada palavra em uma frase, indicando sua função, como substantivo, verbo, adjetivo, etc. Treinamos nosso próprio modelo de POS Tagging usando um banco de dados específico para capturar as nuances sintáticas de nosso corpus. Essa técnica é valiosa para entender a estrutura gramatical e pode fornecer insights sobre como diferentes partes do discurso influenciam o significado e o contexto em que as palavras são usadas. A inclusão de informações gramaticais enriquece a representação textual, auxiliando o modelo a diferenciar entre palavras com múltiplos significados dependendo de seu uso gramatical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    \"\"\"\n",
    "    Realiza o POS tagging em um texto e retorna as tags.\n",
    "\n",
    "    Args:\n",
    "    text (str): Texto de entrada.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tags POS.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return [tag for word, tag in pos_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(dataframe, text_column):\n",
    "    \"\"\"\n",
    "    Converte a coluna de texto em features baseadas nas tags de POS.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): DataFrame contendo a coluna de texto.\n",
    "    text_column (str): Nome da coluna de texto a ser convertida.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame com as tags de POS convertidas em features.\n",
    "    \"\"\"\n",
    "    # Realizar POS tagging para cada texto\n",
    "    dataframe['pos_tags'] = dataframe[text_column].apply(pos_tagging)\n",
    "    \n",
    "    # Criar uma lista de todas as tags de POS possíveis\n",
    "    all_tags = [tag for tags in dataframe['pos_tags'] for tag in tags]\n",
    "    unique_tags = list(set(all_tags))\n",
    "\n",
    "    # Converter tags de POS em features de contagem\n",
    "    def pos_tag_counts(tags):\n",
    "        tag_count = Counter(tags)\n",
    "        return [tag_count.get(tag, 0) for tag in unique_tags]\n",
    "\n",
    "    pos_features = dataframe['pos_tags'].apply(pos_tag_counts)\n",
    "    pos_features_df = pd.DataFrame(pos_features.tolist(), columns=[f'pos_{tag}' for tag in unique_tags])\n",
    "\n",
    "    return pos_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os N-grams são sequências contíguas de n itens de um dado texto, onde \"n\" pode variar. Neste projeto, utilizamos um banco de dados próprio para treinar nossos N-grams, capturando padrões de co-ocorrência entre palavras em nosso corpus. Eles são especialmente úteis para capturar a ordem e a proximidade de palavras, oferecendo uma visão mais profunda sobre frases e expressões comuns que podem não ser evidentes em métodos de vetorização baseados em palavras únicas. Por exemplo, a frase “análise de sentimentos” como um bigrama (n=2) pode capturar um contexto mais específico do que as palavras “análise” e “sentimentos” individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(dataframe, text_column, ngram_range=(1,2)):\n",
    "    \"\"\"\n",
    "    Converte uma coluna de texto em features de N-grams usando TF-IDF.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): DataFrame contendo a coluna de texto.\n",
    "    text_column (str): Nome da coluna de texto a ser convertida.\n",
    "    ngram_range (tuple): O intervalo de N para N-grams (exemplo: (1, 2) para unigrams e bigrams).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame com a representação TF-IDF dos N-grams.\n",
    "    \"\"\"\n",
    "    # Cria um Vetorizador TF-IDF para N-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "    # Ajusta e transforma o texto processado\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataframe[text_column])\n",
    "\n",
    "    # Converte a matriz TF-IDF para um DataFrame para melhor legibilidade\n",
    "    ngram_features_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    return ngram_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação dos Datasets para treinamento dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, nos concentramos em criar diversos datasets derivados do \"processed_text_data.csv\" com o objetivo de treinar nossos modelos utilizando diferentes bancos de dados e técnicas de vetorização. A diversidade de datasets permite uma avaliação mais abrangente e robusta dos modelos de aprendizado de máquina. Para isso, exploramos quatro principais conjuntos de dados, cada um com características únicas que ajudam a testar a eficácia do modelo sob diferentes condições:\n",
    "\n",
    "1. **processed_text_data.csv**\n",
    "   Este é o dataset base que contém os textos processados com suas respectivas classificações de sentimento. Ele serve como ponto de partida para todos os experimentos subsequentes. O dataset é usado diretamente para treinar os modelos sem qualquer modificação adicional, proporcionando uma linha de base para comparar os efeitos de outras abordagens de balanceamento.\n",
    "\n",
    "2. **extended_dataset.csv**\n",
    "   Para lidar com o desbalanceamento do dataset original, criamos o \"extended_dataset.csv\". Este dataset foi ampliado utilizando técnicas de Inteligência Artificial para gerar novos dados sintéticos baseados nos dados originais. Com isso, conseguimos igualar a quantidade de exemplos negativos, neutros e positivos, proporcionando um dataset balanceado que permite ao modelo aprender de forma mais equilibrada e eficaz sobre cada classe de sentimento.\n",
    "\n",
    "3. **balanced_dataset_with_undersampling**\n",
    "   Função que cria dataset (normalmente, uma variante do \"extended_dataset.csv\"), onde aplicamos undersampling para balancear o número de exemplos em cada classe. Especificamente, removemos uma quantidade igual de exemplos positivos e neutros para que o número de exemplos seja equivalente ao de exemplos negativos. Esta abordagem é útil para evitar que o modelo aprenda vieses de classes majoritárias, mantendo o foco em dados representativos e relevantes.\n",
    "\n",
    "4. **balanced_dataset_with_oversampling**\n",
    "   Função que cria dataset (normalmente, uma variante do \"extended_dataset.csv\") onde aplica-se oversampling para aumentar o número de exemplos sintéticos em classes com menos exemplos (como para os testes una-se neutros e positivos, seria a classe negativa). Ao contrário do undersampling, que remove exemplos, o oversampling cria novos exemplos para as classes minoritárias. Isso ajuda a garantir que o modelo tenha uma exposição equilibrada a todas as classes durante o treinamento, promovendo uma melhor generalização e precisão.\n",
    "\n",
    "## Abordagem de Diversidade de Testes\n",
    "\n",
    "Ao criar esses diferentes datasets, buscamos maximizar a diversidade de testes para o nosso modelo de previsão de linguagem natural. Cada dataset oferece uma perspectiva única sobre como os modelos podem se comportar com diferentes distribuições de dados e técnicas de vetorização. Essa estratégia permite uma avaliação mais completa e rigorosa, ajudando a identificar a melhor abordagem para nosso problema de classificação de sentimentos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Função a seguir carrega dois arquivos CSV, adiciona a coluna 'sentiment' do primeiro arquivo ao segundo arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentiment_column(vectorizer, dataframe):\n",
    "    \"\"\"\n",
    "    Carrega dois arquivos CSV, adiciona a coluna 'sentiment' do primeiro arquivo\n",
    "    ao segundo arquivo, e salva o resultado em um novo arquivo CSV.\n",
    "\n",
    "    Args:\n",
    "    dataframe (str): Caminho para o arquivo CSV contendo a coluna 'sentiment'.\n",
    "    vectorizer (str): Caminho para o arquivo CSV contendo os vetores de palavras.\n",
    "    \n",
    "    Returns:\n",
    "    vectorizer: Arquivo CSV contendo a coluna 'sentiment' e o contendo os vetores de palavras.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carregar os arquivos CSV\n",
    "    classification_text_data = pd.read_csv(dataframe)\n",
    "\n",
    "    # Selecionar a coluna 'sentiment' do arquivo classification_text_data\n",
    "    sentiment_column = classification_text_data['sentiment']\n",
    "\n",
    "    if 'sentiment' in vectorizer.columns:\n",
    "        vectorizer = vectorizer.drop('sentiment', axis=1)\n",
    "\n",
    "    # Adicionar a coluna 'sentiment' no arquivo vectorizer na primeira coluna (coluna 0)\n",
    "    vectorizer.insert(0, 'sentiment', sentiment_column)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir cria um banco de dados que remove dados positivos e neutros para equilibrar com os negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_dataset_with_undersampled(file_path):\n",
    "    data_balanced = pd.read_csv(file_path)\n",
    "\n",
    "    # Filtrando os dados positivos e neutros\n",
    "    data_positive = data_balanced[data_balanced['sentiment'] == 1]\n",
    "    data_neutral = data_balanced[data_balanced['sentiment'] == 0]\n",
    "\n",
    "    # Selecionando os índices das últimas 665 linhas de cada subset\n",
    "    indices_to_drop = data_positive.tail(665).index.tolist() + \\\n",
    "                    data_neutral.tail(665).index.tolist()\n",
    "\n",
    "    # Removendo as linhas do DataFrame original\n",
    "    data_balanced = data_balanced.drop(indices_to_drop)\n",
    "    return data_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced = balanced_dataset_with_undersampled('./data/extended_dataset.csv')\n",
    "data_balanced.to_csv('./data/extended_dataset_undersampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir pretende equilibrar o dataset ao aumentar sinteticamente os dados negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset_with_oversampled(file_path):\n",
    "    \"\"\"\n",
    "    Balanceia o dataset para que a quantidade de exemplos da classe negativa seja igual à soma das quantidades das classes positiva e neutra.\n",
    "\n",
    "    Inputs:\n",
    "        file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "\n",
    "    Outputs:\n",
    "        pd.DataFrame: DataFrame balanceado com quantidades iguais de classes negativas e a soma de positivas e neutras.\n",
    "    \"\"\"\n",
    "    \n",
    "        # Carregar os dados\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Verificar a presença da coluna 'sentiment'\n",
    "    if 'sentiment' not in data.columns:\n",
    "        raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "    # Separar as classes\n",
    "    data_negative = data[data['sentiment'] == -1]\n",
    "    data_positive_neutral = data[data['sentiment'] != -1]\n",
    "\n",
    "    # Contar a quantidade de exemplos nas classes positivas e neutras combinadas\n",
    "    num_positive_neutral = len(data_positive_neutral)\n",
    "\n",
    "    # Fazer o oversampling da classe negativa para igualar a quantidade das classes positivas e neutras combinadas\n",
    "    data_negative_oversampled = resample(data_negative,\n",
    "                                         replace=True,\n",
    "                                         n_samples=num_positive_neutral,\n",
    "                                         random_state=42)\n",
    "\n",
    "    # Combinar as classes balanceadas\n",
    "    data_balanced = pd.concat([data_negative_oversampled, data_positive_neutral])\n",
    "\n",
    "    return data_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced = balance_dataset_with_oversampled('./data/extended_dataset.csv')\n",
    "data_balanced.to_csv('./data/extended_dataset_oversampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, apresentamos as funções utilizadas para treinar nossos modelos de previsão de sentimentos utilizando o algoritmo XGBoost. Desenvolvemos duas abordagens específicas que dividem o problema de classificação em tarefas binárias. Essa estratégia permite que o modelo se concentre em distinguir entre classes específicas em cada etapa. Abaixo, descrevemos essas duas funções em detalhes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negativo vs não negativo\n",
    "**Descrição:**  \n",
    "Nesta função, o objetivo é prever se uma sentença é negativa ou não. Para isso, os exemplos de sentimentos positivos e neutros são combinados em uma única categoria chamada \"não-negativos\". Isso simplifica o problema em uma tarefa binária, onde o modelo aprende a diferenciar entre sentenças negativas e todas as outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_negative_vs_rest(data_for_training_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária (negativa ou não negativa).\n",
    "\n",
    "    Inputs:\n",
    "        data_for_training_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados de treinamento a partir de um arquivo CSV\n",
    "        data_for_training = pd.read_csv(data_for_training_path)\n",
    "\n",
    "        if 'id' in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=['id'])\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data_for_training.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data_for_training.drop('sentiment', axis=1)\n",
    "        y = data_for_training['sentiment']\n",
    "\n",
    "        # Reclassificar as classes para binário\n",
    "        y = y.apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Aplicar SMOTE para balancear as classes no conjunto de treinamento\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Configurar o modelo XGBoost\n",
    "        model = xgb.XGBClassifier(max_depth=6, eta=0.3, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        # Realizar validação cruzada\n",
    "        cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=5, scoring='f1_weighted')\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Verificar os rótulos únicos nas previsões\n",
    "        print(\"Rótulos únicos nas previsões:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "            'mean_cross_val_f1': cv_scores.mean(),\n",
    "            'std_cross_val_f1': cv_scores.std()\n",
    "        }\n",
    "\n",
    "        print(metrics)\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### positivo vs neutro \n",
    "**Descrição:**  \n",
    "Após determinar que uma sentença não é negativa usando o primeiro modelo, esta função é aplicada para distinguir entre sentenças positivas e neutras. Neste caso, os exemplos de sentimentos negativos são excluídos do dataset, focando o treinamento apenas na diferenciação entre as classes positivas e neutras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_positive_vs_neutral(file_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária (neutra ou positiva), excluindo frases negativas.\n",
    "\n",
    "    Inputs:\n",
    "        file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Filtrar para remover frases negativas\n",
    "        data = data[data['sentiment'] != -1]\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data.drop('sentiment', axis=1)\n",
    "        y = data['sentiment']\n",
    "\n",
    "        # Reclassificar as classes para binário\n",
    "        y = y.apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Aplicar SMOTE para balancear as classes no conjunto de treinamento\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Configurar o modelo XGBoost\n",
    "        model = xgb.XGBClassifier(max_depth=6, eta=0.3, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        # Realizar validação cruzada\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Verificar os rótulos únicos nas previsões\n",
    "        print(\"Rótulos únicos nas previsões:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "            'mean_cross_val_f1': cv_scores.mean(),\n",
    "            'std_cross_val_f1': cv_scores.std()\n",
    "        }\n",
    "\n",
    "        print(metrics)\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, detalhamos o pipeline completo utilizado para testar diferentes técnicas de vetorização em combinação com os variados datasets. O objetivo é identificar a configuração mais eficaz para a previsão de sentimentos usando o modelo XGBoost. Através desta abordagem, buscamos explorar a combinação ideal de vetorização e dataset para maximizar a performance do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_complete_tdidf(dataframe, text_column):\n",
    "    \"\"\"\n",
    "    Pipeline para vetorização via TD-IDF e treino do modelo XGBoost.\n",
    "\n",
    "    Input:\n",
    "    dataframe (pd.DataFrame): O DataFrame de entrada contendo os dados de texto.\n",
    "    text_column (str): O nome da coluna contendo o texto processado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vetorização via TD-IDF\n",
    "    tfidf_df = compute_tfidf(dataframe, text_column)\n",
    "\n",
    "    # Função que cria o banco de dados de treino:\n",
    "    data_for_training = merge_sentiment_column(tfidf_df, dataframe)\n",
    "\n",
    "    # Salvando o DataFrame resultante para treino\n",
    "    data_for_training_path = './data/teste1_td-idf_data.csv'\n",
    "    data_for_training.to_csv(data_for_training_path, index=False)\n",
    "\n",
    "    print(\"Métricas para o Modelo Negativo vs Não Negativo\")\n",
    "    # Função que treina os dados com o modelo TD-IDF e retorna as métricas\n",
    "    metrics_neg_vs_rest = train_xgboost_model_negative_vs_rest(data_for_training_path)\n",
    "\n",
    "    print(\"Métricas para o Modelo Neutro vs Positivo\")\n",
    "    # Função que treina os dados com o modelo TD-IDF e retorna as métricas\n",
    "    metrics_pos_vs_neutral = train_xgboost_model_positive_vs_neutral(data_for_training_path)\n",
    "\n",
    "    return metrics_neg_vs_rest, metrics_pos_vs_neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste feito com os dados pré-processados\n",
    "\n",
    "dataframe = \"./data/processed_text_data.csv\"\n",
    "text_column = \"processed_text\"\n",
    "# model_save_path = \"./models/xgboost_model_td-idf.pkl\"\n",
    "\n",
    "pipeline_complete_tdidf(dataframe, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste feito com os dados balanceados (com quantidades equivalentes de negativos, positivos e neutros)\n",
    "\n",
    "dataframe = \"./data/extended_dataset.csv\"\n",
    "text_column = \"processed_text\"\n",
    "\n",
    "pipeline_complete_tdidf(dataframe, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste feito com os dados balanceados (com quantidades equivalentes de negativos, positivos e neutros) e então equilibrados (diminuindo os positivos/neutros)\n",
    "dataframe = \"./data/extended_dataset_oversampled.csv\"\n",
    "text_column = \"processed_text\"\n",
    "pipeline_complete_tdidf(dataframe, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste feito com os dados balanceados e então equilibrados (aumentando os negativos)\n",
    "dataframe = \"./data/extended_dataset_undersampled.csv\"\n",
    "pipeline_complete_tdidf(dataframe, text_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe 50 dimensões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_complete_glove_undersampling(csv_input_path, text_column, glove_file_path):\n",
    "    \n",
    "    vector_data = process_text_to_vectors_glove(csv_input_path, text_column, glove_file_path)\n",
    "    \n",
    "    vector_data.to_csv('./data/glove_data.csv', index=False)\n",
    "\n",
    "    vector_data = balanced_dataset_with_undersampled('./data/glove_data.csv')\n",
    "\n",
    "    vector_data.to_csv('./data/glove_data.csv', index=False)\n",
    "\n",
    "    print(\"Métricas para o Modelo Negativo vs Não Negativo\")\n",
    "    train_xgboost_model_negative_vs_rest('./data/glove_data.csv')\n",
    "    \n",
    "    print(\"Métricas para o Modelo Neutro vs Positivo\")\n",
    "    train_xgboost_model_positive_vs_neutral('./data/glove_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_input_path = './data/extended_dataset.csv'\n",
    "text_column = 'processed_text'\n",
    "glove_file_path = './data/glove.6B.50d.txt'\n",
    "\n",
    "pipeline_complete_glove_undersampling(csv_input_path, text_column, glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_with_fasttext(data_for_training_path, fasttext_model_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária usando embeddings de fastText.\n",
    "\n",
    "    Inputs:\n",
    "        data_for_training_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        fasttext_model_path (str): Caminho para o modelo pré-treinado fastText.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados de treinamento a partir de um arquivo CSV\n",
    "        data_for_training = pd.read_csv(data_for_training_path)\n",
    "\n",
    "        if 'id' in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=['id'])\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data_for_training.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Realizar a vetorização com fastText\n",
    "        X = vectorize_text_with_fasttext(data_for_training, 'processed_text', fasttext_model_path)\n",
    "\n",
    "        # Remover colunas não numéricas como 'comment' e qualquer outra coluna de texto\n",
    "        non_numeric_columns = data_for_training.select_dtypes(include=[object]).columns\n",
    "        data_for_training.drop(columns=non_numeric_columns, inplace=True)\n",
    "        \n",
    "        # Adicionar os embeddings ao DataFrame original\n",
    "        embeddings_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "        data_for_training = pd.concat([data_for_training.reset_index(drop=True), embeddings_df], axis=1)\n",
    "\n",
    "        # Verificar se só restam colunas numéricas no DataFrame\n",
    "        non_numeric_columns = data_for_training.drop(columns=['sentiment']).select_dtypes(exclude=[np.number]).columns\n",
    "        if len(non_numeric_columns) > 0:\n",
    "            raise ValueError(f\"O DataFrame contém colunas não numéricas: {list(non_numeric_columns)}\")\n",
    "\n",
    "        # Salvar o novo DataFrame vetorizado para ser usado na função original\n",
    "        processed_data_path = data_for_training_path.replace('.csv', '_processed.csv')\n",
    "        data_for_training.to_csv(processed_data_path, index=False)\n",
    "\n",
    "        print(\"Métricas para o Modelo Negativo vs Não Negativo\")\n",
    "        # Chamar a função original do XGBoost com os dados vetorizados\n",
    "        metrics = train_xgboost_model_negative_vs_rest(processed_data_path)\n",
    "\n",
    "        print(\"Métricas para o Modelo Neutro vs Positivo\")\n",
    "        metrics = train_xgboost_model_positive_vs_neutral(processed_data_path)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "data_for_training_path = './data/processed_text_data.csv'\n",
    "fasttext_model_path = './data/cc.en.300.bin'\n",
    "\n",
    "# Treinar e avaliar o modelo com a integração do fastText\n",
    "train_xgboost_model_with_fasttext(data_for_training_path, fasttext_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "data_for_training_path = './data/extended_dataset_undersampled.csv'\n",
    "fasttext_model_path = './data/cc.en.300.bin'\n",
    "\n",
    "# Treinar e avaliar o modelo com a integração do fastText\n",
    "train_xgboost_model_with_fasttext(data_for_training_path, fasttext_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_with_pos(data_for_training_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária usando features de POS tagging.\n",
    "\n",
    "    Inputs:\n",
    "        data_for_training_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados de treinamento a partir de um arquivo CSV\n",
    "        data_for_training = pd.read_csv(data_for_training_path)\n",
    "\n",
    "        if 'id' in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=['id'])\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data_for_training.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Realizar a conversão de POS tags em features\n",
    "        pos_features_df = pos_features(data_for_training, 'processed_text')\n",
    "\n",
    "        # Remover colunas não numéricas como 'comment' e qualquer outra coluna de texto\n",
    "        non_numeric_columns = data_for_training.select_dtypes(include=[object]).columns\n",
    "        data_for_training.drop(columns=non_numeric_columns, inplace=True)\n",
    "\n",
    "        # Adicionar as features de POS ao DataFrame original\n",
    "        data_for_training = pd.concat([data_for_training.reset_index(drop=True), pos_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Verificar se só restam colunas numéricas no DataFrame\n",
    "        non_numeric_columns = data_for_training.drop(columns=['sentiment']).select_dtypes(exclude=[np.number]).columns\n",
    "        if len(non_numeric_columns) > 0:\n",
    "            raise ValueError(f\"O DataFrame contém colunas não numéricas: {list(non_numeric_columns)}\")\n",
    "\n",
    "        # Salvar o novo DataFrame com as features de POS para ser usado na função original\n",
    "        processed_data_path = data_for_training_path.replace('.csv', '_processed_with_pos.csv')\n",
    "        data_for_training.to_csv(processed_data_path, index=False)\n",
    "\n",
    "        print(\"Métricas para o Modelo Negativo vs Não Negativo\")\n",
    "        # Chamar a função original do XGBoost com os dados vetorizados\n",
    "        metrics = train_xgboost_model_negative_vs_rest(processed_data_path)\n",
    "\n",
    "        print(\"Métricas para o Modelo Neutro vs Positivo\")\n",
    "        metrics = train_xgboost_model_positive_vs_neutral(processed_data_path)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "data_for_training_path = './data/processed_text_data.csv'\n",
    "\n",
    "# Treinar e avaliar o modelo com a integração do POS Tagging\n",
    "train_xgboost_model_with_pos(data_for_training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training_path = './data/extended_dataset_undersampled.csv'\n",
    "\n",
    "# Treinar e avaliar o modelo com a integração do POS Tagging\n",
    "train_xgboost_model_with_pos(data_for_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_with_ngrams(data_for_training_path, ngram_range=(1,2)):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost para classificação binária usando features de N-grams.\n",
    "\n",
    "    Inputs:\n",
    "        data_for_training_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        ngram_range (tuple): O intervalo de N para N-grams (exemplo: (1, 2) para unigrams e bigrams).\n",
    "\n",
    "    Outputs:\n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados de treinamento a partir de um arquivo CSV\n",
    "        data_for_training = pd.read_csv(data_for_training_path)\n",
    "\n",
    "        if 'id' in data_for_training.columns:\n",
    "            data_for_training = data_for_training.drop(columns=['id'])\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data_for_training.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Gerar as features de N-grams\n",
    "        ngram_features_df = generate_ngrams(data_for_training, 'processed_text', ngram_range)\n",
    "\n",
    "        # Remover colunas não numéricas como 'comment' e qualquer outra coluna de texto\n",
    "        non_numeric_columns = data_for_training.select_dtypes(include=[object]).columns\n",
    "        data_for_training.drop(columns=non_numeric_columns, inplace=True)\n",
    "\n",
    "        # Adicionar as features de N-grams ao DataFrame original\n",
    "        data_for_training = pd.concat([data_for_training.reset_index(drop=True), ngram_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Verificar se só restam colunas numéricas no DataFrame\n",
    "        non_numeric_columns = data_for_training.drop(columns=['sentiment']).select_dtypes(exclude=[np.number]).columns\n",
    "        if len(non_numeric_columns) > 0:\n",
    "            raise ValueError(f\"O DataFrame contém colunas não numéricas: {list(non_numeric_columns)}\")\n",
    "\n",
    "        # Salvar o novo DataFrame com as features de N-grams para ser usado na função original\n",
    "        processed_data_path = data_for_training_path.replace('.csv', '_processed_with_ngrams.csv')\n",
    "        data_for_training.to_csv(processed_data_path, index=False)\n",
    "\n",
    "        print(\"Métricas para o Modelo Negativo vs Não Negativo\")\n",
    "        # Chamar a função original do XGBoost com os dados vetorizados\n",
    "        metrics = train_xgboost_model_negative_vs_rest(processed_data_path)\n",
    "\n",
    "        print(\"Métricas para o Modelo Neutro vs Positivo\")\n",
    "        metrics = train_xgboost_model_positive_vs_neutral(processed_data_path)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "data_for_training_path = './data/processed_text_data.csv'\n",
    "\n",
    "# Treinar e avaliar o modelo com a integração de N-grams\n",
    "metrics = train_xgboost_model_with_ngrams(data_for_training_path, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "data_for_training_path = './data/extended_dataset_undersampled.csv'\n",
    "\n",
    "# Treinar e avaliar o modelo com a integração de N-grams\n",
    "metrics = train_xgboost_model_with_ngrams(data_for_training_path, ngram_range=(1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
