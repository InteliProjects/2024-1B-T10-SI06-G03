{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções para o uso do Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar o notebook a seguir corretamente, é necessário seguir alguns passos prévios. Primeiro, baixe os datasets disponíveis no link a seguir: [Datasets](https://drive.google.com/drive/folders/1bm6iQenyZ63gbw_s7j0md8UaHP84O0Vn?usp=sharing) (não é necessário todos, a princípio, somente os citados abaixo). Esses datasets não estão incluídos no notebook devido ao seu tamanho, por isso são ignorados pelo arquivo `.gitignore`. Todos eles devem ser colocados na pasta [`data`](data), localizada em `src/Notebook/Api_Sprint_4/data`.\n",
    "\n",
    "Os arquivos necessários são:\n",
    "\n",
    "- **GoogleNews-vectors-negative300.bin**: [Download](https://drive.google.com/open?id=1IzDDngIEWzGP4onWlBCglAQ87g5gP2CL&usp=drive_copy) - contém palavras pré-treinadas em 300 dimensões.\n",
    "- **classification-labeled.csv**: [Download](https://drive.google.com/file/d/1AjRl3mKWceHjTFEZsHUrAwCRs-12UQQj/view?usp=drive_link) - contém o banco de dados original, entregue pela *Uber*.\n",
    "- **processed_text_data.csv**: [Download](https://drive.google.com/file/d/1hCW3pliSKDWKyr0h0tvNBtUgtw9EsE3c/view?usp=sharing) - contém o banco de dados processado.\n",
    "\n",
    "Todos são necessários para rodar as células seguintes.\n",
    "\n",
    "Outras recomendações:\n",
    "\n",
    "- Mantenha o C++ do seu computador atualizado, pois determinadas bibliotecas são compiladas com essa linguagem.\n",
    "- Caso queira conferir as versões mais atualizaas das bibliotecas, elas estão detalhadas no aquivo [``requirements.txt``](../../requirements.txt). Para usar este arquivo, basta rodar no terminal o comando: ``pip install -r requirements.txt``. Porém, a princípio, é possível ter acesso a todas as bilioteca ao rodar a célula de \"Instalação e Importação das Bibliotecas\" deste Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação e Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na seção \"Instalação e Importação das Bibliotecas\", prepara-se o ambiente de codificação configurando e carregando as bibliotecas necessárias para o projeto. Esta seção é importante pelos seguintes motivos:\n",
    "\n",
    "1. **Instalação de Bibliotecas**: Utilizam-se comandos como `%pip install nome_da_biblioteca` para instalar bibliotecas Python que não estão presentes por padrão no ambiente de execução, mas que são essenciais para a execução do código. Este comando mágico oferece uma integração mais direta com o IPython, garantindo que as instalações ocorram no kernel correto do Python utilizado pelo Jupyter.\n",
    "\n",
    "2. **Importação de Bibliotecas**: Após a instalação, as bibliotecas são importadas usando o comando `import`, permitindo o acesso às funções e ferramentas disponibilizadas por elas. Comandos típicos incluem `import numpy as np` e `import pandas as pd`. Esta prática garante que todas as funcionalidades necessárias estejam disponíveis e prontas para uso nas seções subsequentes do notebook.\n",
    "\n",
    "Esta seção é posicionada no início do notebook para assegurar que todas as dependências estejam corretamente configuradas antes de prosseguir com a análise de dados ou modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de Bibliotecas Necessárias\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emot\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install gensim\n",
    "%pip install seaborn\n",
    "%pip install xgboost\n",
    "%pip install scipy==1.11\n",
    "\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.models import KeyedVectors\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from emot.emo_unicode import UNICODE_EMOJI  \n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Abaixo, foram realizadas as seguintes etapas de pré-processamento:\n",
    "- Conversão de emojis para palavras;\n",
    "- Remoção de URLs, tags HTML, caracteres especiais e números;\n",
    "- Tokenização;\n",
    "- Remoção de Stop-Words;\n",
    "- Stemming e lematização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "classification_labeled = pd.read_csv('./data/classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    '''\n",
    "    Converte emojis no texto para palavras.\n",
    "    Input: texto original como uma string.\n",
    "    Output: texto com emojis convertidos como uma string.\n",
    "    '''\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text\n",
    "\n",
    "# Aplicando a função convert_emojis e exibindo os resultados\n",
    "classification_labeled['converted_emojis'] = classification_labeled['comment'].apply(convert_emojis)\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Limpa o texto removendo URLs, tags HTML, caracteres especiais e números através de expressões regulares.\n",
    "\n",
    "    Input: texto original como uma string.\n",
    "    Output: texto limpo, sem URLs, tags HTML, caracteres especiais e números, como uma string.\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove tags HTML\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove caracteres especiais e números\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Corrigir espaços extras\n",
    "\n",
    "    return text\n",
    "\n",
    "# Aplicando a função clean_text e exibindo os resultados\n",
    "classification_labeled['cleaned_text'] = classification_labeled['comment'].apply(clean_text)\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Tokeniza o texto em palavras.\n",
    "    \n",
    "    Input: texto limpo como uma string.\n",
    "    Output: lista de palavras (tokens).\n",
    "    '''\n",
    "    tokens = word_tokenize(text.lower())  # Convertendo para minúsculas e tokenizando\n",
    "    return tokens\n",
    "\n",
    "# Aplicando a função tokenize e exibindo os resultados\n",
    "classification_labeled['tokenized_text'] = classification_labeled['comment'].apply(tokenize)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    '''\n",
    "    Remove stopwords do texto tokenizado.\n",
    "\n",
    "    Input: lista de palavras (tokens).\n",
    "    Output: lista de palavras sem stopwords.\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Aplicando a função remove_stopwords e exibindo os resultados\n",
    "classification_labeled['without_stopwords'] = classification_labeled['comment'].apply(tokenize).apply(remove_stopwords)\n",
    "\n",
    "def stem_and_lemmatize(tokens):\n",
    "    '''\n",
    "    Aplica stemming e lematização nas palavras para reduzir à forma base.\n",
    "\n",
    "    Input: lista de palavras (tokens).\n",
    "    Output: lista de palavras processadas.\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# Aplicando a função stem_and_lemmatize e exibindo os resultados\n",
    "classification_labeled['stemmed_and_lemmatized'] = classification_labeled['comment'].apply(tokenize).apply(remove_stopwords).apply(stem_and_lemmatize)\n",
    "\n",
    "def preprocess_pipeline(dataframe, text_column):\n",
    "    '''\n",
    "    Pipeline de pré-processamento que aplica todas as funções acima ao texto.\n",
    "\n",
    "    Input: DataFrame e o nome da coluna de texto.\n",
    "    Output: DataFrame com o texto pré-processado.\n",
    "    '''\n",
    "    '''Aplicando funções de pré-processamento'''\n",
    "    dataframe['processed_text'] = dataframe[text_column].apply(lambda x: convert_emojis(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: clean_text(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: tokenize(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: remove_stopwords(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: stem_and_lemmatize(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: ' '.join(x))\n",
    "    return dataframe\n",
    "\n",
    "# Carregando os dados\n",
    "classification_labeled = pd.read_csv('./data/classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Executando o pipeline de pré-processamento\n",
    "processed_df = preprocess_pipeline(classification_labeled, 'comment')\n",
    "\n",
    "# Salvando os dados processados em um novo arquivo CSV\n",
    "processed_df.to_csv('processed_text_data.csv', index=False)\n",
    "\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, implementa-se o modelo Bag of Words nos dados pré-processados. A aplicação do Bag of Words aos dados pré-processados permite uma análise mais estruturada e quantitativa, facilitando a aplicação de técnicas de aprendizado de máquina para tarefas como classificação e análise de sentimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_data = pd.read_csv('./data/processed_text_data.csv')\n",
    "processed_text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seguir é realizado o modelo Bag of Words nos dados pré-processados\n",
    "\n",
    "def generate_bow(dataframe, text_column):\n",
    "    \"\"\"\n",
    "    Gera um DataFrame Bag of Words a partir de uma coluna de texto especificada de um DataFrame.\n",
    "\n",
    "    input:\n",
    "    dataframe (pd.DataFrame): DataFrame contendo a coluna de texto.\n",
    "    text_column (str): Nome da coluna de texto que será processada para criar o Bag of Words.\n",
    "\n",
    "    output:\n",
    "    pd.DataFrame: DataFrame representando o Bag of Words com cada palavra como uma coluna e cada documento como uma linha.\n",
    "    \"\"\"\n",
    "    # Inicializa o vetorizador de contagem\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Ajusta e transforma os dados da coluna de texto especificada\n",
    "    X = vectorizer.fit_transform(dataframe[text_column])\n",
    "\n",
    "    # Obtém o vocabulário\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Cria um DataFrame a partir da matriz esparsa, com o vocabulário como colunas\n",
    "    bow_data = pd.DataFrame(X.toarray(), columns=vocab)\n",
    "\n",
    "    return bow_data\n",
    "\n",
    "\n",
    "# Aplicar a função ao DataFrame\n",
    "bow_data = generate_bow(processed_text_data, 'processed_text')\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "print(bow_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa seção contém o uso de Word2Vec de 300 dimensões, utilizando o conjunto de vetores pré-treinados GoogleNews em inglês, especificamente o arquivo `GoogleNews-vectors-negative300.bin`, que pode ser acessado [nesse link](https://drive.google.com/file/d/1IzDDngIEWzGP4onWlBCglAQ87g5gP2CL/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './data/GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_word2vec_vectors(csv_file_path, embedding_file):\n",
    "    \"\"\"\n",
    "    Carrega dados de um arquivo CSV e utiliza um modelo Word2Vec pré-treinado para\n",
    "    gerar representações vetoriais dos textos.\n",
    "\n",
    "    Inputs:\n",
    "        csv_file_path (str): Caminho para o arquivo CSV contendo a coluna de texto.\n",
    "        embedding_file (str): Caminho para o arquivo binário do modelo Word2Vec pré-treinado.\n",
    "\n",
    "    Output: DataFrame: DataFrame com os vetores de palavras para cada documento.\n",
    "\n",
    "    \"\"\"\n",
    "    # Carregar o modelo pré-treinado\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "\n",
    "    # Ler o arquivo CSV\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Assume-se que a coluna com texto processado é chamada 'processed_text'\n",
    "    texts = data['processed_text'].astype(str)\n",
    "\n",
    "    # Inicializar um vetorizador para tokenizar os textos\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')  # Tokeniza palavras\n",
    "    vectorizer.fit(texts)\n",
    "\n",
    "    # Inicializar matriz para armazenar vetores de palavras\n",
    "    word_embeddings = np.zeros((len(texts), word_vectors.vector_size))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = vectorizer.build_tokenizer()(text)\n",
    "        word_vecs = [word_vectors[word] for word in tokens if word in word_vectors.key_to_index]\n",
    "        if word_vecs:\n",
    "            word_embeddings[i] = np.sum(word_vecs, axis=0)  \n",
    "\n",
    "    # Criar um DataFrame com os vetores\n",
    "    return pd.DataFrame(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da Função\n",
    "embedding_file = './data/GoogleNews-vectors-negative300.bin'\n",
    "csv_file_path = './data/processed_text_data.csv'\n",
    "vector_df = prepare_word2vec_vectors(csv_file_path, embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma em arquivo csv\n",
    "vector_df.to_csv('./data/word_2_vec_text_data_300.csv', index=False)\n",
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentiment_column(classification_csv, word2vec_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Carrega dois arquivos CSV, adiciona a coluna 'sentiment' do primeiro arquivo\n",
    "    ao segundo arquivo, e salva o resultado em um novo arquivo CSV.\n",
    "\n",
    "    Args:\n",
    "    classification_csv (str): Caminho para o arquivo CSV contendo a coluna 'sentiment'.\n",
    "    word2vec_csv (str): Caminho para o arquivo CSV contendo os vetores de palavras.\n",
    "    output_csv (str): Caminho para o arquivo CSV onde o resultado será salvo.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Carregar os arquivos CSV\n",
    "    classification_text_data = pd.read_csv(classification_csv)\n",
    "    word_2_vec_data = pd.read_csv(word2vec_csv)\n",
    "\n",
    "    # Selecionar a coluna 'sentiment' do arquivo classification_text_data\n",
    "    sentiment_column = classification_text_data['sentiment']\n",
    "\n",
    "    # Adicionar a coluna 'sentiment' no arquivo word_2_vec_data na primeira coluna (coluna 0)\n",
    "    word_2_vec_data.insert(0, 'sentiment', sentiment_column)\n",
    "\n",
    "    # Verificar o resultado\n",
    "    print(word_2_vec_data.head())\n",
    "\n",
    "    # Salvar o novo DataFrame em um novo arquivo CSV\n",
    "    word_2_vec_data.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamando a função que 1: vetoriza o banco de dados utilizando o Word2Vec e 2: cria o dataset para treinar os modelos \n",
    "merge_sentiment_column('./data/processed_text_data.csv', './data/word_2_vec_text_data_300.csv', './data/data_for_training_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) é uma técnica amplamente utilizada na recuperação de informações e mineração de texto para refletir a importância de uma palavra em um documento em relação a um corpus (conjunto de documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = 'data/processed_text_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "def compute_tfidf(dataframe, text_column):\n",
    "    \"\"\"\n",
    "    Calcula a matriz TF-IDF para um DataFrame e coluna de texto fornecidos.\n",
    "\n",
    "    Input:\n",
    "    dataframe (pd.DataFrame): O DataFrame de entrada contendo os dados de texto.\n",
    "    text_column (str): O nome da coluna contendo o texto processado.\n",
    "\n",
    "    Output:\n",
    "    pd.DataFrame: Um DataFrame contendo a matriz TF-IDF.\n",
    "    \"\"\"\n",
    "    # Cria um Vetorizador TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Ajusta e transforma o texto processado\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataframe[text_column])\n",
    "\n",
    "    # Converte a matriz TF-IDF para um DataFrame para melhor legibilidade\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return tfidf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a matriz TF-IDF\n",
    "tfidf_df = compute_tfidf(data, 'processed_text')\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste isolado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste: Verifica o valor TF-IDF para a palavra 'uber'\n",
    "if 'uber' in tfidf_df.columns:\n",
    "    uber_tfidf = tfidf_df['uber']\n",
    "else:\n",
    "    uber_tfidf = \"A palavra 'uber' não está presente na matriz TF-IDF.\"\n",
    "\n",
    "print(uber_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, utilizaremos o modelo Gaussian Naive Bayes para classificar as frases como positivas ou negativas para depois ser salvo na pasta [models](models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(file_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina e salva um modelo de classificação Naive Bayes em um arquivo.\n",
    "\n",
    "    Input:\n",
    "    file_path (str): O caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "    model_save_path (str): O caminho para salvar o arquivo do modelo treinado.\n",
    "\n",
    "    Output:\n",
    "    dict: Um dicionário contendo a acurácia do modelo, o relatório de classificação, e os conjuntos de teste.\n",
    "    \"\"\"\n",
    "    # Lendo os dados do arquivo CSV\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Verificando se a coluna 'processed_text' existe\n",
    "    if 'processed_text' not in data.columns:\n",
    "        raise ValueError(\"O arquivo CSV não contém a coluna 'processed_text'. Verifique o pré-processamento dos dados.\")\n",
    "\n",
    "    # Usando a coluna 'processed_text' para os dados de entrada\n",
    "    X = data['processed_text']\n",
    "    y = data['sentiment']\n",
    "\n",
    "    # Convertendo os textos para uma matriz de características usando Bag of Words\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Dividindo os dados em conjuntos de treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Modelo de classificação\n",
    "    model = GaussianNB()\n",
    "\n",
    "    # Treinando o modelo (usando toarray() porque o GaussianNB não suporta matrizes sparse)\n",
    "    model.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    # Salvando o modelo treinado com pickle\n",
    "    # Garantindo que o diretório de salvamento existe\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    \n",
    "    with open(model_save_path, 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "\n",
    "    # Predições no conjunto de teste\n",
    "    y_pred = model.predict(X_test.toarray())\n",
    "\n",
    "    # Avaliação do modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"classification_report\": class_report,\n",
    "        \"X_test\": X_test,  # Adiciona X_test ao retorno para uso posterior\n",
    "        \"y_test\": y_test   # Adiciona y_test ao retorno para uso posterior\n",
    "    }\n",
    "\n",
    "file_path = './data/processed_text_data.csv'\n",
    "model_save_path = 'models/naive_bayes_model.pkl'\n",
    "\n",
    "results = train_and_save_model(file_path, model_save_path)\n",
    "print(f\"Modelo salvo em: {model_save_path}\")\n",
    "print(f\"Acurácia: {results['accuracy']}\")\n",
    "print(f\"Relatório de Classificação: \\n{results['classification_report']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost com W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir é chamado o modelo XGBoost com os dados W2V, que treina o modelo e salva na pasta [models](models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_xgboost_model(file_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost utilizando Word2Vec, exibe as métricas de avaliação e exporta o modelo treinado.\n",
    "\n",
    "    Inputs: \n",
    "    file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "    model_save_path (str): Caminho para o arquivo para salvar o modelo treinado.\n",
    "\n",
    "    Outputs: dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Verificar a presença da coluna 'processed_text'\n",
    "        if 'processed_text' not in data.columns:\n",
    "            raise KeyError(\"'processed_text' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Usar a coluna 'processed_text' para os dados de entrada\n",
    "        X = data['processed_text']\n",
    "        y = data['sentiment']\n",
    "\n",
    "        # Re-rotular os rótulos para garantir que estão no intervalo [0, num_class)\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # Usar o CountVectorizer para transformar os textos em uma matriz de características\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        vectorizer = CountVectorizer()\n",
    "        X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Criar o DMatrix para o XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # Definir os parâmetros do XGBoost\n",
    "        params = {\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.3,\n",
    "            'objective': 'multi:softmax',  # Usa softmax para classificação multi-classes\n",
    "            'num_class': len(set(y))  # Número de classes no seu problema de classificação\n",
    "        }\n",
    "\n",
    "        # Treinar o modelo\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = bst.predict(dtest)\n",
    "\n",
    "        # Exibindo o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular e visualizar a matriz de confusão\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                    xticklabels=['Negativo', 'Neutro', 'Positivo'], \n",
    "                    yticklabels=['Negativo', 'Neutro', 'Positivo'])\n",
    "        plt.title(\"Matriz de Confusão\")\n",
    "        plt.xlabel(\"Valor Previsto\")\n",
    "        plt.ylabel(\"Valor Real\")\n",
    "        plt.show()\n",
    "\n",
    "        # Exportar o modelo usando pickle\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        with open(model_save_path, 'wb') as model_file:\n",
    "            pickle.dump(bst, model_file)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'classification_report': class_report,  # Adiciona explicitamente o relatório de classificação ao dicionário\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None\n",
    "\n",
    "file_path = 'data/processed_text_data.csv'\n",
    "model_save_path = 'models/xgboost_model.pkl'\n",
    "\n",
    "results = train_and_save_xgboost_model(file_path, model_save_path)\n",
    "if results:\n",
    "    print(f\"Modelo salvo em: {model_save_path}\")\n",
    "    print(f\"Acurácia: {results['accuracy']}\")\n",
    "    print(f\"Relatório de Classificação: \\n{results['classification_report']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
