{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções para o uso do Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar o notebook a seguir corretamente, é necessário seguir alguns passos prévios. Primeiro, baixe os datasets disponíveis no link a seguir: [Datasets](https://drive.google.com/drive/folders/1bm6iQenyZ63gbw_s7j0md8UaHP84O0Vn?usp=sharing) (não é necessário todos, a princípio, somente os citados abaixo). Esses datasets não estão incluídos no notebook devido ao seu tamanho, por isso são ignorados pelo arquivo `.gitignore`. Todos eles devem ser colocados na pasta [`data`](data), localizada em `src/Notebook/Api_Sprint_4/data`.\n",
    "\n",
    "Os arquivos necessários são:\n",
    "\n",
    "- **GoogleNews-vectors-negative300.bin**: [Download](https://drive.google.com/open?id=1IzDDngIEWzGP4onWlBCglAQ87g5gP2CL&usp=drive_copy) - contém palavras pré-treinadas em 300 dimensões.\n",
    "- **classification-labeled.csv**: [Download](https://drive.google.com/file/d/1AjRl3mKWceHjTFEZsHUrAwCRs-12UQQj/view?usp=drive_link) - contém o banco de dados original, entregue pela *Uber*.\n",
    "- **processed_text_data.csv**: [Download](https://drive.google.com/file/d/1hCW3pliSKDWKyr0h0tvNBtUgtw9EsE3c/view?usp=sharing) - contém o banco de dados processado.\n",
    "- **data_for_training_models.csv**: [Download](https://drive.google.com/file/d/1yJIvk_He-zeB9x6LDXhv0cAll5Tzwla9/view?usp=drive_link) - contém as palavras do banco de dados em vetores numéricos (a etapa de desenvolvimento do Word2Vec pode ser acompanhado no arquivo [Encapsulamento.ipynb](Encapsulamento.ipynb)).\n",
    "\n",
    "Todos são necessários para rodar as células seguintes.\n",
    "\n",
    "Outras recomendações:\n",
    "\n",
    "- Mantenha o C++ do seu computador atualizado, pois determinadas bibliotecas são compiladas com essa linguagem.\n",
    "- Caso queira conferir as versões mais atualizaas das bibliotecas, elas estão detalhadas no aquivo [``requirements.txt``](../../requirements.txt). Para usar este arquivo, basta rodar no terminal o comando: ``pip install -r requirements.txt``. Porém, a princípio, é possível ter acesso a todas as bilioteca ao rodar a célula de \"Instalação e Importação das Bibliotecas\" deste Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utilização de diversas bibliotecas é essencial para a implementação de pipelines de processamento de linguagem natural (PLN) e aprendizado de máquina. Neste documento, destacamos as bibliotecas utilizadas em nossos projetos e suas respectivas funções.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de \"Bibliotecas Mágicas\"\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emoji\n",
    "%pip install wordcloud\n",
    "%pip install emot\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "%pip install scipy==1.11\n",
    "%pip install gensim\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tempfile\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from emot.emo_unicode import UNICODE_EMOJI  \n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from xgboost import DMatrix\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "O pré-processamento de texto é uma etapa crucial em projetos de análise de sentimentos e processamento de linguagem natural (PLN). Ele transforma texto bruto em um formato limpo e estruturado, melhorando a qualidade dos dados para modelos de aprendizado de máquina.\n",
    "\n",
    "Este pipeline de pré-processamento foi desenvolvido para tratar textos em inglês e inclui as seguintes etapas:\n",
    "\n",
    "1. **Conversão de Emojis em Palavras:** Utiliza a biblioteca `emoji` para converter emojis em descrições textuais, capturando emoções e sentimentos expressos por símbolos.\n",
    "2. **Limpeza do Texto:** Remove URLs, tags HTML, caracteres especiais e números, e corrige espaços extras.\n",
    "3. **Correção Ortográfica:** Usa a biblioteca `TextBlob` para corrigir erros ortográficos, preservando palavras específicas que não devem ser modificadas.\n",
    "4. **Tokenização:** Divide o texto em palavras individuais (tokens).\n",
    "5. **Remoção de Stopwords:** Filtra palavras comuns que geralmente não contribuem para o significado principal do texto.\n",
    "\n",
    "Essas etapas garantem que os dados textuais estejam prontos para serem usados em modelos de aprendizado de máquina, melhorando a precisão e a eficácia das análises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter emojis em palavras\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Função para limpar o texto\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove tags HTML\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove caracteres especiais e números\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Corrigir espaços extras\n",
    "    return text\n",
    "\n",
    "# Função para tokenizar o texto\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convertendo para minúsculas e tokenizando\n",
    "    return tokens\n",
    "\n",
    "# Função para remover stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Lista de palavras a serem mantidas\n",
    "words_to_keep = ['uber']\n",
    "\n",
    "def correct_spelling(text):\n",
    "    # Substituir palavras a serem mantidas por placeholders temporários\n",
    "    for i, word in enumerate(words_to_keep):\n",
    "        text = text.replace(word, f'PLACEHOLDER_{i}')\n",
    "    \n",
    "    # Corrigir o texto\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # Restaurar as palavras mantidas\n",
    "    for i, word in enumerate(words_to_keep):\n",
    "        corrected_text = corrected_text.replace(f'PLACEHOLDER_{i}', word)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "# Função principal de pré-processamento para uma única frase\n",
    "def preprocess_text(text):\n",
    "    text = convert_emojis(text)\n",
    "    text = clean_text(text)\n",
    "    text = correct_spelling(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "frase_aleatoria = 'uber is good'\n",
    "# Aplicando o pré-processamento na frase aleatória\n",
    "texto_processado = preprocess_text(frase_aleatoria)\n",
    "print(f\"Texto processado: {texto_processado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorização"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A vetorização de texto é uma etapa essencial no processamento de linguagem natural (PLN). Ela converte palavras e frases em representações vetoriais permitindo que sejam processadas por modelos de aprendizado de máquina. Utilizar modelos pré-treinados como o Word2Vec permite capturar as relações semânticas entre palavras, o que é crucial para tarefas como análise de sentimentos, tradução automática e reconhecimento de entidades.\n",
    "\n",
    "No processo de vetorização, um modelo Word2Vec pré-treinado é utilizado para transformar cada palavra em um vetor numérico. Esses vetores representam a semântica das palavras e permitem operações matemáticas para analisar similaridades e diferenças. Este documento descreve o uso de um modelo Word2Vec pré-treinado, especificamente o modelo do Google News, para gerar vetores de palavras a partir de textos, demonstrando como esses vetores podem ser utilizados para representar frases de forma eficaz em um formato vetorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './data/GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_word2vec_vectors(text, embedding_file):\n",
    "    \"\"\"\n",
    "    Gera uma representação vetorial para uma única frase usando um modelo Word2Vec pré-treinado.\n",
    "\n",
    "    Inputs:\n",
    "        text (str): Frase que pretende ser vetorizada.\n",
    "        embedding_file (str): Caminho para o arquivo binário do modelo Word2Vec pré-treinado.\n",
    "\n",
    "    Output: DataFrame: DataFrame com o vetor de palavras para a frase.\n",
    "    \"\"\"\n",
    "    # Carregar o modelo pré-treinado\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "\n",
    "    # Inicializar um vetorizador para tokenizar o texto\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')  # Tokeniza palavras\n",
    "    vectorizer.fit([text])  # Ajustar o vetorizador com a frase\n",
    "\n",
    "    # Inicializar vetor para armazenar o vetor de palavras\n",
    "    word_embedding = np.zeros((word_vectors.vector_size,))\n",
    "\n",
    "    tokens = vectorizer.build_tokenizer()(text)\n",
    "    word_vecs = [word_vectors[word] for word in tokens if word in word_vectors.key_to_index]\n",
    "    if word_vecs:\n",
    "        word_embedding = np.sum(word_vecs, axis=0)\n",
    "\n",
    "    # Criar um DataFrame com o vetor\n",
    "    return pd.DataFrame([word_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'uber is good'\n",
    "\n",
    "embedding_file = './data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Obter o vetor da frase completa usando a função\n",
    "vector_df_sentence = prepare_word2vec_vectors(text, embedding_file)\n",
    "full_text_vector = vector_df_sentence.iloc[0].values\n",
    "\n",
    "# Visualizar a matriz da frase completa \n",
    "print(f\"Vetor da frase completa '{text}':\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_df_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e salvamento do modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "O treinamento e salvamento de modelos de aprendizado de máquina são etapas fundamentais no desenvolvimento de soluções de inteligência artificial. Este processo envolve preparar os dados, treinar o modelo, avaliar seu desempenho e, por fim, salvar o modelo treinado para uso futuro. \n",
    "\n",
    "Neste exemplo, utilizamos o algoritmo XGBoost para treinar um modelo de classificação de sentimentos, utilizando dados vetorizados previamente com Word2Vec. O processo inclui a carga e preparação dos dados, a divisão em conjuntos de treinamento e teste, o treinamento do modelo XGBoost, a avaliação de seu desempenho por meio de várias métricas, e o salvamento do modelo treinado em um arquivo. Esse fluxo de trabalho garante que o modelo esteja pronto para ser aplicado em novos dados de maneira eficiente e precisa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def train_xgboost_model(file_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost utilizando Word2Vec e exibe as métricas de avaliação.\n",
    "\n",
    "    Inputs:\n",
    "        file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "        model_save_path (str): Caminho para salvar o modelo XGBoost treinado.\n",
    "\n",
    "    Outputs: \n",
    "        dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data.drop('sentiment', axis=1)\n",
    "        y = data['sentiment']\n",
    "\n",
    "        # Verificar os rótulos únicos antes da codificação\n",
    "        print(\"Rótulos únicos antes da codificação:\", y.unique())\n",
    "\n",
    "        # Re-rotular os rótulos para garantir que estão no intervalo [0, num_class)\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "\n",
    "        # Verificar os rótulos únicos após a codificação\n",
    "        print(\"Rótulos únicos após a codificação:\", np.unique(y))\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Criar o DMatrix para o XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # Definir os parâmetros do XGBoost\n",
    "        params = {\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.3,\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3\n",
    "        }\n",
    "\n",
    "        # Treinar o modelo\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "        # Salvar o modelo treinado\n",
    "        with open(model_save_path, 'wb') as file:\n",
    "            pickle.dump(bst, file)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = bst.predict(dtest)\n",
    "\n",
    "        # Verificar os rótulos únicos nas previsões\n",
    "        print(\"Rótulos únicos nas previsões:\", np.unique(y_pred))\n",
    "\n",
    "        # Calcular e exibir o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular e visualizar a matriz de confusão\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negativo', 'Neutro', 'Positivo'], yticklabels=['Negativo', 'Neutro', 'Positivo'])\n",
    "        plt.title(\"Matriz de Confusão\")\n",
    "        plt.xlabel(\"Valor Previsto\")\n",
    "        plt.ylabel(\"Valor Real\")\n",
    "        plt.show()\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "file_path = './data/data_for_training_models.csv'\n",
    "model_save_path = './models/xgboost_model.pkl'\n",
    "metrics = train_xgboost_model(file_path, model_save_path)\n",
    "print(\"Métricas:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A classificação de texto em sentimentos positivos, negativos ou neutros é uma aplicação comum em processamento de linguagem natural (PLN). Para realizar essa tarefa, é possível utilizar um modelo de aprendizado de máquina previamente treinado, como o XGBoost. \n",
    "\n",
    "Nesta abordagem, a função `classify_text` é responsável por classificar uma frase com base em seu vetor de características. O modelo XGBoost, treinado anteriormente e salvo em um arquivo, é carregado e utilizado para prever a classe do texto. Esta função facilita a aplicação do modelo em novos dados, proporcionando uma classificação eficiente e precisa de sentimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificar o texto\n",
    "def classify_text(vectorized_text, model_file):\n",
    "    \"\"\"\n",
    "    Classifica uma frase como positiva (1), negativa (-1) ou neutra (0) usando um modelo XGBoost treinado.\n",
    "\n",
    "    Inputs:\n",
    "        vectorized_text (np.ndarray): Vetor representando a frase vetorizada.\n",
    "        model_file (str): Caminho para o arquivo pickle contendo o modelo XGBoost treinado.\n",
    "\n",
    "    Output: int: Classificação da frase (1: positiva, -1: negativa, 0: neutra).\n",
    "    \"\"\"\n",
    "    # Carregar o modelo XGBoost treinado\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    # Criar um DMatrix a partir do vetorizado texto\n",
    "    feature_names = [str(i) for i in range(vectorized_text.shape[1])]\n",
    "    dmatrix = xgb.DMatrix(vectorized_text, feature_names=feature_names)\n",
    "\n",
    "    # Fazer a predição usando o modelo carregado\n",
    "    prediction = model.predict(dmatrix)\n",
    "    \n",
    "    return int(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chamada do modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Para categorizar uma frase em termos de sentimentos, utilizamos um pipeline que combina pré-processamento, vetorização e classificação. Este pipeline permite transformar texto bruto em uma classificação de sentimento de maneira eficiente e precisa.\n",
    "\n",
    "A função `categorizing_sentence` integra todas as etapas necessárias para processar e classificar uma frase. Ela pré-processa a frase para remover ruídos, vetoriza a frase utilizando um modelo Word2Vec pré-treinado e, em seguida, classifica o vetor de características utilizando um modelo XGBoost treinado.\n",
    "\n",
    "Essa abordagem é testada com uma frase de exemplo, demonstrando a aplicação prática do pipeline para determinar o sentimento do texto. Com isso, é possível aplicar esta metodologia para analisar sentimentos em diversos textos em inglês, proporcionando uma ferramenta poderosa para várias aplicações de análise de sentimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal para categorizar a frase\n",
    "def categorizing_sentence(text, embedding_file, model_file):\n",
    "    \"\"\"\n",
    "    Processa e classifica uma frase como positiva (1), negativa (-1) ou neutra (0).\n",
    "\n",
    "    Inputs:\n",
    "        text (str): Frase a ser classificada.\n",
    "        embedding_file (str): Caminho para o arquivo binário do modelo Word2Vec pré-treinado.\n",
    "        model_file (str): Caminho para o arquivo pickle contendo o modelo XGBoost treinado.\n",
    "\n",
    "    Output: int: Classificação da frase (1: positiva, -1: negativa, 0: neutra).\n",
    "    \"\"\"\n",
    "    # Pré-processar a frase\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Vetorizar a frase\n",
    "    vectorized_text = prepare_word2vec_vectors(preprocessed_text, embedding_file)\n",
    "    \n",
    "    # Classificar a frase\n",
    "    classification = classify_text(vectorized_text.values, model_file)\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com uma frase de exemplo\n",
    "text = \"i love uber!\"\n",
    "embedding_file = './data/GoogleNews-vectors-negative300.bin'\n",
    "model_file = './models/xgboost_model.pkl'\n",
    "\n",
    "result = categorizing_sentence(text, embedding_file, model_file)\n",
    "print(\"Classificação:\", result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Comparativa de Modelos de Classificação de Sentimentos\n",
    "\n",
    "Nesta seção, apresentamos uma análise comparativa de três modelos de classificação de sentimentos: FastText, TF-IDF e GloVe de 50 dimensões. A análise considera duas categorias de classificação: Negativo vs Não Negativo e Neutro vs Positivo.\n",
    "\n",
    "## Resultados dos Modelos\n",
    "\n",
    "### FastText\n",
    "\n",
    "**Negativo vs Não Negativo**\n",
    "- **Precision**: 0.94\n",
    "- **Recall**: 0.94\n",
    "- **F1-Score**: 0.94\n",
    "- **Accuracy**: 0.94\n",
    "- **Kappa**: 0.88\n",
    "- **Mean Cross-Val F1**: 0.933\n",
    "- **Std Cross-Val F1**: 0.018\n",
    "\n",
    "**Neutro vs Positivo**\n",
    "- **Precision**: 0.97\n",
    "- **Recall**: 0.97\n",
    "- **F1-Score**: 0.97\n",
    "- **Accuracy**: 0.97\n",
    "- **Kappa**: 0.93\n",
    "- **Mean Cross-Val F1**: 0.931\n",
    "- **Std Cross-Val F1**: 0.006\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "**Negativo vs Não Negativo**\n",
    "- **Precision**: 0.92\n",
    "- **Recall**: 0.91\n",
    "- **F1-Score**: 0.91\n",
    "- **Accuracy**: 0.91\n",
    "- **Kappa**: 0.82\n",
    "- **Mean Cross-Val F1**: 0.928\n",
    "\n",
    "**Neutro vs Positivo**\n",
    "- **Precision**: 0.94\n",
    "- **Recall**: 0.94\n",
    "- **F1-Score**: 0.94\n",
    "- **Accuracy**: 0.94\n",
    "- **Kappa**: 0.88\n",
    "- **Mean Cross-Val F1**: 0.908\n",
    "\n",
    "### GloVe de 50 Dimensões\n",
    "\n",
    "**Negativo vs Não Negativo**\n",
    "- **Precision**: 0.91\n",
    "- **Recall**: 0.91\n",
    "- **F1-Score**: 0.91\n",
    "- **Accuracy**: 0.91\n",
    "- **Kappa**: 0.83\n",
    "- **Mean Cross-Val F1**: 0.897\n",
    "\n",
    "**Neutro vs Positivo**\n",
    "- **Precision**: 0.94\n",
    "- **Recall**: 0.94\n",
    "- **F1-Score**: 0.94\n",
    "- **Accuracy**: 0.94\n",
    "- **Kappa**: 0.89\n",
    "- **Mean Cross-Val F1**: 0.912\n",
    "\n",
    "\n",
    "### Comparação de Desempenho\n",
    "\n",
    "#### FastText\n",
    "\n",
    "O modelo FastText apresenta os melhores resultados em quase todas as métricas para ambas as classificações:\n",
    "- **Negativo vs Não Negativo**: Apresenta uma precisão, recall, f1-score e acurácia de 0.94, além de um kappa de 0.88. A média de validação cruzada do f1-score é 0.933 com um desvio padrão de 0.018.\n",
    "- **Neutro vs Positivo**: Também lidera com precisão, recall, f1-score e acurácia de 0.97, e um kappa de 0.93. A média de validação cruzada do f1-score é 0.931 com um desvio padrão de 0.006.\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "O modelo TF-IDF também apresenta bons resultados, mas é ligeiramente inferior ao FastText:\n",
    "- **Negativo vs Não Negativo**: A precisão é 0.92, recall é 0.91, f1-score é 0.91, acurácia é 0.91 e o kappa é 0.82. A média de validação cruzada do f1-score é 0.928.\n",
    "- **Neutro vs Positivo**: A precisão, recall, f1-score e acurácia são todas 0.94, com um kappa de 0.88. A média de validação cruzada do f1-score é 0.908.\n",
    "\n",
    "#### GloVe de 50 Dimensões\n",
    "\n",
    "O modelo GloVe de 50 dimensões mostra desempenho similar ao TF-IDF:\n",
    "- **Negativo vs Não Negativo**: A precisão, recall, f1-score e acurácia são todas 0.91, com um kappa de 0.83. A média de validação cruzada do f1-score é 0.897.\n",
    "- **Neutro vs Positivo**: A precisão, recall, f1-score e acurácia são todas 0.94, com um kappa de 0.89. A média de validação cruzada do f1-score é 0.912.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "Com base nas métricas apresentadas, o **FastText** é o melhor modelo para as classificações Negativo vs Não Negativo e Neutro vs Positivo. Ele apresenta as melhores métricas de precisão, recall, f1-score, acurácia e kappa, indicando um desempenho superior na classificação de sentimentos.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
