{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação e Importação das Bibliotecas\n",
    "\n",
    "Na seção \"Instalação e Importação das Bibliotecas\", prepara-se o ambiente de codificação configurando e carregando as bibliotecas necessárias para o projeto. Esta seção é importante pelos seguintes motivos:\n",
    "\n",
    "1. **Instalação de Bibliotecas**: Utilizam-se comandos como `%pip install nome_da_biblioteca` para instalar bibliotecas Python que não estão presentes por padrão no ambiente de execução, mas que são essenciais para a execução do código. Este comando mágico oferece uma integração mais direta com o IPython, garantindo que as instalações ocorram no kernel correto do Python utilizado pelo Jupyter.\n",
    "\n",
    "2. **Importação de Bibliotecas**: Após a instalação, as bibliotecas são importadas usando o comando `import`, permitindo o acesso às funções e ferramentas disponibilizadas por elas. Comandos típicos incluem `import numpy as np` e `import pandas as pd`. Esta prática garante que todas as funcionalidades necessárias estejam disponíveis e prontas para uso nas seções subsequentes do notebook.\n",
    "\n",
    "Esta seção é posicionada no início do notebook para assegurar que todas as dependências estejam corretamente configuradas antes de prosseguir com a análise de dados ou modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.11.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wordcloud in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wordcloud) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: emot in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.12.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.11.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.11.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\inteli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalação de \"Bibliotecas Mágicas\"\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emoji\n",
    "%pip install wordcloud\n",
    "%pip install emot\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import pandas as pd\n",
    "import emoji \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from emot.emo_unicode import UNICODE_EMOJI  \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursos necessários para pré-processamento\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Descritiva dos Dados\n",
    "\n",
    "Na seção \"Análise Descritiva dos Dados\", realiza-se uma investigação inicial dos dados fornecidos pela Uber, dividindo a análise em duas subseções principais:\n",
    "\n",
    "1. **Análise de Dados Iniciais**: Esta parte concentra-se na exploração inicial dos dados conforme recebidos, antes de qualquer manipulação ou limpeza. Examina-se a estrutura geral dos dados, incluindo tipos de dados, presença de valores ausentes, e estatísticas descritivas básicas. Também se realiza uma visualização preliminar das distribuições e relações entre variáveis. Esta análise é importante para entender o ponto de partida do conjunto de dados e identificar possíveis desafios que podem surgir durante o pré-processamento.\n",
    "\n",
    "2. **Análise de Dados Processados**: Após a aplicação de etapas de pré-processamento, como limpeza, retirada de stop-words, e *oversample* (estratégia de aumento de dados minoritários realizado nessa seção), esta subseção avalia como os dados foram transformados e o impacto dessas mudanças. Inclui-se a revisão de estatísticas descritivas pós-processamento, comparação de distribuições antes e depois da limpeza, e uma nova avaliação de correlações entre as variáveis. O objetivo é assegurar que os dados estejam prontos para a modelagem.\n",
    "\n",
    "Este resumo destaca os tópicos principais da análise dos dados. Para visualizações detalhadas e análises mais aprofundadas, consulte a seção 4. Análise Descritiva dos Dados na documentação completa, disponível em [este link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos Dados Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise das Colunas\n",
    "\n",
    "- **Avaliação das Colunas**: Exibe as características e o conteúdo de cada coluna no conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo para ver as primeiras linhas e descobrir a estrutura\n",
    "file_path = 'classification-labeled.csv'\n",
    "\n",
    "# Ler as primeiras linhas do arquivo como texto puro para identificar o delimitador\n",
    "with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "    lines = [file.readline() for _ in range(5)]\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "classification_labeled = pd.read_csv('classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "classification_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organização dos Dados:\n",
    "\n",
    "- **Remoção da Coluna de ID**: Elimina a coluna 'id' do conjunto de dados.\n",
    "- **Contagem de Palavras por Frase**: Calcula e registra o número de palavras em cada frase.\n",
    "- **Contagem de Emojis por Frase**: Identifica e conta os emojis presentes em cada frase.\n",
    "- **Consolidação das Métricas**: Agrega todas as métricas acima em uma única tabela para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O código a seguir remove a coluna de ID\n",
    "\n",
    "def remove_column(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Remove uma coluna específica de um DataFrame pandas.\n",
    "\n",
    "    Input:\n",
    "    dataframe (pd.DataFrame): O DataFrame do qual a coluna será removida.\n",
    "    column_name (str): O nome da coluna a ser removida.\n",
    "\n",
    "    Output:\n",
    "    pd.DataFrame: O DataFrame com a coluna especificada removida.\n",
    "\n",
    "    \"\"\"\n",
    "    dataframe.drop(columns=column_name, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# Carregar os dados do arquivo CSV\n",
    "classification_labele = pd.read_csv('classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Excluir a coluna 'id' usando a função definida\n",
    "classification_labele = remove_column(classification_labele, 'id')\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame para verificar se a coluna foi removida corretamente\n",
    "print(classification_labele.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O código a seguir conta a quantidade de palavras por frase do banco de dados\n",
    "\n",
    "def word_count(texto):\n",
    "    \"\"\"\n",
    "    Conta o número de palavras em uma string fornecida.\n",
    "\n",
    "    Inputs:\n",
    "    texto (str): Uma string da qual as palavras serão contadas.\n",
    "\n",
    "    Outputs: int: O número de palavras na string.\n",
    "    \"\"\"\n",
    "    return len(texto.split())\n",
    "\n",
    "# Aplicar a função à coluna 'comment' e criar uma nova coluna 'word_counter'\n",
    "classification_labele['word_counter'] = classification_labele['comment'].apply(word_count)\n",
    "\n",
    "# Calcular a média de palavras na coluna 'word_counter'\n",
    "media_words = classification_labele['word_counter'].mean()\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame para verificar a aplicação da contagem de palavras\n",
    "print(classification_labele[['comment', 'word_counter']].head())\n",
    "\n",
    "# Mostrar a média de palavras por frase na coluna 'comment'\n",
    "print(f\"\\nA média de palavras por frase na coluna 'comment' é: {media_words:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O código a seguir conta a quantidade de emojis por frase do banco de dados\n",
    "\n",
    "def account_emojis(texto):\n",
    "    \"\"\"\n",
    "    Conta o número de emojis em uma string fornecida.\n",
    "\n",
    "    inputs: texto (str): Uma string da qual os emojis serão contados.\n",
    "\n",
    "    Outputs: int: O número de emojis na string.\n",
    "    \"\"\"\n",
    "    return emoji.emoji_count(texto)\n",
    "\n",
    "# Aplicar a função à coluna 'comment' para contar emojis\n",
    "classification_labele['count_emojis'] = classification_labele['comment'].apply(account_emojis)\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame para verificar a aplicação da contagem de emojis\n",
    "print(classification_labele[['comment', 'count_emojis']].head())\n",
    "\n",
    "# Calcular a média de emojis na coluna 'count_emojis'\n",
    "media_emojis = classification_labele['count_emojis'].mean()\n",
    "\n",
    "# Mostrar a média de emojis por frase na coluna 'comment'\n",
    "print(f\"\\nA média de emojis por frase na coluna 'comment' é: {media_emojis:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O código a seguir une em uma tabela todas as métricas realizadas acima\n",
    " \n",
    "# Aplicar a função à coluna 'comment' e criar uma nova coluna 'word_counter'\n",
    "classification_labele['word_counter'] = classification_labele['comment'].apply(word_count)\n",
    "\n",
    "# Salvar a base de dados com as alterações para a realização da Visualização Gráfica\n",
    "classification_labele.to_csv('classification-labeled_with_word_counter.csv', index=False)\n",
    "\n",
    "classification_labele.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização Gráfica dos Dados Iniciais\n",
    "\n",
    "- **Contagem de Valores Nulos**: Cria tabelas para visualizar a quantidade de valores nulos em cada coluna.\n",
    "- **Nuvem de Palavras**: Configura e exibe uma nuvem de palavras com os termos mais frequentes.\n",
    "- **Ocorrência de Sentimentos**: Contabiliza e visualiza a frequência de cada categoria de sentimento.\n",
    "- **Relação entre Tamanho da Frase e Sentimento**: Mostra graficamente como o tamanho das frases se relaciona com a classificação de sentimentos.\n",
    "- **Análise de Sentimentos das Palavras Frequentes**: Visualiza a classificação de sentimentos das vinte palavras mais comuns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo CSV com a contagem de palavras\n",
    "classification_labeled_with_word_counter = pd.read_csv('classification-labeled_with_word_counter.csv')\n",
    "classification_labeled_with_word_counter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar valores nulos em cada coluna\n",
    "null_values = classification_labeled_with_word_counter.isnull().sum()\n",
    "\n",
    "# Criar gráfico de barras\n",
    "plt.figure(figsize=(10, 5))\n",
    "null_values.plot(kind='bar', color='skyblue')\n",
    "plt.title('Valores Nulos por Coluna')\n",
    "plt.xlabel('Colunas')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.ylabel('Quantidade de Valores Nulos')\n",
    "plt.grid(axis='y', linestyle='--', alpha=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização das palavras mais frequentes do banco de dados em uma Nuvem de Palavras\n",
    "\n",
    "# Concatenando todos os comentários em uma única string\n",
    "comments = classification_labeled_with_word_counter['comment'].values.tolist()\n",
    "all_comments = \" \".join(comment for comment in comments)\n",
    "\n",
    "# Configurando a nuvem de palavras\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis', \n",
    "                      max_words=100, prefer_horizontal=1.0).generate(all_comments)\n",
    "\n",
    "# Plotando a nuvem de palavras\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica a quantidade de comentários por sentimento\n",
    "\n",
    "# Contando a ocorrência de cada sentimento\n",
    "sentiment_counts = classification_labeled_with_word_counter['sentiment'].value_counts()\n",
    "\n",
    "# Plotando o gráfico de barras\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'gray', 'green'])\n",
    "plt.title('Distribuição dos Sentimentos')\n",
    "plt.xlabel('Sentimento')\n",
    "plt.ylabel('Número de Comentários')\n",
    "plt.xticks([0, 1, 2], ['Negativo', 'Neutro', 'Positivo'], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica a relação entre tamanho da frase e a classificação de sentimento\n",
    "\n",
    "# Definir uma função para contar as palavras em cada comentário\n",
    "def word_count(texto):\n",
    "    \"\"\"\n",
    "    Conta o número de palavras em uma string fornecida.\n",
    "\n",
    "    Inputs:\n",
    "    texto (str): Uma string da qual as palavras serão contadas.\n",
    "\n",
    "    Outputs: int: O número de palavras na string.\n",
    "    \"\"\"\n",
    "    return len(texto.split())\n",
    "\n",
    "# Aplicar a função à coluna 'comment' e criar uma nova coluna 'word_counter'\n",
    "classification_labeled_with_word_counter['word_counter'] = classification_labeled_with_word_counter['comment'].apply(word_count)\n",
    "\n",
    "# Definir os intervalos de tamanho das frases\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "# Criar uma nova coluna para armazenar os intervalos de tamanho das frases\n",
    "classification_labeled_with_word_counter['phrase_length_bins'] = pd.cut(classification_labeled_with_word_counter['word_counter'], bins=bins, right=False)\n",
    "\n",
    "# Calculando a contagem de cada classe de sentimento dentro de cada intervalo\n",
    "sentiment_counts = classification_labeled_with_word_counter.groupby(['phrase_length_bins', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Definindo as cores para os sentimentos negativo, neutro e positivo, respectivamente\n",
    "colors = ['red', 'gray', 'green']\n",
    "\n",
    "# Plotando o gráfico de barras empilhadas com as cores personalizadas\n",
    "sentiment_counts.plot(kind='bar', stacked=True, figsize=(12, 6), color=colors)\n",
    "\n",
    "# Adicionando título e rótulos aos eixos\n",
    "plt.title('Relação entre Tamanho da Frase e Classificação de Sentimento')\n",
    "plt.xlabel('Tamanho da Frase')\n",
    "plt.ylabel('Contagem')\n",
    "\n",
    "# Rotacionando os rótulos do eixo x para melhor visualização\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adicionando legenda\n",
    "plt.legend(title='Sentimento', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Exibindo o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico qu eclassifica o sentimento relacionado às 20 palavras mais frequentes\n",
    "\n",
    "# Contar a frequência de todas as palavras nos comentários\n",
    "word_freq = Counter(' '.join(classification_labeled_with_word_counter['comment']).split())\n",
    "\n",
    "# Selecionar as dez palavras mais frequentes\n",
    "top_words = [word[0] for word in word_freq.most_common(20)]\n",
    "\n",
    "# Criar um dicionário para armazenar as contagens de sentimentos para cada palavra\n",
    "word_sentiment_counts = {word: {'Positive': 0, 'Neutral': 0, 'Negative': 0} for word in top_words}\n",
    "\n",
    "# Preencher o dicionário com as contagens de sentimentos para cada palavra\n",
    "for word in top_words:\n",
    "    for index, row in classification_labeled_with_word_counter.iterrows():\n",
    "        if word.lower() in row['comment'].lower():\n",
    "            sentiment = row['sentiment']\n",
    "            if sentiment == 1:\n",
    "                word_sentiment_counts[word]['Positive'] += 1\n",
    "            elif sentiment == 0:\n",
    "                word_sentiment_counts[word]['Neutral'] += 1\n",
    "            elif sentiment == -1:\n",
    "                word_sentiment_counts[word]['Negative'] += 1\n",
    "\n",
    "# Criar um DataFrame a partir do dicionário\n",
    "word_sentiment_classification_labeled_with_word_counter = pd.DataFrame(word_sentiment_counts).T\n",
    "\n",
    "# Definindo as cores para os sentimentos negativo, neutro e positivo, respectivamente\n",
    "colors = ['green', 'gray', 'red']\n",
    "\n",
    "# Plotar um gráfico de barras empilhadas com as cores personalizadas\n",
    "word_sentiment_classification_labeled_with_word_counter.plot(kind='bar', stacked=True, figsize=(10, 6), color=colors)\n",
    "plt.title('Classificação de Sentimentos das Vinte Palavras Mais Frequentes')\n",
    "plt.xlabel('Palavra')\n",
    "plt.ylabel('Contagem de Palavras por Sentimentos')\n",
    "plt.legend(title='Sentimento')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Naive Bayes\n",
    "\n",
    "- **Modelo de Naive Bayes**: Utilizou-se o modelo Naive Bayes para estabelecer uma linha de base de desempenho com os dados não processados. Esta etapa foi executada para proporcionar uma métrica inicial de análise e comparação, permitindo avaliar o impacto das etapas subsequentes de pré-processamento nos resultados do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir valores NaN por strings vazias\n",
    "classification_labele['comment'].fillna('', inplace=True)\n",
    "\n",
    "# Define as variáveis independentes e dependentes\n",
    "X = classification_labele['comment']   # Textos\n",
    "y = classification_labele['sentiment'] # Labels de sentimento\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criação do pipeline com SMOTE, TF-IDF e Naive Bayes\n",
    "pipeline = make_pipeline_imb(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=42),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "# Define o grid de hiperparâmetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams, bigrams, and trigrams\n",
    "    'tfidfvectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidfvectorizer__min_df': [1, 2, 3],\n",
    "    'multinomialnb__alpha': [0.01, 0.1, 1]  # Suavização aditiva (Laplace/Lidstone)\n",
    "}\n",
    "\n",
    "# Configuração do GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime a melhor acurácia encontrada e os parâmetros correspondentes\n",
    "print(\"Melhor acurácia de validação cruzada: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "print(\"Melhores parâmetros:\", grid_search.best_params_)\n",
    "\n",
    "# Avalia o modelo no conjunto de teste\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Acurácia no conjunto de teste:\", (y_test == y_pred).mean() * 100, \"%\")\n",
    "\n",
    "# Relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos Dados Processados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample\n",
    "\n",
    "- **Implementação do Oversample**: Baseando-se nos insights adquiridos na \"Análise dos Dados Iniciais\", optou-se por aplicar a técnica de oversample. Esta abordagem visa aumentar a quantidade de representantes das classes minoritárias no conjunto de dados, proporcionando um equilíbrio melhor entre as classes.\n",
    "\n",
    "- **Aplicação Pós-Pré-processamento**: A técnica de oversample foi implementada após o pré-processamento dos dados. Esta sequência permite uma comparação efetiva do impacto do oversample nos dados já tratados e normalizados.\n",
    "\n",
    "- **Exclusão em Análises Específicas**: É importante destacar que o oversample *não* é aplicado na seção de \"Bag of Words\" nem na análise das métricas relacionadas a esta técnica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "processed_text_data = pd.read_csv('processed_text_data.csv')\n",
    "\n",
    "# Substituir valores NaN por strings vazias\n",
    "processed_text_data['processed_text'].fillna('', inplace=True)\n",
    "\n",
    "# Separar os dados em features e target\n",
    "X = processed_text_data['processed_text']\n",
    "y = processed_text_data['sentiment']\n",
    "\n",
    "# Vetorizar o texto processado\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Aplicar SMOTE para balancear os dados\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)\n",
    "\n",
    "# Converter a matriz esparsa de volta para texto \n",
    "X_resampled_text = vectorizer.inverse_transform(X_resampled)\n",
    "\n",
    "# Criar um DataFrame com os dados resampleados\n",
    "data_resampled = pd.DataFrame({\n",
    "    'processed_text': [' '.join(text) for text in X_resampled_text],  # Simplificado para sempre juntar o texto\n",
    "    'sentiment': y_resampled\n",
    "})\n",
    "\n",
    "# Visualizar a distribuição dos sentimentos\n",
    "print('Distribuição dos sentimentos após o oversampling:', pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os dados processados em um novo arquivo CSV\n",
    "data_resampled.to_csv('oversampled_text_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise das Colunas\n",
    "- **Avaliação das Colunas**: Exibe as características e o conteúdo de cada coluna no conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo para ver as primeiras linhas e descobrir a estrutura\n",
    "file_path = 'oversampled_text_data.csv'\n",
    "\n",
    "# Ler as primeiras linhas do arquivo como texto puro para identificar o delimitador\n",
    "with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "    lines = [file.readline() for _ in range(5)]\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "oversampled_text_data = pd.read_csv('oversampled_text_data.csv')\n",
    "oversampled_text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organização dos Dados\n",
    "\n",
    "- **Contagem de Palavras por Frase**: Calcula e registra o número de palavras em cada frase.\n",
    "\n",
    "*Nota*: Não foi necessário realizar as etapas de Remoção da Coluna de ID, pois essa já não contém no banco de dados trabalhado. Bem como contagem de emojis por frase, pois estes foram retirados durante o pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(texto):\n",
    "    \"\"\"\n",
    "    Conta o número de palavras em uma string fornecida, tratando possíveis valores não-string.\n",
    "\n",
    "    Inputs:\n",
    "    texto (str): Uma string da qual as palavras serão contadas.\n",
    "\n",
    "    Outputs:\n",
    "    int: O número de palavras na string, ou 0 se o input não for uma string.\n",
    "    \"\"\"\n",
    "    if isinstance(texto, str):\n",
    "        return len(texto.split())\n",
    "    return 0\n",
    "\n",
    "oversampled_text_data['word_counter'] = oversampled_text_data['processed_text'].apply(word_count)\n",
    "\n",
    "# Calcular a média de palavras na coluna 'word_counter'\n",
    "media_words = oversampled_text_data['word_counter'].mean()\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame para verificar a aplicação da contagem de palavras\n",
    "oversampled_text_data = oversampled_text_data[['processed_text', 'sentiment', 'word_counter']]\n",
    "print(oversampled_text_data.head())\n",
    "\n",
    "# Mostrar a média de palavras por frase na coluna 'processed_text'\n",
    "print(f\"\\nA média de palavras por frase na coluna 'processed_text' é: {media_words:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização Gráfica dos Dados Processados \n",
    "\n",
    "- **Contagem de Valores Nulos**: Cria tabelas para visualizar a quantidade de valores nulos em cada coluna.\n",
    "- **Nuvem de Palavras**: Configura e exibe uma nuvem de palavras com os termos mais frequentes.\n",
    "- **Ocorrência de Sentimentos**: Contabiliza e visualiza a frequência de cada categoria de sentimento.\n",
    "- **Relação entre Tamanho da Frase e Sentimento**: Mostra graficamente como o tamanho das frases se relaciona com a classificação de sentimentos.\n",
    "- **Análise de Sentimentos das Palavras Frequentes**: Visualiza a classificação de sentimentos das vinte palavras mais comuns.\n",
    "\n",
    "Todos para os dados pré-processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar valores nulos em cada coluna\n",
    "null_values = oversampled_text_data.isnull().sum()\n",
    "\n",
    "# Criar gráfico de barras\n",
    "plt.figure(figsize=(10, 5))\n",
    "null_values.plot(kind='bar', color='skyblue')\n",
    "plt.title('Valores Nulos por Coluna')\n",
    "plt.xlabel('Colunas')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.ylabel('Quantidade de Valores Nulos')\n",
    "plt.grid(axis='y', linestyle='--', alpha=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização das palavras mais frequentes do banco de dados em uma Nuvem de Palavras\n",
    "\n",
    "# Filtrando os textos processados para garantir que sejam strings\n",
    "processed_texts = [str(processed_text) for processed_text in oversampled_text_data['processed_text'].values.tolist()]\n",
    "\n",
    "# Concatenando todos os comentários processados em uma única string\n",
    "all_processed_texts = \" \".join(processed_text for processed_text in processed_texts)\n",
    "\n",
    "# Configurando a nuvem de palavras\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis', \n",
    "                      max_words=100, prefer_horizontal=1.0).generate(all_processed_texts)\n",
    "\n",
    "# Plotando a nuvem de palavras\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Nuvem de Palavras dos Comentários Processados')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica a quantidade de comentários por sentimento\n",
    "\n",
    "# Contando a ocorrência de cada sentimento\n",
    "sentiment_counts = oversampled_text_data['sentiment'].value_counts()\n",
    "\n",
    "# Plotando o gráfico de barras\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'gray', 'green'])\n",
    "plt.title('Distribuição dos Sentimentos')\n",
    "plt.xlabel('Sentimento')\n",
    "plt.ylabel('Número de Comentários')\n",
    "plt.xticks([0, 1, 2], ['Negativo', 'Neutro', 'Positivo'], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica a relação entre tamanho da frase e a classificação de sentimento\n",
    "\n",
    "# Definir os intervalos de tamanho das frases\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "# Criar uma nova coluna para armazenar os intervalos de tamanho das frases\n",
    "oversampled_text_data['phrase_length_bins'] = pd.cut(oversampled_text_data['word_counter'], bins=bins, right=False)\n",
    "\n",
    "# Calculando a contagem de cada classe de sentimento dentro de cada intervalo\n",
    "sentiment_counts = oversampled_text_data.groupby(['phrase_length_bins', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Definindo as cores para os sentimentos negativo, neutro e positivo, respectivamente\n",
    "colors = ['red', 'gray', 'green']\n",
    "\n",
    "# Plotando o gráfico de barras empilhadas com as cores personalizadas\n",
    "sentiment_counts.plot(kind='bar', stacked=True, figsize=(12, 6), color=colors)\n",
    "\n",
    "# Adicionando título e rótulos aos eixos\n",
    "plt.title('Relação entre Tamanho da Frase e Classificação de Sentimento')\n",
    "plt.xlabel('Tamanho da Frase')\n",
    "plt.ylabel('Contagem')\n",
    "\n",
    "# Rotacionando os rótulos do eixo x para melhor visualização\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adicionando legenda\n",
    "plt.legend(title='Sentimento', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Exibindo o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico qu eclassifica o sentimento relacionado às 20 palavras mais frequentes\n",
    "\n",
    "# Convertendo todos os valores para strings antes de unir\n",
    "word_freq = Counter(' '.join(str(text) for text in oversampled_text_data['processed_text'] if pd.notnull(text)).split())\n",
    "\n",
    "# Selecionar as dez palavras mais frequentes\n",
    "top_words = [word[0] for word in word_freq.most_common(20)]\n",
    "\n",
    "# Criar um dicionário para armazenar as contagens de sentimentos para cada palavra\n",
    "word_sentiment_counts = {word: {'Positive': 0, 'Neutral': 0, 'Negative': 0} for word in top_words}\n",
    "\n",
    "# Preencher o dicionário com as contagens de sentimentos para cada palavra\n",
    "for word in top_words:\n",
    "    for index, row in oversampled_text_data.iterrows():\n",
    "        # Verificar se o valor não é nulo\n",
    "        if pd.notnull(row['processed_text']):\n",
    "            # Converter para minúsculas e verificar a presença da palavra\n",
    "            if word.lower() in str(row['processed_text']).lower():\n",
    "                sentiment = row['sentiment']\n",
    "                if sentiment == 1:\n",
    "                    word_sentiment_counts[word]['Positive'] += 1\n",
    "                elif sentiment == 0:\n",
    "                    word_sentiment_counts[word]['Neutral'] += 1\n",
    "                elif sentiment == -1:\n",
    "                    word_sentiment_counts[word]['Negative'] += 1\n",
    "\n",
    "# Criar um DataFrame a partir do dicionário\n",
    "word_sentiment_df = pd.DataFrame(word_sentiment_counts).T\n",
    "\n",
    "# Definindo as cores para os sentimentos negativo, neutro e positivo, respectivamente\n",
    "colors = ['green', 'gray', 'red']\n",
    "\n",
    "# Plotar um gráfico de barras empilhadas com as cores personalizadas\n",
    "word_sentiment_df.plot(kind='bar', stacked=True, figsize=(10, 6), color=colors)\n",
    "plt.title('Classificação de Sentimentos das Vinte Palavras Mais Frequentes')\n",
    "plt.xlabel('Palavra')\n",
    "plt.ylabel('Contagem de Sentimentos')\n",
    "plt.legend(title='Sentimento')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Naive Bayes \n",
    "\n",
    "- **Modelo de Naive Bayes**: Utilizou-se o modelo Naive Bayes para estabelecer uma linha de base de desempenho com os dados não processados. Essa versão do modelo foi utilizada com os dados pré-processados e com a técnica de Oversample aplicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados processados\n",
    "oversampled_text_data = pd.read_csv('oversampled_text_data.csv')\n",
    "\n",
    "# Substituir valores NaN por strings vazias\n",
    "oversampled_text_data['processed_text'].fillna('', inplace=True)\n",
    "\n",
    "# Define as variáveis independentes e dependentes\n",
    "X = oversampled_text_data['processed_text']  # Textos pré-processados\n",
    "y = oversampled_text_data['sentiment']       # Labels de sentimento\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criação do pipeline com SMOTE, TF-IDF e Naive Bayes\n",
    "pipeline = make_pipeline_imb(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=42),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "# Define o grid de hiperparâmetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigrams, bigrams, and trigrams\n",
    "    'tfidfvectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidfvectorizer__min_df': [1, 2, 3],\n",
    "    'multinomialnb__alpha': [0.01, 0.1, 1]  # Suavização aditiva (Laplace/Lidstone)\n",
    "}\n",
    "\n",
    "# Configuração do GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime a melhor acurácia encontrada e os parâmetros correspondentes\n",
    "print(\"Melhor acurácia de validação cruzada: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "print(\"Melhores parâmetros:\", grid_search.best_params_)\n",
    "\n",
    "# Avalia o modelo no conjunto de teste\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Acurácia no conjunto de teste:\", (y_test == y_pred).mean() * 100, \"%\")\n",
    "\n",
    "# Relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados do arquivo CSV\n",
    "df = pd.read_csv('classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpa o texto removendo pontuações repetidas e espaços extras.\n",
    "\n",
    "    Args:\n",
    "    text (str): O texto a ser limpo.\n",
    "\n",
    "    Returns:\n",
    "    str: O texto limpo.\n",
    "    \"\"\"\n",
    "    # Remover pontuações excessivas (mais de um ponto de interrogação, ponto, etc.)\n",
    "    text = re.sub(r'[\\?\\.!]+(?=[\\?\\.!])', '', text)\n",
    "\n",
    "    # Corrigir espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Aplicar a função de limpeza à coluna 'comment'\n",
    "df['comment_clean'] = df['comment'].apply(clean_text)\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame para verificar a aplicação da limpeza\n",
    "print(df[['comment', 'comment_clean']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento\n",
    "Haddi, Liu e Shi (2013) afirmam que pré-processar o texto significa limpá-lo e prepará-lo para classificação. Textos online geralmente possuem elementos extras como HTML, scripts e anúncios. Muitas palavras não carregam um peso significativo para entender ou classificar o texto. Removê-las ajuda na classificação e análise de sentimentos em tempo real. Dessa forma, o pré-processamento realizado inclui etapas como limpar caracteres especiais, remover palavras comuns, e outras técnicas que melhoram a análise.\n",
    "\n",
    "É importante reforçar que todos os processos realizados na etapa de pré-processamento estão melhores detalhados na seção 5. Pré-processamento dos Dados na documentação completa, disponível em [este link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapas do Pré-processamento\n",
    "Abaixo, foram realizadas as seguintes etapas de pré-processamento:\n",
    "- Conversão de emojis para palavras;\n",
    "- Remoção de URLs, tags HTML, caracteres especiais e números;\n",
    "- Tokenização;\n",
    "- Remoção de Stop-Words;\n",
    "- Stemming e lematização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment  sentiment\n",
       "0   1     That, my friend, is why The Mighty Swift Ra...          0\n",
       "1   2  Spent 20 minutes in an Uber listening to what ...          0\n",
       "2   3  via The Guardian  Guardian front page, Monday ...         -1\n",
       "3   4  My real job is being my girlfriends personal U...          0\n",
       "4   5               i had a bad drive . i want my refund         -1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando os dados\n",
    "classification_labeled = pd.read_csv('classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Exibindo os primeiros registros do DataFrame\n",
    "classification_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão de emojis para palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>converted_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0     That, my friend, is why The Mighty Swift Ra...   \n",
       "1  Spent 20 minutes in an Uber listening to what ...   \n",
       "2  via The Guardian  Guardian front page, Monday ...   \n",
       "3  My real job is being my girlfriends personal U...   \n",
       "4               i had a bad drive . i want my refund   \n",
       "\n",
       "                                    converted_emojis  \n",
       "0     That, my friend, is why The Mighty Swift Ra...  \n",
       "1  Spent 20 minutes in an Uber listening to what ...  \n",
       "2  via The Guardian  Guardian front page, Monday ...  \n",
       "3  My real job is being my girlfriends personal U...  \n",
       "4               i had a bad drive . i want my refund  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis(text):\n",
    "    '''\n",
    "    Converte emojis no texto para palavras.\n",
    "    Input: texto original como uma string.\n",
    "    Output: texto com emojis convertidos como uma string.\n",
    "    '''\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text\n",
    "\n",
    "# Aplicando a função convert_emojis e exibindo os resultados\n",
    "classification_labeled['converted_emojis'] = classification_labeled['comment'].apply(convert_emojis)\n",
    "classification_labeled[['comment', 'converted_emojis']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de URLs, tags HTML, caracteres especiais e números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>That my friend is why The Mighty Swift Radio C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>Spent minutes in an Uber listening to what I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>via The Guardian Guardian front page Monday Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>i had a bad drive i want my refund</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0     That, my friend, is why The Mighty Swift Ra...   \n",
       "1  Spent 20 minutes in an Uber listening to what ...   \n",
       "2  via The Guardian  Guardian front page, Monday ...   \n",
       "3  My real job is being my girlfriends personal U...   \n",
       "4               i had a bad drive . i want my refund   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  That my friend is why The Mighty Swift Radio C...  \n",
       "1  Spent minutes in an Uber listening to what I c...  \n",
       "2  via The Guardian Guardian front page Monday Ju...  \n",
       "3  My real job is being my girlfriends personal U...  \n",
       "4                 i had a bad drive i want my refund  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Limpa o texto removendo URLs, tags HTML, caracteres especiais e números através de expressões regulares.\n",
    "\n",
    "    Input: texto original como uma string.\n",
    "    Output: texto limpo, sem URLs, tags HTML, caracteres especiais e números, como uma string.\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove tags HTML\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove caracteres especiais e números\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Corrigir espaços extras\n",
    "\n",
    "    return text\n",
    "\n",
    "# Aplicando a função clean_text e exibindo os resultados\n",
    "classification_labeled['cleaned_text'] = classification_labeled['comment'].apply(clean_text)\n",
    "classification_labeled[['comment', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>[that, ,, my, friend, ,, is, why, the, mighty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>[spent, 20, minutes, in, an, uber, listening, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>[via, the, guardian, guardian, front, page, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>[my, real, job, is, being, my, girlfriends, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>[i, had, a, bad, drive, ., i, want, my, refund]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0     That, my friend, is why The Mighty Swift Ra...   \n",
       "1  Spent 20 minutes in an Uber listening to what ...   \n",
       "2  via The Guardian  Guardian front page, Monday ...   \n",
       "3  My real job is being my girlfriends personal U...   \n",
       "4               i had a bad drive . i want my refund   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [that, ,, my, friend, ,, is, why, the, mighty,...  \n",
       "1  [spent, 20, minutes, in, an, uber, listening, ...  \n",
       "2  [via, the, guardian, guardian, front, page, ,,...  \n",
       "3  [my, real, job, is, being, my, girlfriends, pe...  \n",
       "4    [i, had, a, bad, drive, ., i, want, my, refund]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Tokeniza o texto em palavras.\n",
    "    \n",
    "    Input: texto limpo como uma string.\n",
    "    Output: lista de palavras (tokens).\n",
    "    '''\n",
    "    tokens = word_tokenize(text.lower())  # Convertendo para minúsculas e tokenizando\n",
    "    return tokens\n",
    "\n",
    "# Aplicando a função tokenize e exibindo os resultados\n",
    "classification_labeled['tokenized_text'] = classification_labeled['comment'].apply(tokenize)\n",
    "classification_labeled[['comment', 'tokenized_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>[,, friend, ,, mighty, swift, radio, cars, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>[spent, 20, minutes, uber, listening, best, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>[via, guardian, guardian, front, page, ,, mond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>[real, job, girlfriends, personal, uber/uber, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>[bad, drive, ., want, refund]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0     That, my friend, is why The Mighty Swift Ra...   \n",
       "1  Spent 20 minutes in an Uber listening to what ...   \n",
       "2  via The Guardian  Guardian front page, Monday ...   \n",
       "3  My real job is being my girlfriends personal U...   \n",
       "4               i had a bad drive . i want my refund   \n",
       "\n",
       "                                   without_stopwords  \n",
       "0  [,, friend, ,, mighty, swift, radio, cars, sta...  \n",
       "1  [spent, 20, minutes, uber, listening, best, de...  \n",
       "2  [via, guardian, guardian, front, page, ,, mond...  \n",
       "3  [real, job, girlfriends, personal, uber/uber, ...  \n",
       "4                      [bad, drive, ., want, refund]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    '''\n",
    "    Remove stopwords do texto tokenizado.\n",
    "\n",
    "    Input: lista de palavras (tokens).\n",
    "    Output: lista de palavras sem stopwords.\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Aplicando a função remove_stopwords e exibindo os resultados\n",
    "classification_labeled['without_stopwords'] = classification_labeled['comment'].apply(tokenize).apply(remove_stopwords)\n",
    "classification_labeled[['comment', 'without_stopwords']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming e lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>stemmed_and_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>[,, friend, ,, mighti, swift, radio, car, stal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>[spent, 20, minut, uber, listen, best, describ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>[via, guardian, guardian, front, page, ,, mond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>[real, job, girlfriend, person, uber/ub, eat, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>[bad, drive, ., want, refund]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0     That, my friend, is why The Mighty Swift Ra...   \n",
       "1  Spent 20 minutes in an Uber listening to what ...   \n",
       "2  via The Guardian  Guardian front page, Monday ...   \n",
       "3  My real job is being my girlfriends personal U...   \n",
       "4               i had a bad drive . i want my refund   \n",
       "\n",
       "                              stemmed_and_lemmatized  \n",
       "0  [,, friend, ,, mighti, swift, radio, car, stal...  \n",
       "1  [spent, 20, minut, uber, listen, best, describ...  \n",
       "2  [via, guardian, guardian, front, page, ,, mond...  \n",
       "3  [real, job, girlfriend, person, uber/ub, eat, ...  \n",
       "4                      [bad, drive, ., want, refund]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_and_lemmatize(tokens):\n",
    "    '''\n",
    "    Aplica stemming e lematização nas palavras para reduzir à forma base.\n",
    "\n",
    "    Input: lista de palavras (tokens).\n",
    "    Output: lista de palavras processadas.\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# Aplicando a função stem_and_lemmatize e exibindo os resultados\n",
    "classification_labeled['stemmed_and_lemmatized'] = classification_labeled['comment'].apply(tokenize).apply(remove_stopwords).apply(stem_and_lemmatize)\n",
    "classification_labeled[['comment', 'stemmed_and_lemmatized']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>friend mighti swift radio car stalybridg retai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spent minut uber listen best describ eagl bsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>via guardian guardian front page monday juli u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>0</td>\n",
       "      <td>real job girlfriend person uberub eat driver e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>-1</td>\n",
       "      <td>bad drive want refund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>3519</td>\n",
       "      <td>Top story: Uber broke laws, duped police and s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>top stori uber broke law dupe polic secretli l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>3521</td>\n",
       "      <td>Top story: Uber broke laws, duped police and s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>top stori uber broke law dupe polic secretli l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>3522</td>\n",
       "      <td>Top story: Uber broke laws, duped police and s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>top stori uber broke law dupe polic secretli l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>3523</td>\n",
       "      <td>reading about this uber leak reminded me how f...</td>\n",
       "      <td>-1</td>\n",
       "      <td>read uber leak remind frustrat repeatedli hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>3526</td>\n",
       "      <td>It?s clear that Uber broke laws&amp;amp; behaved a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>clear uber broke lawsamp behav anticompetet ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            comment  sentiment  \\\n",
       "0        1     That, my friend, is why The Mighty Swift Ra...          0   \n",
       "1        2  Spent 20 minutes in an Uber listening to what ...          0   \n",
       "2        3  via The Guardian  Guardian front page, Monday ...         -1   \n",
       "3        4  My real job is being my girlfriends personal U...          0   \n",
       "4        5               i had a bad drive . i want my refund         -1   \n",
       "...    ...                                                ...        ...   \n",
       "2871  3519  Top story: Uber broke laws, duped police and s...         -1   \n",
       "2872  3521  Top story: Uber broke laws, duped police and s...         -1   \n",
       "2873  3522  Top story: Uber broke laws, duped police and s...         -1   \n",
       "2874  3523  reading about this uber leak reminded me how f...         -1   \n",
       "2875  3526  It?s clear that Uber broke laws&amp; behaved a...         -1   \n",
       "\n",
       "                                         processed_text  \n",
       "0     friend mighti swift radio car stalybridg retai...  \n",
       "1     spent minut uber listen best describ eagl bsid...  \n",
       "2     via guardian guardian front page monday juli u...  \n",
       "3     real job girlfriend person uberub eat driver e...  \n",
       "4                                 bad drive want refund  \n",
       "...                                                 ...  \n",
       "2871  top stori uber broke law dupe polic secretli l...  \n",
       "2872  top stori uber broke law dupe polic secretli l...  \n",
       "2873  top stori uber broke law dupe polic secretli l...  \n",
       "2874  read uber leak remind frustrat repeatedli hear...  \n",
       "2875  clear uber broke lawsamp behav anticompetet ye...  \n",
       "\n",
       "[2876 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_pipeline(dataframe, text_column):\n",
    "    '''\n",
    "    Pipeline de pré-processamento que aplica todas as funções acima ao texto.\n",
    "\n",
    "    Input: DataFrame e o nome da coluna de texto.\n",
    "    Output: DataFrame com o texto pré-processado.\n",
    "    '''\n",
    "    '''Aplicando funções de pré-processamento'''\n",
    "    dataframe['processed_text'] = dataframe[text_column].apply(lambda x: convert_emojis(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: clean_text(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: tokenize(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: remove_stopwords(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: stem_and_lemmatize(x))\n",
    "    dataframe['processed_text'] = dataframe['processed_text'].apply(lambda x: ' '.join(x))\n",
    "    return dataframe\n",
    "\n",
    "# Carregando os dados\n",
    "classification_labeled = pd.read_csv('classification-labeled.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Executando o pipeline de pré-processamento\n",
    "processed_df = preprocess_pipeline(classification_labeled, 'comment')\n",
    "\n",
    "# Salvando os dados processados em um novo arquivo CSV\n",
    "processed_df.to_csv('processed_text_data.csv', index=False)\n",
    "\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figura que Exemplifica o pré-processamento\n",
    "\n",
    "A imagem a seguir apresenta uma ilustração detalhada do pipeline de pré-processamento utilizado neste projeto. A figura inclui descrições de cada etapa envolvida no pré-processamento, bem como exemplos que demonstram como os dados são transformados em cada fase. Esta visualização facilita a compreensão do fluxo de tratamento dos dados e destaca as técnicas aplicadas para preparar os dados para análises subsequentes. A descrição da figura está melhor detalhada na documentação do projeto, a seção em questão pode ser acessada [nesse link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c5.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exemplificação do pré-processamento](assets/pipeline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "O \"Bag of Words\" é uma etapa importante no processamento de linguagem natural, pois converte texto em uma **representação numérica** que os modelos de aprendizado de máquina processam com maior facilidade. Esta técnica contabiliza a frequência de todas as palavras presentes no conjunto de dados, ignorando a ordem das palavras mas mantendo a informação sobre sua ocorrência. Isso facilita a identificação de padrões e tendências no uso de palavras, que são fundamentais para tarefas como classificação de texto e análise de sentimentos.\n",
    "\n",
    "Após a implementação desta métrica, aplicou-se o modelo **Naive Bayes** para análise dos dados e comparação dos resultados. Esta abordagem permite **avaliar a eficácia do modelo Naive Bayes** em classificar os dados com base na representação simplificada fornecida pelo \"Bag of Words\", destacando o impacto das características extraídas durante o pré-processamento.\n",
    "\n",
    "Todas as etapas do método de Bag of Words, bem com sua análise, pode ser acessada com mais profundidade na seção 6. Modelo de Bag of Words (IPYNB) na documentação completa, disponível em [este link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Bag of Words\n",
    "\n",
    "Nesta seção, implementa-se o modelo Bag of Words nos dados pré-processados. A aplicação do Bag of Words aos dados pré-processados permite uma análise mais estruturada e quantitativa, facilitando a aplicação de técnicas de aprendizado de máquina para tarefas como classificação e análise de sentimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>That, my friend, is why The Mighty Swift Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>friend mighti swift radio car stalybridg retai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spent 20 minutes in an Uber listening to what ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spent minut uber listen best describ eagl bsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>via The Guardian  Guardian front page, Monday ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>via guardian guardian front page monday juli u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>My real job is being my girlfriends personal U...</td>\n",
       "      <td>0</td>\n",
       "      <td>real job girlfriend person uberub eat driver e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>i had a bad drive . i want my refund</td>\n",
       "      <td>-1</td>\n",
       "      <td>bad drive want refund</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment  sentiment  \\\n",
       "0   1     That, my friend, is why The Mighty Swift Ra...          0   \n",
       "1   2  Spent 20 minutes in an Uber listening to what ...          0   \n",
       "2   3  via The Guardian  Guardian front page, Monday ...         -1   \n",
       "3   4  My real job is being my girlfriends personal U...          0   \n",
       "4   5               i had a bad drive . i want my refund         -1   \n",
       "\n",
       "                                      processed_text  \n",
       "0  friend mighti swift radio car stalybridg retai...  \n",
       "1  spent minut uber listen best describ eagl bsid...  \n",
       "2  via guardian guardian front page monday juli u...  \n",
       "3  real job girlfriend person uberub eat driver e...  \n",
       "4                              bad drive want refund  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text_data = pd.read_csv('processed_text_data.csv')\n",
    "processed_text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aa  aal  aapl  ab  abbotsford  abbott  abet  abil  abl  abroad  ...  \\\n",
      "0      0    0     0   0           0       0     0     0    0       0  ...   \n",
      "1      0    0     0   0           0       0     0     0    0       0  ...   \n",
      "2      0    0     0   0           0       0     0     0    0       0  ...   \n",
      "3      0    0     0   0           0       0     0     0    0       0  ...   \n",
      "4      0    0     0   0           0       0     0     0    0       0  ...   \n",
      "...   ..  ...   ...  ..         ...     ...   ...   ...  ...     ...  ...   \n",
      "2871   0    0     0   0           0       0     0     0    0       0  ...   \n",
      "2872   0    0     0   0           0       0     0     0    0       0  ...   \n",
      "2873   0    0     0   0           0       0     0     0    0       0  ...   \n",
      "2874   0    0     0   0           0       0     0     0    0       0  ...   \n",
      "2875   0    0     0   0           0       0     0     0    1       0  ...   \n",
      "\n",
      "      zero  zidi  zie  zimbabw  zimbabwean  zip  zisil  zone  zoom  zumba  \n",
      "0        0     0    0        0           0    0      0     0     0      0  \n",
      "1        0     0    0        0           0    0      0     0     0      0  \n",
      "2        0     0    0        0           0    0      0     0     0      0  \n",
      "3        0     0    0        0           0    0      0     0     0      0  \n",
      "4        0     0    0        0           0    0      0     0     0      0  \n",
      "...    ...   ...  ...      ...         ...  ...    ...   ...   ...    ...  \n",
      "2871     0     0    0        0           0    0      0     0     0      0  \n",
      "2872     0     0    0        0           0    0      0     0     0      0  \n",
      "2873     0     0    0        0           0    0      0     0     0      0  \n",
      "2874     0     0    0        0           0    0      0     0     0      0  \n",
      "2875     0     0    0        0           0    0      0     0     0      0  \n",
      "\n",
      "[2876 rows x 5023 columns]\n"
     ]
    }
   ],
   "source": [
    "# A seguir é realizado o modelo Bag of Words nos dados pré-processados\n",
    "\n",
    "def generate_bow(dataframe, text_column):\n",
    "    \"\"\"\n",
    "    Gera um DataFrame Bag of Words a partir de uma coluna de texto especificada de um DataFrame.\n",
    "\n",
    "    input:\n",
    "    dataframe (pd.DataFrame): DataFrame contendo a coluna de texto.\n",
    "    text_column (str): Nome da coluna de texto que será processada para criar o Bag of Words.\n",
    "\n",
    "    output:\n",
    "    pd.DataFrame: DataFrame representando o Bag of Words com cada palavra como uma coluna e cada documento como uma linha.\n",
    "    \"\"\"\n",
    "    # Inicializa o vetorizador de contagem\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Ajusta e transforma os dados da coluna de texto especificada\n",
    "    X = vectorizer.fit_transform(dataframe[text_column])\n",
    "\n",
    "    # Obtém o vocabulário\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Cria um DataFrame a partir da matriz esparsa, com o vocabulário como colunas\n",
    "    vectorized_data = pd.DataFrame(X.toarray(), columns=vocab)\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "# Aplicar a função ao DataFrame\n",
    "vectorized_data = generate_bow(processed_text_data, 'processed_text')\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "print(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aal</th>\n",
       "      <th>aapl</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbotsford</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abet</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abroad</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zidi</th>\n",
       "      <th>zie</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zip</th>\n",
       "      <th>zisil</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5023 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aal  aapl  ab  abbotsford  abbott  abet  abil  abl  abroad  ...  zero  \\\n",
       "0   0    0     0   0           0       0     0     0    0       0  ...     0   \n",
       "1   0    0     0   0           0       0     0     0    0       0  ...     0   \n",
       "2   0    0     0   0           0       0     0     0    0       0  ...     0   \n",
       "3   0    0     0   0           0       0     0     0    0       0  ...     0   \n",
       "4   0    0     0   0           0       0     0     0    0       0  ...     0   \n",
       "\n",
       "   zidi  zie  zimbabw  zimbabwean  zip  zisil  zone  zoom  zumba  \n",
       "0     0    0        0           0    0      0     0     0      0  \n",
       "1     0    0        0           0    0      0     0     0      0  \n",
       "2     0    0        0           0    0      0     0     0      0  \n",
       "3     0    0        0           0    0      0     0     0      0  \n",
       "4     0    0        0           0    0      0     0     0      0  \n",
       "\n",
       "[5 rows x 5023 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvando os dados processados em um novo arquivo CSV\n",
    "vectorized_data.to_csv('vectorized_text_data.csv', index=False)\n",
    "vectorized_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Métricas do Modelo Bag of Words\n",
    "\n",
    "Este seção fornece uma análise das métricas associadas ao DataFrame do modelo Bag of Words. Avalia-se a predominância de zeros, a utilização de colunas e o tamanho do arquivo gerado a partir dos dados processados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Número de Zeros\n",
    "A matriz do modelo Bag of Words geralmente contém uma alta quantidade de zeros, indicando a ausência de certas palavras nos documentos. Este código calcula o total de elementos no DataFrame, identifica a quantidade de zeros e determina a porcentagem de zeros em relação ao total de elementos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A porcentagem de 0s na matriz é de 99.76%.\n"
     ]
    }
   ],
   "source": [
    "total_elements = vectorized_data.size\n",
    "total_zeros = (vectorized_data == 0).sum().sum()\n",
    "zero_percentage = (total_zeros / total_elements) * 100\n",
    "\n",
    "print(f\"A porcentagem de 0s na matriz é de {zero_percentage:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Colunas Utilizadas\n",
    "O número de colunas em um DataFrame Bag of Words corresponde ao tamanho do vocabulário. Este código extrai o número de colunas do DataFrame, que também representa a quantidade de palavras distintas identificadas no conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tamanho do vocabulário criado é de 5023 palavras.\n"
     ]
    }
   ],
   "source": [
    "total_columns = vectorized_data.shape[1]\n",
    "\n",
    "print(f\"A tamanho do vocabulário criado é de {total_columns} palavras.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tamanho do Arquivo\n",
    "O tamanho do arquivo gerado por este DataFrame é uma métrica para entender o armazenamento necessário. Para estimar o tamanho do arquivo é usanda a função `getsizeof` do módulo `sys`, estima-se o tamanho aproximado em bytes do DataFrame em memória, que pode ser um indicativo do tamanho quando salvo em disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do arquivo com a matriz criada é de 115569348 bythes.\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "file_size = getsizeof(vectorized_data)\n",
    "\n",
    "print(f\"O tamanho do arquivo com a matriz criada é de {file_size} bythes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise do modelo Bag of Words revela informações sobre a eficiência e a viabilidade do modelo em termos de armazenamento e processamento. A predominância de zeros sugere uma matriz esparsa, o que é típico em modelos de texto. O número de colunas nos fornece uma ideia sobre a diversidade do vocabulário, enquanto o tamanho estimado do arquivo nos ajuda a avaliar requisitos de armazenamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Nesta seção, apresenta-se o código utilizado para implementar o modelo Naive Bayes, utilizando os dados transformados pelo modelo Bag of Words. Este modelo é aplicado para classificar os dados com base nas características extraídas, permitindo avaliar a eficácia do Bag of Words na preparação dos dados para análises preditivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.88      0.77      0.82       417\n",
      "           0       0.39      0.61      0.48       122\n",
      "           1       0.11      0.05      0.07        37\n",
      "\n",
      "    accuracy                           0.69       576\n",
      "   macro avg       0.46      0.48      0.46       576\n",
      "weighted avg       0.73      0.69      0.70       576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Carrega os dados processados que contêm o Bag of Words\n",
    "vectorized_data = pd.read_csv('vectorized_text_data.csv')\n",
    "\n",
    "# Carrega os dados originais para adicionar a coluna 'sentiment'\n",
    "original_data = pd.read_csv('processed_text_data.csv')\n",
    "\n",
    "# Garante que os dados estão na mesma ordem, assumindo que as linhas não foram embaralhadas\n",
    "vectorized_data['sentiment'] = original_data['sentiment']\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data.drop('sentiment', axis=1), vectorized_data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Criação do modelo Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões e avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise das Métricas do Modelo Naive Bayes\n",
    "Nesta seção, são apresentadas as métricas de avaliação do modelo Naive Bayes, com o desempenho alcançado. As métricas incluem acurácia, matriz de confusão, precisão, revocação e pontuação F1, fornecendo uma análise da eficiência do modelo em classificar os dados processados pelo Bag of Words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 3. Precision, Recall, F1-Score\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# 4. ROC Curve and AUC - Note: This requires binary or binary-converted labels\n",
    "if len(set(y_test)) == 2:  # Check if the problem is binary classification\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC and AUC not applicable for multi-class classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo das Métricas do Modelo Naive Bayes\n",
    "\n",
    "Abaixo está um breve resumo da análise das métricas, que podem ser melhor observadas dentro da documentação desenvolvida pela equipe *Thunder*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia:\n",
    "\n",
    "- **Valor**: 0.69\n",
    "- **Interpretação**: O modelo acerta em 69% dos casos em todo o conjunto de dados. Esta acurácia sugere que o modelo possui uma habilidade moderada de prever corretamente as classes.\n",
    "\n",
    "### Matriz de Confusão:\n",
    "``` python\n",
    "[[322, 90, 5],\n",
    " [35, 75, 12],\n",
    " [7, 28, 2]]\n",
    "```\n",
    "\n",
    "- **Interpretação**: A matriz mostra que o modelo performa bem para a classe -1, conseguindo uma boa taxa de previsões corretas. No entanto, para as classes 0 e 1, o modelo enfrenta mais dificuldades, especialmente na classe 1, onde há um número significativamente baixo de previsões corretas.\n",
    "\n",
    "### Relatórios de Classificação:\n",
    "\n",
    "**Precision**\n",
    "- Classe -1: 0.88 (Alta, eficaz em identificar positivos reais para esta classe)\n",
    "- Classe 0: 0.39 (Moderada, com muitos falsos positivos)\n",
    "- Classe 1: 0.11 (Baixa, indica muitos falsos positivos)\n",
    "\n",
    "**Recall**\n",
    "- Classe -1: 0.77 (Boa, a maior parte dos casos positivos são identificados corretamente)\n",
    "- Classe 0: 0.61 (Moderada, falha em capturar alguns casos verdadeiros)\n",
    "- Classe 1: 0.05 (Muito baixa, falha em identificar a maioria dos casos verdadeiros)\n",
    "\n",
    "**F1-Score**\n",
    "- Classe -1: 0.82 (Alto, indica um bom equilíbrio entre precisão e revocação)\n",
    "- Classe 0: 0.48 (Moderado)\n",
    "- Classe 1: 0.07 (Muito baixo, reflete o fraco desempenho em precisão e revocação)\n",
    "\n",
    "## Médias:\n",
    "\n",
    "- **Média Macro**: Precisão, revocação e F1-score são todos aproximadamente 0.46, o que indica um desempenho mais equilibrado quando não se considera o desbalanceamento entre as classes.\n",
    "- **Média Ponderada**: A precisão ponderada, revocação e F1-score são cerca de 0.70, sugerindo um ajuste do modelo que favorece a classe com mais amostras.\n",
    "\n",
    "## Curva ROC e AUC\n",
    "- Não aplicável nessa situação, pois o problema envolve classificação multiclasse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "Segue as referências utilizadas durante a execução deste Documento Jupyther."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AssemblyAI. \"How to implement Naive Bayes from scratch with Python.\" YouTube, publicado por AssemblyAI. Acesso em: 05 maio 2024. Disponível em: https://www.youtube.com/watch?v=TLnuAorxqE\n",
    "\n",
    "2. Ciência dos Dados. \"COMO FAZER ANÁLISE DE SENTIMENTOS COM DADOS TEXTUAIS.\" YouTube, publicado por Ciência dos Dados. Acesso em: 06 maio 2024. Disponível em: https://www.youtube.com/watch?v=Wq-n7ZPce8k\n",
    "\n",
    "3. Google Cloud Tech. \"Getting started with Natural Language Processing: Bag of words.\" YouTube, publicado por Google Cloud Tech. Acesso em: 07 maio 2024. Disponível em: https://www.youtube.com/watch?v=UFtxy0KRvVI\n",
    "\n",
    "4. Google Cloud LATAM. \"Processamento de linguagem natural (PLN) | Decodificando a Nuvem | PT-BR.\" YouTube, publicado por Google Cloud LATAM. Acesso em: 08 maio 2024. Disponível em: https://www.youtube.com/watch?v=Dt2DRQN6uwg\n",
    "\n",
    "5. Mendes, Eduardo. \"spaCy: Introdução a Processamento de Linguagem Natural | Live de Python #226.\" YouTube, publicado por Eduardo Mendes. Acesso em: 09 maio 2024. Disponível em: https://www.youtube.com/watch?v=Vr9QXpELdrs\n",
    "\n",
    "6. ProgrammingKnowledge. \"Introduction To Natural Language Toolkit (NLTK) | NLTK Tutorial in Python.\" YouTube, publicado por ProgrammingKnowledge. Acesso em: 10 maio 2024. Disponível em: https://www.youtube.com/watch?v=WYgeOKZBhe0\n",
    "\n",
    "7. \"PLN Avançado com SpaCy: Capítulo 1: Selecionando palavras, frases, nomes e alguns conceitos.\" Acesso em: 29 abril 2024. Disponível em: https://course.spacy.io/pt/chapter1\n",
    "\n",
    "8. \"Dive into Deep Learning: 15.1. Análise de Sentimentos e o Dataset.\" Acesso em: 30 abril 2024. Disponível em: https://pt.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html\n",
    "\n",
    "9. \"Medium: A Simple Explanation of the Bag-of-Words Model.\" Acesso em: 01 maio 2024. Disponível em: https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971\n",
    "\n",
    "10. \"FreeCodeCamp: An introduction to Bag of Words and how to code it in Python for NLP.\" Acesso em: 02 maio 2024. Disponível em: https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04\n",
    "\n",
    "11. \"GitHub Docs: Licenciar um repositório.\" Acesso em: 03 maio 2024. Disponível em: https://docs.github.com/pt/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository#disclaimer\n",
    "\n",
    "12. \"NLTK Documentation: Sample usage for portuguese_en.\" Acesso em: 04 maio 2024. Disponível em: https://www.nltk.org/howto/portuguese_en.html\n",
    "\n",
    "13. \"Medium: A Hitchhiker’s Guide to Sentiment Analysis using Naive-Bayes Classifier.\" Acesso em: 05 maio 2024. Disponível em: https://towardsdatascience.com/a-hitchhikers-guide-to-sentiment-analysis-using-naive-bayes-classifier-b921c0fb694\n",
    "\n",
    "14. \"geeksforgeeks: Programming Paradigms in Python.\" Acesso em: 06 maio 2024. Disponível em: https://www.geeksforgeeks.org/programming-paradigms-in-python/\n",
    "\n",
    "15. \"geeksforgeeks: Python Data Structures.\" Acesso em: 07 maio 2024. Disponível em: https://www.geeksforgeeks.org/python-data-structures/\n",
    "\n",
    "16. ChatGPT 3.5. OpenAI, utilizado em várias interações e consultas durante o desenvolvimento do projeto. Acesso em: 08 maio 2024.\n",
    "\n",
    "17. ChatGPT 4. OpenAI, utilizado em várias interações e consultas durante o desenvolvimento do projeto. Acesso em: 08 maio 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
