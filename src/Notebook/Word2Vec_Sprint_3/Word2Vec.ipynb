{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções para o uso do Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar o notebook a seguir corretamente, é necessário seguir alguns passos prévios. Primeiro, baixe os datasets disponíveis no link a seguir: [Datasets](https://drive.google.com/drive/folders/1bm6iQenyZ63gbw_s7j0md8UaHP84O0Vn?usp=sharing) (não é necessário todos, a princípio, somente os citados abaixo). Esses datasets não estão incluídos no notebook devido ao seu tamanho, por isso são ignorados pelo arquivo `.gitignore`. Todos eles devem ser colocados na pasta [`data`](data), localizada em `src/Notebook/Word2Vec_Sprint_3/data`.\n",
    "\n",
    "Os arquivos necessários são:\n",
    "- **GoogleNews-vectors-negative300.bin**: [Download](https://drive.google.com/open?id=1IzDDngIEWzGP4onWlBCglAQ87g5gP2CL&usp=drive_copy) - contém palavras pré-treinadas em 300 dimensões.\n",
    "- **processed_text_data.csv**: [Download](https://drive.google.com/file/d/1hCW3pliSKDWKyr0h0tvNBtUgtw9EsE3c/view?usp=sharing) - contém o banco de dados processado (a etapa de pré-processamento pode ser acompanhado no arquivo [BOW.ipynb](../BoW_Sprint_2/BOW.ipynb)).\n",
    "\n",
    "Todos são necessários para rodar os modelos de Word2Vec.\n",
    "\n",
    "Outras recomendações:\n",
    "\n",
    "- Mantenha o C++ do seu computador atualizado, pois determinadas bibliotecas são compiladas com essa linguagem.\n",
    "- Caso queira conferir as versões mais atualizaas das bibliotecas, elas estão detalhadas no aquivo [``requirements.txt``](../../requirements.txt). Para usar este arquivo, basta rodar no terminal o comando: ``pip install -r requirements.txt``. Porém, a princípio, é possível ter acesso a todas as bilioteca ao rodar a célula de \"Instalação e Importação das Bibliotecas\" deste Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação e Importação das Bibliotecas\n",
    "\n",
    "Na seção \"Instalação e Importação das Bibliotecas\", prepara-se o ambiente de codificação configurando e carregando as bibliotecas necessárias para o projeto. Esta seção é importante pelos seguintes motivos:\n",
    "\n",
    "1. **Instalação de Bibliotecas**: Utilizam-se comandos como `%pip install nome_da_biblioteca` para instalar bibliotecas Python que não estão presentes por padrão no ambiente de execução, mas que são essenciais para a execução do código. Este comando mágico oferece uma integração mais direta com o IPython, garantindo que as instalações ocorram no kernel correto do Python utilizado pelo Jupyter.\n",
    "\n",
    "2. **Importação de Bibliotecas**: Após a instalação, as bibliotecas são importadas usando o comando `import`, permitindo o acesso às funções e ferramentas disponibilizadas por elas. Comandos típicos incluem `import numpy as np` e `import pandas as pd`. Esta prática garante que todas as funcionalidades necessárias estejam disponíveis e prontas para uso nas seções subsequentes do notebook.\n",
    "\n",
    "Esta seção é posicionada no início do notebook para assegurar que todas as dependências estejam corretamente configuradas antes de prosseguir com a análise de dados ou modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de \"Bibliotecas Mágicas\"\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install emoji\n",
    "%pip install wordcloud\n",
    "%pip install emot\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "%pip install scipy==1.11\n",
    "%pip install gensim\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tempfile\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Word2Vec é uma técnica de aprendizado de máquina que transforma palavras em vetores numéricos, onde palavras com significados semelhantes têm representações próximas. Utilizando modelos de redes neurais, captura a semântica das palavras, sendo crucial para tarefas de processamento de linguagem natural, como análise de sentimentos, tradução automática e chatbots, devido à sua eficiência e capacidade de melhorar a compreensão contextual em grandes corpora de texto.\n",
    "\n",
    "Essa seção contém:\n",
    "- **Word2Vec de 300 dimensões**: Embeddings de palavras representadas por vetores de 300 dimensões, proporcionando uma representação mais detalhada das palavras.\n",
    "- **Análise das métricas do Modelo Word2Vec de 300 dimensões**: Avaliação do desempenho do modelo de 300 dimensões.\n",
    "\n",
    "No projeto em questão, o dataset de treinamento está em inglês, justificando a escolha do modelo `GoogleNews-vectors-negative300.bin` para garantir uma melhor adequação e desempenho nas tarefas de PLN específicas. Além disso, maior dimensionalidade permite que o modelo capture mais nuances semânticas e relacionais entre as palavras, tendem a se sair melhor em tarefas complexas de PLN e podem generalizar melhor e representar com mais precisão as relações entre palavras.\n",
    "\n",
    "A documentação dessa seção, com suas análises descritiva e crítica, pode ser acessada [nesse link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 300 dimensões \n",
    "Essa seção contém o uso de Word2Vec de 300 dimensões, utilizando o conjunto de vetores pré-treinados GoogleNews em inglês, especificamente o arquivo `GoogleNews-vectors-negative300.bin`, que pode ser acessado [nesse link](https://drive.google.com/file/d/1IzDDngIEWzGP4onWlBCglAQ87g5gP2CL/view?usp=drive_link). As subseções incluem:\n",
    "\n",
    "- **Vetores de Palavras Pré-Treinadas 300 dimensões**: Carregamento e preparação dos vetores de palavras com 300 dimensões pré-treinados em inglês.\n",
    "- **Modelo Word2Vec em 300 dimensões**: Implementação do modelo Word2Vec utilizando os vetores GoogleNews de 300 dimensões.\n",
    "- **Teste isolado com conjunto de dados fictício**: Avaliação do modelo com um conjunto de dados fictício para validar a eficácia dos vetores de 300 dimensões.\n",
    "\n",
    "Orientação: para rodar esse modelo, deve-se baixar o aquivo `GoogleNews-vectors-negative300.bin` no link oferecido e colocá-lo na pasta [data](data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetores de Palavras Pré-Treinadas 300 dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './data/GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Word2Vec 300 dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_word2vec_vectors(csv_file_path, embedding_file):\n",
    "    \"\"\"\n",
    "    Carrega dados de um arquivo CSV e utiliza um modelo Word2Vec pré-treinado para\n",
    "    gerar representações vetoriais dos textos.\n",
    "\n",
    "    Inputs:\n",
    "        csv_file_path (str): Caminho para o arquivo CSV contendo a coluna de texto.\n",
    "        embedding_file (str): Caminho para o arquivo binário do modelo Word2Vec pré-treinado.\n",
    "\n",
    "    Output: DataFrame: DataFrame com os vetores de palavras para cada documento.\n",
    "\n",
    "    \"\"\"\n",
    "    # Carregar o modelo pré-treinado\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "\n",
    "    # Ler o arquivo CSV\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Assume-se que a coluna com texto processado é chamada 'processed_text'\n",
    "    texts = data['processed_text'].astype(str)\n",
    "\n",
    "    # Inicializar um vetorizador para tokenizar os textos\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')  # Tokeniza palavras\n",
    "    vectorizer.fit(texts)\n",
    "\n",
    "    # Inicializar matriz para armazenar vetores de palavras\n",
    "    word_embeddings = np.zeros((len(texts), word_vectors.vector_size))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = vectorizer.build_tokenizer()(text)\n",
    "        word_vecs = [word_vectors[word] for word in tokens if word in word_vectors.key_to_index]\n",
    "        if word_vecs:\n",
    "            word_embeddings[i] = np.sum(word_vecs, axis=0)  \n",
    "\n",
    "    # Criar um DataFrame com os vetores\n",
    "    return pd.DataFrame(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da Função\n",
    "embedding_file = './data/GoogleNews-vectors-negative300.bin'\n",
    "csv_file_path = './data/processed_text_data.csv'\n",
    "vector_df = prepare_word2vec_vectors(csv_file_path, embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma em arquivo csv\n",
    "vector_df.to_csv('./data/word_2_vec_text_data_300.csv', index=False)\n",
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste isolado com conjunto de dados fictício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O teste abaixo verifica se o tamanho da matriz está correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_dict = {\n",
    "'processed_text': [\n",
    "    'uber is cool',\n",
    "    'i do not like uber',\n",
    "    'data science'\n",
    "]\n",
    "}\n",
    "data_text = pd.DataFrame(data_dict)\n",
    "\n",
    "# Criar um arquivo CSV temporário\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_csv:\n",
    "    csv_input_path = temp_csv.name\n",
    "    data_text.to_csv(csv_input_path, index=False)\n",
    "\n",
    "\n",
    "embedding_file = './data/GoogleNews-vectors-negative300.bin'\n",
    "csv_file_path = csv_input_path\n",
    "vector_df = prepare_word2vec_vectors(csv_file_path, embedding_file)\n",
    "\n",
    "# Printar somente a saída de vetor no final\n",
    "print(vector_df)\n",
    "\n",
    "# Remover o arquivo temporário\n",
    "os.remove(csv_input_path)\n",
    "\n",
    "# Deveria retornar um arquivos de 3 linhas e 300 colunas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O teste abaixo pretende verificar se o vetor da frase completa é igual à soma dos vetores das palavras individuais da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_uber = word_vectors['uber']\n",
    "v_uber[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_is = word_vectors['is']\n",
    "v_is[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_good = word_vectors['good']\n",
    "v_good[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_test = v_uber + v_is + v_good\n",
    "v_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'uber is good'\n",
    "\n",
    "# Criar um arquivo CSV temporário com a frase completa\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_csv_sentence:\n",
    "    csv_sentence_path = temp_csv_sentence.name\n",
    "    pd.DataFrame({'processed_text': [sentence]}).to_csv(csv_sentence_path, index=False)\n",
    "\n",
    "\n",
    "embedding_file = './data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Obter o vetor da frase completa usando a função\n",
    "vector_df_sentence = prepare_word2vec_vectors(csv_sentence_path, embedding_file)\n",
    "full_text_vector = vector_df_sentence.iloc[0].values\n",
    "\n",
    "# Visualizar a matriz da frase completa (primeiras colunas)\n",
    "print(\"Vetor da frase completa 'uber is good':\")\n",
    "\n",
    "print(full_text_vector[0])\n",
    "\n",
    "# Remover os arquivos temporários\n",
    "os.remove(csv_sentence_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do Word2Vec de 300 dimensões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Clusters\n",
    "- Análise de Agrupamento (Clustering) com K-means\n",
    "- Visualização dos Vetores com t-SNE\n",
    "- Distribuição dos vetores de palavras entre os diferentes clusters.\n",
    "\n",
    "Ambos os métodos de visualização complementam-se e fornecem uma compreensão mais profunda da estrutura e dos relacionamentos entre os vetores de palavras. A aplicação desses métodos pode ser útil para várias tarefas de NLP, como melhoria de modelos, análise semântica e exploração de dados.\n",
    "\n",
    "A descrição desses gráficos, bem como sua análise, podem ser acessadas na seção 7.2.1 Análise Gráfica do Modelo Word2Vec da documentação oficial do projeto, disponível [nesse link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c7.2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo CSV\n",
    "file_path = './data/word_2_vec_text_data_300.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Definir o número de clusters\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data)\n",
    "\n",
    "# Adicionar os rótulos dos clusters ao DataFrame\n",
    "data['cluster'] = kmeans.labels_\n",
    "\n",
    "# Reduzir a dimensionalidade para visualização com PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data.iloc[:, :-1])\n",
    "\n",
    "# Visualizar os clusters (PCA)\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_clusters):\n",
    "    points = reduced_data[data['cluster'] == i]\n",
    "    plt.scatter(points[:, 0], points[:, 1], label=f'Cluster {i}')\n",
    "plt.title(\"Clusters de Vetores de Palavras (PCA)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduzir a dimensionalidade para 2D usando t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_results = tsne.fit_transform(data.iloc[:, :-1])\n",
    "\n",
    "# Visualizar os vetores de palavras (t-SNE)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=data['cluster'], cmap='viridis')\n",
    "plt.title(\"Visualização t-SNE dos Vetores de Palavras\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de vetores em cada cluster\n",
    "cluster_counts = data['cluster'].value_counts()\n",
    "\n",
    "# Plotar o histograma da distribuição dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "cluster_counts.plot(kind='bar')\n",
    "plt.title(\"Distribuição dos Vetores de Palavras entre os Clusters\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Número de Vetores de Palavras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento dos Modelos\n",
    "\n",
    "A seguir, apresenta-se o código da função para a vetorização do banco de dados utilizando Word2Vec (chamada da função) e a criação do dataset para o treinamento dos modelos de machine learning. Além disso, incluímos os códigos dos modelos escolhidos para o processamento de linguagem natural. Os modelos são:\n",
    "\n",
    "1. Naive Bayes\n",
    "2. Random Forest\n",
    "3. SVM\n",
    "4. XGBoost\n",
    "5. Embedding Layers\n",
    "\n",
    "A análise detalhada de cada um dos modelos pode ser acessada na documentação, [neste link](https://github.com/Inteli-College/2024-1B-T10-SI06-G03/blob/main/docs/documentacao.md#c7.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do Dataset de testagem dos Modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentiment_column(classification_csv, word2vec_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Carrega dois arquivos CSV, adiciona a coluna 'sentiment' do primeiro arquivo\n",
    "    ao segundo arquivo, e salva o resultado em um novo arquivo CSV.\n",
    "\n",
    "    Args:\n",
    "    classification_csv (str): Caminho para o arquivo CSV contendo a coluna 'sentiment'.\n",
    "    word2vec_csv (str): Caminho para o arquivo CSV contendo os vetores de palavras.\n",
    "    output_csv (str): Caminho para o arquivo CSV onde o resultado será salvo.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Carregar os arquivos CSV\n",
    "    classification_text_data = pd.read_csv(classification_csv)\n",
    "    word_2_vec_data = pd.read_csv(word2vec_csv)\n",
    "\n",
    "    # Selecionar a coluna 'sentiment' do arquivo classification_text_data\n",
    "    sentiment_column = classification_text_data['sentiment']\n",
    "\n",
    "    # Adicionar a coluna 'sentiment' no arquivo word_2_vec_data na primeira coluna (coluna 0)\n",
    "    word_2_vec_data.insert(0, 'sentiment', sentiment_column)\n",
    "\n",
    "    # Verificar o resultado\n",
    "    print(word_2_vec_data.head())\n",
    "\n",
    "    # Salvar o novo DataFrame em um novo arquivo CSV\n",
    "    word_2_vec_data.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamando a função que 1: vetoriza o banco de dados utilizando o Word2Vec e 2: cria o dataset para treinar os modelos \n",
    "merge_sentiment_column('./data/processed_text_data.csv', './data/word_2_vec_text_data_300.csv', './data/data_for_training_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, utilizaremos o modelo Gaussian Naive Bayes para classificar as frases como positivas ou negativas.\n",
    "\n",
    "<b> Diferença entre `MultiNomialNB` e `GaussianNB` </b>\n",
    " Como nossos dados são contínuos e possuem valores negativos, eles acabam sendo inapropriados para uso direto com `MultinomialNB`. \n",
    " \n",
    " Para esses tipos de dados, modelos como o GaussianNB, que presume que as características seguem uma distribuição normal, são mais adequados.\n",
    "\n",
    " Vale ressaltar que o `MultiNomialNB` é utilizada para:\n",
    "\n",
    " - Dados Discretos: Sendo ideal para dados que representam contagens ou frequências de eventos. Por exemplo, é comum em tarefas de classificação de texto onde os recursos são normalmente as contagens ou frequências de termos (bag of words).\n",
    "\n",
    "- Dados Positivos: Ele requer que os recursos sejam não-negativos, pois representa a frequência de ocorrência de características. Em contextos onde os recursos podem ser negativos, como é o caso em muitos conjuntos de dados de características contínuas ou normalizadas, o MultinomialNB não seria adequado sem transformações prévias que garantam a não-negatividade.\n",
    "\n",
    "Para ver o resultado obtido, basta rodar o código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report # não modificar o local dessa importação\n",
    "import pickle\n",
    "\n",
    "def train_and_save_model(file_path, model_save_path):\n",
    "    \"\"\"\n",
    "    Treina e salva um modelo de classificação Naive Bayes em um arquivo.\n",
    "\n",
    "    Input:\n",
    "    file_path (str): O caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "    model_save_path (str): O caminho para salvar o arquivo do modelo treinado.\n",
    "\n",
    "    Output:\n",
    "    dict: Um dicionário contendo a acurácia do modelo, o relatório de classificação, e os conjuntos de teste.\n",
    "    \"\"\"\n",
    "    # Lendo os dados do arquivo CSV\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Dividindo os dados em conjuntos de treino e teste\n",
    "    X = data.drop('sentiment', axis=1)\n",
    "    y = data['sentiment']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Modelo de classificação\n",
    "    model = GaussianNB()\n",
    "\n",
    "    # Treinando o modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Salvando o modelo treinado com pickle\n",
    "    with open(model_save_path, 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "\n",
    "    # Predições no conjunto de teste\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Avaliação do modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"classification_report\": class_report,\n",
    "        \"X_test\": X_test,  # Adiciona X_test ao retorno para uso posterior\n",
    "        \"y_test\": y_test   # Adiciona y_test ao retorno para uso posterior\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict(model_load_path, X_test):\n",
    "    \"\"\"\n",
    "    Carrega um modelo salvo e faz previsões no conjunto de teste.\n",
    "\n",
    "    Input:\n",
    "    model_load_path (str): O caminho para o arquivo do modelo salvo.\n",
    "    X_test (DataFrame): Conjunto de dados de teste.\n",
    "\n",
    "    Output:\n",
    "    array: Predições feitas pelo modelo carregado.\n",
    "    \"\"\"\n",
    "    # Carregar o modelo salvo com pickle\n",
    "    with open(model_load_path, 'rb') as model_file:\n",
    "        loaded_model = pickle.load(model_file)\n",
    "\n",
    "    # Fazer previsões usando o modelo carregado\n",
    "    return loaded_model.predict(X_test)\n",
    "\n",
    "# Treinando e salvando o modelo\n",
    "result = train_and_save_model('./data/data_for_training_models.csv', './models/naive_bayes_model.pkl')\n",
    "print(\"Acurácia:\", result[\"accuracy\"])\n",
    "print(\"Relatório de Classificação:\\n\", result[\"classification_report\"])\n",
    "\n",
    "# Carregar o modelo salvo e fazer previsões\n",
    "y_pred_loaded_model = load_and_predict('./models/naive_bayes_model.pkl', result[\"X_test\"])\n",
    "\n",
    "# Avaliação das previsões do modelo carregado\n",
    "accuracy_loaded_model = accuracy_score(result[\"y_test\"], y_pred_loaded_model)\n",
    "class_report_loaded_model = classification_report(result[\"y_test\"], y_pred_loaded_model)\n",
    "\n",
    "print(\"Acurácia do Modelo Carregado:\", accuracy_loaded_model)\n",
    "print(\"Relatório de Classificação do Modelo Carregado:\\n\", class_report_loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo Random Forest é uma técnica de aprendizado de máquina utilizada em problemas de classificação e regressão. É uma extensão do método de Árvores de Decisão, uma abordagem de aprendizado supervisionado que cria uma árvore de decisão com base nas características dos dados de treinamento. No entanto, em vez de depender de uma única árvore de decisão, o modelo Random Forest constrói uma \"floresta\" de árvores de decisão e combina suas previsões para obter uma previsão final.\n",
    "\n",
    "A principal ideia por trás do modelo Random Forest é criar uma variedade de árvores de decisão durante o treinamento, cada uma delas sendo treinada com uma amostra aleatória dos dados de treinamento e com um conjunto aleatório de características. Isso é conhecido como bootstrap aggregating, ou bagging, e ajuda a reduzir a variância do modelo, melhorando sua capacidade de generalização. \n",
    "\n",
    "Além disso, durante a divisão de cada nó em uma árvore de decisão, o modelo Random Forest seleciona um subconjunto aleatório das características, em vez de usar todas elas. Isso ajuda a reduzir a correlação entre as árvores individuais na floresta, tornando o modelo mais robusto a overfitting e melhorando sua capacidade de generalização.\n",
    "\n",
    "A seguir, o modelo Random Forest será aplicado para classificar o sentimento com base nos comentários fornecidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando a base de dados do Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_model_W2V(data_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo Random Forest para classificação de sentimentos.\n",
    "\n",
    "    Inputs: data_path (str): O caminho para o arquivo CSV contendo os dados.\n",
    "\n",
    "    Outputs: Métricas do Modelo Random Forest Utilizando a base de dados do Word2Vec.\n",
    "    \"\"\"\n",
    "\n",
    "    # Carregar os dados\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # Dividir os dados em features (X) e target (y)\n",
    "    X = data.drop('sentiment', axis=1)\n",
    "    y = data['sentiment']\n",
    "\n",
    "    # Dividir os dados em conjuntos de treinamento e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Criar o classificador Random Forest com ajuste de pesos das classes\n",
    "    rf_classifier = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "    # Treinar o classificador com os dados de treinamento\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Fazer previsões nos dados de teste\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Converter os valores das classes para strings\n",
    "    target_names = [str(cls) for cls in rf_classifier.classes_]\n",
    "\n",
    "    # Exibir o relatório de classificação para cada classe\n",
    "    class_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    # Calcular a matriz de confusão\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Visualizar a matriz de confusão usando seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(\"Matriz de Confusão\")\n",
    "    plt.xlabel(\"Valor Previsto\")\n",
    "    plt.ylabel(\"Valor Real\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamar a função com o caminho para o arquivo CSV\n",
    "train_random_forest_model_W2V('./data/data_for_training_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo SVM *(Support Vector Machine)* é uma técnica de aprendizado de máquina utilizada tanto para tarefas de classificação quanto de regressão. Ele opera identificando o melhor hiperplano que separa os dados de diferentes classes no espaço de características.\n",
    "\n",
    "A ideia central do SVM é maximizar a margem de separação entre as classes de dados. Esta margem é definida como a distância entre o hiperplano de separação e os pontos de dados mais próximos, conhecidos como vetores de suporte. Ao maximizar essa margem, o modelo SVM busca melhorar sua capacidade de generalização, reduzindo a probabilidade de overfitting.\n",
    "\n",
    "O SVM é particularmente poderoso em espaços de alta dimensão e é eficaz mesmo quando o número de dimensões é maior do que o número de amostras. Além disso, quando os dados não são linearmente separáveis, o SVM utiliza uma técnica chamada *kernel trick*. Esta técnica transforma os dados para um espaço de maior dimensão onde é possível encontrar um hiperplano que separe as classes. Os kernels mais comuns utilizados são o linear, o polinomial e o RBF (*Radial Basis Function*).\n",
    "\n",
    "A seguir, o grupo *Thunder* aplicará o modelo SVM à tarefa de classificação de sentimentos com base nos comentários fornecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report # não modificar o local dessa importação\n",
    "\n",
    "def svm_model(csv_word2vec):\n",
    "    \"\"\"\n",
    "    Treina e avalia um modelo SVM usando um conjunto de dados fornecido.\n",
    "\n",
    "    Inputs: csv_word2vec (pd.DataFrame): DataFrame contendo os dados de entrada, com a coluna 'sentiment' como a variável alvo.\n",
    "\n",
    "    Outputs: None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Separando características (X) e rótulos (y)\n",
    "    X = csv_word2vec.drop(columns=[\"sentiment\"])\n",
    "    y = csv_word2vec[\"sentiment\"]\n",
    "    \n",
    "    # Dividindo o conjunto de dados em conjuntos de treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Padronizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Treinando o modelo SVM com kernel RBF\n",
    "    svm_model = SVC(kernel=\"rbf\", probability=True)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Fazendo previsões no conjunto de teste\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "\n",
    "    # Calculando a acurácia\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Calculando a matriz de confusão\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "    # Exibindo o relatório de classificação\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "    # Binarizando as classes para cálculo da curva ROC e AUC\n",
    "    y_test_binarized = label_binarize(y_test, classes=np.unique(y))\n",
    "    n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "    # Calculando as probabilidades para cada classe\n",
    "    y_pred_prob = svm_model.predict_proba(X_test)\n",
    "\n",
    "    # Calculando a curva ROC e a área sob a curva (AUC) para cada classe\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_prob[:, i])\n",
    "        roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_pred_prob[:, i])\n",
    "\n",
    "    # Plotando as curvas ROC\n",
    "    plt.figure()\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Exibindo os scores AUC\n",
    "    for i in range(n_classes):\n",
    "        print(f\"ROC AUC Score for class {i}: {roc_auc[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os dados do arquivo CSV\n",
    "data = pd.read_csv(\"./data/data_for_training_models.csv\")\n",
    "svm_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando dados ficticios para uso em testes isolados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "def create_sample_data():\n",
    "    \"\"\"\n",
    "    Cria um conjunto de dados de exemplo para testar a função svm_model.\n",
    "\n",
    "    Returns:\n",
    "    data (pd.DataFrame): DataFrame contendo os dados de exemplo.\n",
    "    \"\"\"\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=3, n_clusters_per_class=1, random_state=42)\n",
    "    data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    data['sentiment'] = y\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste de implantação do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_sample_data()\n",
    "svm_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBooster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo XGBoost, ou *Extreme Gradient Boosting*, é uma técnica avançada de aprendizado de máquina que combina previsões de múltiplos modelos fracos para formar um modelo robusto e preciso. Amplamente utilizado em competições de ciência de dados, o XGBoost é conhecido por seu desempenho superior, eficiência computacional e capacidade de lidar com grandes volumes de dados. Suas características, como regularização L1 e L2, manuseio automático de dados faltantes, paralelização e flexibilidade em funções de perda, tornam-no uma escolha ideal para tarefas de classificação e regressão.\n",
    "\n",
    "Neste projeto, o XGBoost foi aplicado para classificar sentimentos em três categorias: negativo (-1), neutro (0) e positivo (1), utilizando um conjunto de dados com vetores *Word2Vec*. O objetivo foi construir um modelo capaz de prever com precisão o sentimento expressado em textos, avaliando seu desempenho por meio de métricas como acurácia, precisão, recall e F1-score, visualizando os resultados em uma matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_xgboost_model(file_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost utilizando Word2Vec e exibe as métricas de avaliação.\n",
    "\n",
    "    Inputs: file_path (str): Caminho para o arquivo CSV contendo os dados de treinamento.\n",
    "\n",
    "    Outputs: dict: Um dicionário contendo as métricas de avaliação do modelo ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar os dados\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Verificar a presença da coluna 'sentiment'\n",
    "        if 'sentiment' not in data.columns:\n",
    "            raise KeyError(\"'sentiment' não encontrado nas colunas do DataFrame\")\n",
    "\n",
    "        # Dividir os dados em features (X) e target (y)\n",
    "        X = data.drop('sentiment', axis=1)\n",
    "        y = data['sentiment']\n",
    "\n",
    "        # Re-rotular os rótulos para garantir que estão no intervalo [0, num_class)\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # Dividir os dados em conjuntos de treinamento e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Criar o DMatrix para o XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # Definir os parâmetros do XGBoost\n",
    "        params = {\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.3,\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 3\n",
    "        }\n",
    "\n",
    "        # Treinar o modelo\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "        # Fazer previsões\n",
    "        y_pred = bst.predict(dtest)\n",
    "\n",
    "        # Exibindo o relatório de classificação\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "        # Calcular e visualizar a matriz de confusão\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negativo', 'Neutro', 'Positivo'], yticklabels=['Negativo', 'Neutro', 'Positivo'])\n",
    "        plt.title(\"Matriz de Confusão\")\n",
    "        plt.xlabel(\"Valor Previsto\")\n",
    "        plt.ylabel(\"Valor Real\")\n",
    "        plt.show()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamar a função com o caminho para o arquivo CSV\n",
    "metrics = train_xgboost_model('./data/data_for_training_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layers_model_encoding(csv_file_path):\n",
    "    \"\"\"\n",
    "    Treina um modelo LSTM com dados de vetores de palavras e calcula métricas de avaliação.\n",
    "\n",
    "    Inputs: csv_file_path (str): Caminho para o arquivo CSV contendo os dados.\n",
    "    \n",
    "    Outputs: dict: Um dicionário contendo as métricas de avaliação (accuracy, precision, recall, f1).\n",
    "    \"\"\"\n",
    "    # Carregar o arquivo CSV\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Combinar as colunas de vetores em uma única coluna 'vectors'\n",
    "    vector_columns = [str(i) for i in range(300)]\n",
    "    data['vectors'] = data[vector_columns].values.tolist()\n",
    "\n",
    "    # Remover as colunas de vetores separadas\n",
    "    data = data.drop(columns=vector_columns)\n",
    "\n",
    "    # Converter a coluna 'vectors' de listas de listas para arrays numpy\n",
    "    data['vectors'] = data['vectors'].apply(np.array)\n",
    "\n",
    "    # Criar as sequências de vetores\n",
    "    X = np.array(data['vectors'].tolist())\n",
    "    y = np.array(data['sentiment'])  # Usando a coluna 'sentiment' como rótulo\n",
    "\n",
    "    # Expandir as dimensões de X para ter a forma correta para LSTM\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "\n",
    "    # One-hot encode os rótulos\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    y = onehot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    # Dividir os dados em conjuntos de treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Definir e compilar o modelo\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(300, 1), return_sequences=False))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Treinar o modelo\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Prever os resultados\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Calcular as métricas\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='macro')  # 'macro' para multiclasse\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='macro')        # 'macro' para multiclasse\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='macro')                # 'macro' para multiclasse\n",
    "\n",
    "    # Imprimir a matriz de avaliação\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Calcular e imprimir o relatório de classificação\n",
    "    class_report = classification_report(y_test_classes, y_pred_classes, target_names=['Negativo', 'Neutro', 'Positivo'])\n",
    "    print(\"\\nRelatório de Classificação:\\n\", class_report)\n",
    "\n",
    "    # Calcular e visualizar a matriz de confusão\n",
    "    conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negativo', 'Neutro', 'Positivo'], yticklabels=['Negativo', 'Neutro', 'Positivo'])\n",
    "    plt.title(\"Matriz de Confusão\")\n",
    "    plt.xlabel(\"Valor Previsto\")\n",
    "    plt.ylabel(\"Valor Real\")\n",
    "    plt.show()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamando a função com o banco de dados que está sendo utilizado\n",
    "embedding_layers_model_encoding('./data/data_for_training_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "Segue as referências utilizadas durante a execução deste Documento Jupyther."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Métricas de Avaliação: Acesso em: 19 de Maio de 2024. Disponível em: https://vitorborbarodrigues.medium.com/m%C3%A9tricas-de-avalia%C3%A7%C3%A3o-acur%C3%A1cia-precis%C3%A3o-recall-quais-as-diferen%C3%A7as-c8f05e0a513c\n",
    "2. Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32. Acesso em: 21 de Maio de 2024. Disponível em: https://link.springer.com/article/10.1023/A:1010933404324.\n",
    "3. Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273-297. Acesso em: 21 de Maio de 2024. Disponível em: https://link.springer.com/article/10.1007/BF00994018.\n",
    "4. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785-794. Acesso em: 21 de Maio de 2024. Disponível em: https://dl.acm.org/doi/10.1145/2939672.2939785.\n",
    "5. Google Cloud Tech. \"Getting started with keras\". Acesso em 19 de maio de 2024. https://www.youtube.com/watch?v=J6Ok8p463C4\n",
    "6. Código Fonte TV. \"Tensor flower\". Acesso em: 22 de maio de 2024. Disponível em: https://www.youtube.com/watch?v=2eYLt1NA4Ss\n",
    "7. ChatGPT 3.5. OpenAI, utilizado em várias interações e consultas durante o desenvolvimento do projeto. Acesso em: 13 maio 2024.\n",
    "8. ChatGPT 4. OpenAI, utilizado em várias interações e consultas durante o desenvolvimento do projeto. Acesso em: 13 maio 2024.\n",
    "9. Understanding Word Vectors. Acessado em: 23 de maio de 2024. Disponível em: https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\n",
    "10. Codebasics. \"Whats is word 2 vec\". Acessado em 17 de maio de 2024. Diwponível em: https://www.youtube.com/watch?v=hQwFeIupNP0\n",
    "11. Entenda como funciona o Random Foresr. Acessado em : 20 de maio de 2024. Disponível em: https://didatica.tech/o-que-e-e-como-funciona-o-algoritmo-randomforest/\n",
    "12. Word2Vec: interpretação da linguagem humana com Word embedding. Acessado em 21 de maio de 2024. Disponível em: https://www.alura.com.br/conteudo/introducao-word-embedding\n",
    "13. SVM. Acessado em 22 de maio de 2024. Disponível em: https://www.inf.ufpr.br/dagoncalves/IA07.pdf\n",
    "14. Acessado em 24 de maio de 2024. Disponível em: https://medium.com/data-hackers/entendendo-o-que-%C3%A9-matriz-de-confus%C3%A3o-com-python-114e683ec509"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
